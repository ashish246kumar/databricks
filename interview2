how broadcast join internally improve the performance

Spark improves broadcast join performance by avoiding shuffle.
Spark sends a small table to all executors so each executor can join locally with its partition of the big table.
Since no large data is moved across the network, the join becomes significantly faster.â€
__________________________________________________________________________
Repartition improves performance by increasing parallelism and evenly distributing data across executors
df.repartition(N) tells Spark:
âž¡ Shuffle the entire dataset
âž¡ Divide it into N equal-sized partitions

2. Increases parallel tasks
More partitions â†’
More tasks â†’
More executors working simultaneously
_____________________________________


________________________________________________________________________________
Explain why lineage in Spark is crucial for fault tolerance. 

Lineage in Spark refers to the record of all transformations that have been applied to an RDD or DataFrame to produce the current dataset
Instead of recomputing everything from scratch or keeping multiple data copies (like Hadoop), Spark uses the lineage information to rebuild only that lost partition.
It traces back through the chain of transformations to find which earlier RDD and which transformation produced that specific lost partition.
It recomputes just that part from the original source data (data.txt) or cached intermediate data.
_______________________________________________________________________________________________
Modify a word count script to output results in descending frequency 
order.

# Load text file as DataFrame
df = spark.read.text("input.txt")

# Split lines into words and explode
words_df = df.select(explode(split(lower(col("value")), "\\s+")).alias("word"))

# Group by word and count occurrences
count_df = words_df.groupBy("word").count()

# Sort by count descending
sorted_df = count_df.orderBy(col("count").desc())

or 

from collections import Counter

# Read file and split into words
with open("input.txt", "r") as f:
    words = f.read().split()

# Count occurrences
word_count = Counter(words)

# Sort by frequency (descending)
sorted_counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True)

# Print sorted results
for word, count in sorted_counts:
    print(f"{word}: {count}")


_________________________________________________________________________________
without unity catalog
usermanagement , metastore and compute need to be managed  for each workspace
with unitycatalog
usermanagement , metastore  and compte for all worspace it is managed

ðŸ§© Without Unity Catalog
Each workspace manages its own:
ðŸ‘¤ User management (users and groups are created separately in each workspace)
ðŸ§  Metastore (each workspace has an independent metastore â€” separate databases, tables, permissions)
âš™ï¸ Compute resources (clusters, jobs, permissions managed locally per workspace)

âž¡ï¸ In other words, no central governance â€” every workspace is siloed and must be managed individually.
ðŸ§  With Unity Catalog
Unity Catalog provides centralized governance across all workspaces in the same Databricks account.
It offers:
âœ… Centralized user and access management â€” users, groups, and permissions are managed once and shared across workspaces.
âœ… Shared metastore â€” one unified metastore for all workspaces (so all catalogs, schemas, and tables are visible and governed consistently).
âœ… Consistent compute governance â€” cluster policies, access controls, and data permissions are consistent across all workspaces.
âž¡ï¸ Unity Catalog enables cross-workspace data access, consistent governance, and simplified administration.


metastore------>catalog----------->schema------------->table , view,volume,model, function
metastore should be one across region

___________________________________________________________________________________________________________________
___
Given a nested JSON dictionary from an API, how would you flatten it and convert it into a Pandas DataFrame?


data = {
  "user": {
    "id": 1,
    "name": "Ashish",
    "location": {"city": "Noida", "country": "India"},
    "skills": [
      {"name": "Python", "level": "Advanced"},
      {"name": "SQL", "level": "Intermediate"}
    ]
  }
}

import pandas as pd
from pandas import json_normalize

# Flatten the nested JSON
df = json_normalize(
    data,
    record_path=['user', 'skills'],         # list to expand into rows
    meta=[
        ['user', 'id'], 
        ['user', 'name'], 
        ['user', 'location', 'city'], 
        ['user', 'location', 'country']
    ]
)

print(df)
_____________________________________________________________________________________________________________________
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("FlattenJSON").getOrCreate()

# Read JSON file
df = spark.read.option("multiline", "true").json("user_data.json")
df.printSchema()
______
from pyspark.sql.functions import col, explode

# Flatten nested struct fields
df_flat = df.select(
    col("user.id").alias("user_id"),
    col("user.name").alias("user_name"),
    col("user.location.city").alias("city"),
    col("user.location.country").alias("country"),
    explode(col("user.skills")).alias("skill")   # Explode array into multiple rows
)

df_flat.printSchema()
df_flat.show(truncate=False)
______

df_final = df_flat.select(
    "user_id", "user_name", "city", "country",
    col("skill.name").alias("skill_name"),
    col("skill.level").alias("skill_level")
)

df_final.show(truncate=False)
_____________________________________________________________________________________________________



    

