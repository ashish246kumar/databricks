Write PySpark code to remove duplicate records based on package_id.
df_dedup = df.dropDuplicates(["package_id"])

window_spec = Window.partitionBy("package_id") \
                    .orderBy(col("updated_at").desc())

df_dedup = (
    df.withColumn("rn", row_number().over(window_spec))
      .filter(col("rn") == 1)
      .drop("rn")
)

_________________________________________________________
Given a DataFrame with event_time, filter records of last 24 hours.
from pyspark.sql.functions import col, current_timestamp

df_last_24h = df.filter(
    col("event_time") >= current_timestamp() - expr("INTERVAL 24 HOURS")
)
___________________________________
Write PySpark code to convert string timestamp to proper timestamp type
df = df.withColumn(
    "event_time_ts",
    to_timestamp("event_time", "yyyy-MM-dd HH:mm:ss")
)
__________________________________________-
How do you read data from Azure Data Lake path into Databricks?
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("abfss://raw@mydatalake.dfs.core.windows.net/input/data.csv")
____________________________
Write code to handle null values in multiple columns dynamically.

from pyspark.sql.functions import col, when, lit

df_filled = df.select(
    *[
        when(col(c).isNull(), lit("NA")).otherwise(col(c)).alias(c)
        if c in string_cols else col(c)
        for c in df.columns
    ]
)
______________________
differense between cache and persist 

df.cache()
What it does
Stores the DataFrame/RDD in memory
Uses default storage level

persist() â€” Configurable storage
Stores data using a specified storage level
You choose memory, disk, serialized, replicated, etc.
________________________
Write PySpark code to append data safely into Delta Table.

Append with Schema Enforcement (Recommended)
Ensures incoming data matches existing schema.
df.write \
  .format("delta") \
  .mode("append") \
  .option("mergeSchema", "false") \
  .save("/mnt/silver/customer_delta")

df.write \
  .format("delta") \
  .mode("append") \
  .saveAsTable("silver.customer_delta")
________________________________

____________________________
how broadcast join internally improve the performance

Spark improves broadcast join performance by avoiding shuffle.
Spark sends a small table to all executors so each executor can join locally with its partition of the big table.
Since no large data is moved across the network, the join becomes significantly faster.â€
__________________________________________________________________________
Repartition improves performance by increasing parallelism and evenly distributing data across executors
df.repartition(N) tells Spark:
âž¡ Shuffle the entire dataset
âž¡ Divide it into N equal-sized partitions

2. Increases parallel tasks
More partitions â†’
More tasks â†’
More executors working simultaneously
_____________________________________


________________________________________________________________________________
Explain why lineage in Spark is crucial for fault tolerance. 

Lineage in Spark refers to the record of all transformations that have been applied to an RDD or DataFrame to produce the current dataset
Instead of recomputing everything from scratch or keeping multiple data copies (like Hadoop), Spark uses the lineage information to rebuild only that lost partition.
It traces back through the chain of transformations to find which earlier RDD and which transformation produced that specific lost partition.
It recomputes just that part from the original source data (data.txt) or cached intermediate data.
_______________________________________________________________________________________________
Modify a word count script to output results in descending frequency 
order.

# Load text file as DataFrame
df = spark.read.text("input.txt")

# Split lines into words and explode
words_df = df.select(explode(split(lower(col("value")), "\\s+")).alias("word"))

# Group by word and count occurrences
count_df = words_df.groupBy("word").count()

# Sort by count descending
sorted_df = count_df.orderBy(col("count").desc())

or 

from collections import Counter

# Read file and split into words
with open("input.txt", "r") as f:
    words = f.read().split()

# Count occurrences
word_count = Counter(words)

# Sort by frequency (descending)
sorted_counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True)

# Print sorted results
for word, count in sorted_counts:
    print(f"{word}: {count}")


_________________________________________________________________________________
without unity catalog
usermanagement , metastore and compute need to be managed  for each workspace
with unitycatalog
usermanagement , metastore  and compte for all worspace it is managed

ðŸ§© Without Unity Catalog
Each workspace manages its own:
ðŸ‘¤ User management (users and groups are created separately in each workspace)
ðŸ§  Metastore (each workspace has an independent metastore â€” separate databases, tables, permissions)
âš™ï¸ Compute resources (clusters, jobs, permissions managed locally per workspace)

âž¡ï¸ In other words, no central governance â€” every workspace is siloed and must be managed individually.
ðŸ§  With Unity Catalog
Unity Catalog provides centralized governance across all workspaces in the same Databricks account.
It offers:
âœ… Centralized user and access management â€” users, groups, and permissions are managed once and shared across workspaces.
âœ… Shared metastore â€” one unified metastore for all workspaces (so all catalogs, schemas, and tables are visible and governed consistently).
âœ… Consistent compute governance â€” cluster policies, access controls, and data permissions are consistent across all workspaces.
âž¡ï¸ Unity Catalog enables cross-workspace data access, consistent governance, and simplified administration.


metastore------>catalog----------->schema------------->table , view,volume,model, function
metastore should be one across region

___________________________________________________________________________________________________________________
___
Given a nested JSON dictionary from an API, how would you flatten it and convert it into a Pandas DataFrame?


data = {
  "user": {
    "id": 1,
    "name": "Ashish",
    "location": {"city": "Noida", "country": "India"},
    "skills": [
      {"name": "Python", "level": "Advanced"},
      {"name": "SQL", "level": "Intermediate"}
    ]
  }
}

import pandas as pd
from pandas import json_normalize

# Flatten the nested JSON
df = json_normalize(
    data,
    record_path=['user', 'skills'],         # list to expand into rows
    meta=[
        ['user', 'id'], 
        ['user', 'name'], 
        ['user', 'location', 'city'], 
        ['user', 'location', 'country']
    ]
)

print(df)
________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________
| Option                        | Purpose                         | Example                                          |
| ----------------------------- | ------------------------------- | ------------------------------------------------ |
| **mode**                      | How to handle bad rows          | `"mode", "FAILFAST"`                             |
|                               | `PERMISSIVE` â†’ keep bad rows    |                                                  |
|                               | `DROPMALFORMED` â†’ drop bad rows |                                                  |
|                               | `FAILFAST` â†’ stop immediately   |                                                  |
| **badRecordsPath**            | Store corrupted rows            | `"badRecordsPath", "/tmp/bad/"`                  |
| **columnNameOfCorruptRecord** | Name for corrupt column         | `"columnNameOfCorruptRecord", "_corrupt_record"` |

| Option         | Purpose                  | Example               |
| -------------- | ------------------------ | --------------------- |
| **nullValue**  | Treat this value as NULL | `"nullValue", "NULL"` |
| **nanValue**   | Value representing NaN   | `"nanValue", "NaN"`   |
| **emptyValue** | Replace empty strings    | `"emptyValue", None`  |

| Option        | Purpose                       | Example                |
| ------------- | ----------------------------- | ---------------------- |
| **quote**     | Defines quote char            | `"quote", "\"" `       |
| **escape**    | Escape quotes inside text     | `"escape", "\\"`       |
| **multiLine** | Allow multi-line fields       | `"multiLine", True`    |
| **encoding**  | File encoding (default UTF-8) | `"encoding", "UTF-16"` |
| Option              | Purpose                         | Example               |                        |
| ------------------- | ------------------------------- | --------------------- | ---------------------- |
| **header**          | First row is column names       | `"header", True`      |                        |
| **inferSchema**     | Detect data types automatically | `"inferSchema", True` |                        |
| **schema**          | Manually specify schema         | `.schema(my_schema)`  |                        |
| **sep / delimiter** | Column separator                | `"sep", "             | "`, `"delimiter", ";"` |
____________________________________________________________________________________________________________










_______________________________________________________________________
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("FlattenJSON").getOrCreate()

# Read JSON file
df = spark.read.option("multiline", "true").json("user_data.json")
df.printSchema()
______
from pyspark.sql.functions import col, explode

# Flatten nested struct fields
df_flat = df.select(
    col("user.id").alias("user_id"),
    col("user.name").alias("user_name"),
    col("user.location.city").alias("city"),
    col("user.location.country").alias("country"),
    explode(col("user.skills")).alias("skill")   # Explode array into multiple rows
)

df_flat.printSchema()
df_flat.show(truncate=False)
______

df_final = df_flat.select(
    "user_id", "user_name", "city", "country",
    col("skill.name").alias("skill_name"),
    col("skill.level").alias("skill_level")
)

df_final.show(truncate=False)
_____________________________________________________________________________________________________



    

