without unity catalog
usermanagement , metastore and compute need to be managed  for each workspace
with unitycatalog
usermanagement , metastore  and compte for all worspace it is managed

ðŸ§© Without Unity Catalog
Each workspace manages its own:
ðŸ‘¤ User management (users and groups are created separately in each workspace)
ðŸ§  Metastore (each workspace has an independent metastore â€” separate databases, tables, permissions)
âš™ï¸ Compute resources (clusters, jobs, permissions managed locally per workspace)

âž¡ï¸ In other words, no central governance â€” every workspace is siloed and must be managed individually.
ðŸ§  With Unity Catalog
Unity Catalog provides centralized governance across all workspaces in the same Databricks account.
It offers:
âœ… Centralized user and access management â€” users, groups, and permissions are managed once and shared across workspaces.
âœ… Shared metastore â€” one unified metastore for all workspaces (so all catalogs, schemas, and tables are visible and governed consistently).
âœ… Consistent compute governance â€” cluster policies, access controls, and data permissions are consistent across all workspaces.
âž¡ï¸ Unity Catalog enables cross-workspace data access, consistent governance, and simplified administration.


metastore------>catalog----------->schema------------->table , view,volume,model, function
metastore should be one across region

___________________________________________________________________________________________________________________
___
Given a nested JSON dictionary from an API, how would you flatten it and convert it into a Pandas DataFrame?


data = {
  "user": {
    "id": 1,
    "name": "Ashish",
    "location": {"city": "Noida", "country": "India"},
    "skills": [
      {"name": "Python", "level": "Advanced"},
      {"name": "SQL", "level": "Intermediate"}
    ]
  }
}

import pandas as pd
from pandas import json_normalize

# Flatten the nested JSON
df = json_normalize(
    data,
    record_path=['user', 'skills'],         # list to expand into rows
    meta=[
        ['user', 'id'], 
        ['user', 'name'], 
        ['user', 'location', 'city'], 
        ['user', 'location', 'country']
    ]
)

print(df)
_____________________________________________________________________________________________________________________
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("FlattenJSON").getOrCreate()

# Read JSON file
df = spark.read.option("multiline", "true").json("user_data.json")
df.printSchema()
______
from pyspark.sql.functions import col, explode

# Flatten nested struct fields
df_flat = df.select(
    col("user.id").alias("user_id"),
    col("user.name").alias("user_name"),
    col("user.location.city").alias("city"),
    col("user.location.country").alias("country"),
    explode(col("user.skills")).alias("skill")   # Explode array into multiple rows
)

df_flat.printSchema()
df_flat.show(truncate=False)
______

df_final = df_flat.select(
    "user_id", "user_name", "city", "country",
    col("skill.name").alias("skill_name"),
    col("skill.level").alias("skill_level")
)

df_final.show(truncate=False)
_____________________________________________________________________________________________________



    

