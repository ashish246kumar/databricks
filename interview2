
______________________________________________________________________________________
4)How would you implement CDC (Change Data Capture) in a PySpark pipeline?

Change Data Capture (CDC) using Delta Live Tables (DLT) in Databricks. Instead of writing complex MERGE 
INTO statements, DLT provides the apply_changes API to handle
SCD Type 1 (overwriting data) and SCD Type 2 (maintaining history with start/end dates) automatically


@dlt.table(name="bronze_load")
def bronze_load():
    return (
        spark.readStream.format("cloudFiles")
        .option("cloudFiles.format", "csv")
        .schema(source_schema)
        .load("/Volumes/path/to/your/data/") # Path to your files
        .withColumn("file_process_date", date_format(current_timestamp(), "yyyy-MM-dd HH:mm:ss"))
    )
# Create the target streaming table
dlt.create_streaming_table("silver_load_scd1")

# Apply changes for SCD Type 1
dlt.apply_changes(
    target = "silver_load_scd1",
    source = "bronze_load",
    keys = ["P_key"],
    sequence_by = col("file_process_date"),
    stored_as_scd_type = 1
)
# Create the target streaming table
dlt.create_streaming_table("silver_load_scd2")

# Apply changes for SCD Type 2
dlt.apply_changes(
    target = "silver_load_scd2",
    source = "bronze_load",
    keys = ["P_key"],
    sequence_by = col("file_process_date"),
    stored_as_scd_type = 2,
    track_history_column_list = ["file_process_date"] # Tracks changes in this column to trigger a new version
)
(
Track History Column List: Specifically used for SCD Type 2. It monitors specific columns for changes; 
if a change occurs, it "expires" the old record (sets an end date) and inserts a new one (sets a start date
)

_____________________________________________________________________________________________-
A recursive CTE is a way to repeatedly run a query on its own result until there is nothing new to find.
Use a recursive CTE when:
Data has parent â†’ child relationships
You donâ€™t know the depth of the hierarchy
Examples:
Employee â†’ Manager
Category â†’ Subcategory
Folder â†’ Subfolder

recursive CTE has two parts:
1. Anchor Member
Returns the starting rows (roots).
2. Recursive Member
References the CTE itself to fetch child rows.
WITH RECURSIVE emp_cte AS (

    -- Anchor: start with top-level employee
    SELECT
        emp_id,
        emp_name,
        manager_id,
        0 AS level
    FROM employee
    WHERE manager_id IS NULL

    UNION ALL

    -- Recursive: get subordinates
    SELECT
        e.emp_id,
        e.emp_name,
        e.manager_id,
        c.level + 1
    FROM employee e
    JOIN emp_cte c
        ON e.manager_id = c.emp_id
)

SELECT * FROM emp_cte;


__________________________________________________________________________
Design a schema for tracking insurance claims lifecycle â€” how do you handle versioning?


+------------------+
|      POLICY      |
+------------------+
| policy_id (PK)   |
| policy_type      |
| start_date       |
| end_date         |
+------------------+
        |
        | 1
        |
        | N
+------------------+
|      CLAIM       |
+------------------+
| claim_id (PK)    |
| policy_id (FK)   |
| customer_id      |
| current_status   |
| current_version  |
| created_at       |
| updated_at       |
+------------------+
        |
        | 1
        |
        | N
+-------------------------+
|     CLAIM_VERSION       |
+-------------------------+
| claim_id (PK, FK)       |
| version_no (PK)         |
| claim_amount            |
| claim_details           |
| valid_from              |
| valid_to                |
| is_current              |
| changed_by              |
| change_reason           |
+-------------------------+

        |
        | 1
        |
        | N
+-----------------------------+
|   CLAIM_STATUS_HISTORY      |
+-----------------------------+
| status_id (PK)              |
| claim_id (FK)               |
| status                      |
| status_reason               |
| changed_by                  |
| changed_at                  |
+-----------------------------+


# 1ï¸âƒ£ Problem Understanding (Start Like This)

> *â€œAn insurance claim goes through multiple states (created, verified, approved, rejected, settled), and the data can change over time. We must track both the **current state** and the **full historical evolution** of the claim.â€*

Key requirements:

* Track claim lifecycle
* Preserve historical changes (audit)
* Support corrections & reprocessing
* Enable reporting on both **latest** and **past** states

---

# 2ï¸âƒ£ Core Design Principle

âœ” **Separate current state from history**
âœ” **Never update history â€” only append**
âœ” **Version every meaningful change**

This leads to a **bi-table pattern**:

1. **Current Claim table**
2. **Claim Version / History table**

---

# 3ï¸âƒ£ High-Level Schema Overview

```text
CLAIM
CLAIM_VERSION
CLAIM_STATUS_HISTORY
```

---

# 4ï¸âƒ£ Core Claim Table (Current Snapshot)

This table always holds the **latest version**.

```sql
CLAIM (
    claim_id            VARCHAR(50) PRIMARY KEY,
    policy_id           VARCHAR(50),
    customer_id         VARCHAR(50),
    claim_type          VARCHAR(30),
    claim_amount        DECIMAL(15,2),
    current_status      VARCHAR(20),
    current_version     INT,
    created_at          TIMESTAMP,
    updated_at          TIMESTAMP
);
```

### Why this table exists

* Fast reads for applications
* No need to scan history for latest data
* Optimized for operational workloads

ðŸ“Œ **Interview keyword:** *â€œcurrent snapshotâ€*

---

# 5ï¸âƒ£ Claim Version Table (Versioning Backbone)

This is where **versioning happens**.

```sql
CLAIM_VERSION (
    claim_id            VARCHAR(50),
    version_no          INT,
    policy_id           VARCHAR(50),
    claim_amount        DECIMAL(15,2),
    claim_details       JSON,
    changed_by          VARCHAR(50),
    change_reason       VARCHAR(100),
    valid_from          TIMESTAMP,
    valid_to            TIMESTAMP,
    is_current          BOOLEAN,
    PRIMARY KEY (claim_id, version_no)
);
```

### Key points to highlight in interview

* Every update â†’ **new version**
* No overwrite, only insert
* `valid_from` / `valid_to` enable **time-travel queries**
* `is_current = true` identifies active version

ðŸ“Œ **This is SCD Type-2 applied to transactional systems**

---

# 6ï¸âƒ£ Claim Status Lifecycle Table

Status changes happen more frequently than data changes.

```sql
CLAIM_STATUS_HISTORY (
    claim_id        VARCHAR(50),
    status          VARCHAR(20),
    status_reason   VARCHAR(100),
    changed_by      VARCHAR(50),
    changed_at      TIMESTAMP
);
```

### Why separate status?

* Status transitions are audit-critical
* Easier SLA and turnaround reporting
* Avoids bloating main version table



__
12)How do you approach historical data archiving in a large-scale DW? 
Historical data archiving is about optimizing performance and cost while still meeting compliance and audit requirements.
In a large-scale DW:
Not all data is queried equally
Old data increases storage cost, query latency, and maintenance overhead
But deleting it is not allowed due to audits, analytics, and regulations
So the goal is to:
âœ” Keep frequently used data fast
âœ” Move rarely used data cheaply
âœ” Still make archived data accessible

2ï¸âƒ£ Step 1: Define Data Retention & Classification

The first step is data classification based on usage and business needs.

ðŸ”¹ Hot / Warm / Cold Model
Data Type	Age	Usage	Storage
Hot data	0â€“3 / 6 months	Daily dashboards, reports	Core DW
Warm data	6â€“24 months	Ad-hoc analysis	DW or Nearline
Cold data	>2 years	Audit, compliance	Archive storage

Step 3: Choose the Archival Storage Layer
Archived data is moved from expensive DW storage to low-cost storage.
Common archival targets:
Object storage (ADLS / S3 / GCS)
Cold or Archive tiers
Data format used:
Parquet / ORC
Highly compressed
Columnar â†’ faster reads for audits

Step 5: Accessing Archived Data (Very Important)
Archived data should still be queryable.
How?
External tables
Federated queries
Read-only schemas
Example:
SELECT SUM(amount)
FROM archived_fact_transactions
WHERE transaction_date BETWEEN '2021-01-01' AND '2021-12-31';
Emphasize:
â€œArchived data is not part of daily reporting, but available for audit or historical analysis.â€

______________________________________________
what problem deletion vector solve

How do you delete or update a few rows in huge immutable Parquet files without rewriting the entire file?
Why this is a problem
Delta Lake data is stored in immutable Parquet files
Traditionally, DELETE / UPDATE / MERGE meant:
Read the whole file
Modify rows
Rewrite a new file
âž¡ï¸ Very expensive for small row-level changes on large tables
What Deletion Vectors solve
Stores a Deletion Vector that marks which rows are logically deleted
_________________________________________________
Write PySpark code to remove duplicate records based on package_id.
df_dedup = df.dropDuplicates(["package_id"])

window_spec = Window.partitionBy("package_id") \
                    .orderBy(col("updated_at").desc())

df_dedup = (
    df.withColumn("rn", row_number().over(window_spec))
      .filter(col("rn") == 1)
      .drop("rn")
)

_________________________________________________________
Given a DataFrame with event_time, filter records of last 24 hours.
from pyspark.sql.functions import col, current_timestamp

df_last_24h = df.filter(
    col("event_time") >= current_timestamp() - expr("INTERVAL 24 HOURS")
)
___________________________________
Write PySpark code to convert string timestamp to proper timestamp type
df = df.withColumn(
    "event_time_ts",
    to_timestamp("event_time", "yyyy-MM-dd HH:mm:ss")
)
__________________________________________-
How do you read data from Azure Data Lake path into Databricks?
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("abfss://raw@mydatalake.dfs.core.windows.net/input/data.csv")
____________________________
Write code to handle null values in multiple columns dynamically.

from pyspark.sql.functions import col, when, lit

df_filled = df.select(
    *[
        when(col(c).isNull(), lit("NA")).otherwise(col(c)).alias(c)
        if c in string_cols else col(c)
        for c in df.columns
    ]
)
______________________
differense between cache and persist 

df.cache()
What it does
Stores the DataFrame/RDD in memory
Uses default storage level

persist() â€” Configurable storage
Stores data using a specified storage level
You choose memory, disk, serialized, replicated, etc.
________________________
Write PySpark code to append data safely into Delta Table.

Append with Schema Enforcement (Recommended)
Ensures incoming data matches existing schema.
df.write \
  .format("delta") \
  .mode("append") \
  .option("mergeSchema", "false") \
  .save("/mnt/silver/customer_delta")

df.write \
  .format("delta") \
  .mode("append") \
  .saveAsTable("silver.customer_delta")
________________________________

____________________________
how broadcast join internally improve the performance

Spark improves broadcast join performance by avoiding shuffle.
Spark sends a small table to all executors so each executor can join locally with its partition of the big table.
Since no large data is moved across the network, the join becomes significantly faster.â€
__________________________________________________________________________
Repartition improves performance by increasing parallelism and evenly distributing data across executors
df.repartition(N) tells Spark:
âž¡ Shuffle the entire dataset
âž¡ Divide it into N equal-sized partitions

2. Increases parallel tasks
More partitions â†’
More tasks â†’
More executors working simultaneously
_____________________________________


________________________________________________________________________________
Explain why lineage in Spark is crucial for fault tolerance. 

Lineage in Spark refers to the record of all transformations that have been applied to an RDD or DataFrame to produce the current dataset
Instead of recomputing everything from scratch or keeping multiple data copies (like Hadoop), Spark uses the lineage information to rebuild only that lost partition.
It traces back through the chain of transformations to find which earlier RDD and which transformation produced that specific lost partition.
It recomputes just that part from the original source data (data.txt) or cached intermediate data.
_______________________________________________________________________________________________
Modify a word count script to output results in descending frequency 
order.

# Load text file as DataFrame
df = spark.read.text("input.txt")

# Split lines into words and explode
words_df = df.select(explode(split(lower(col("value")), "\\s+")).alias("word"))

# Group by word and count occurrences
count_df = words_df.groupBy("word").count()

# Sort by count descending
sorted_df = count_df.orderBy(col("count").desc())

or 

from collections import Counter

# Read file and split into words
with open("input.txt", "r") as f:
    words = f.read().split()

# Count occurrences
word_count = Counter(words)

# Sort by frequency (descending)
sorted_counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True)

# Print sorted results
for word, count in sorted_counts:
    print(f"{word}: {count}")


_________________________________________________________________________________
without unity catalog
usermanagement , metastore and compute need to be managed  for each workspace
with unitycatalog
usermanagement , metastore  and compte for all worspace it is managed

ðŸ§© Without Unity Catalog
Each workspace manages its own:
ðŸ‘¤ User management (users and groups are created separately in each workspace)
ðŸ§  Metastore (each workspace has an independent metastore â€” separate databases, tables, permissions)
âš™ï¸ Compute resources (clusters, jobs, permissions managed locally per workspace)

âž¡ï¸ In other words, no central governance â€” every workspace is siloed and must be managed individually.
ðŸ§  With Unity Catalog
Unity Catalog provides centralized governance across all workspaces in the same Databricks account.
It offers:
âœ… Centralized user and access management â€” users, groups, and permissions are managed once and shared across workspaces.
âœ… Shared metastore â€” one unified metastore for all workspaces (so all catalogs, schemas, and tables are visible and governed consistently).
âœ… Consistent compute governance â€” cluster policies, access controls, and data permissions are consistent across all workspaces.
âž¡ï¸ Unity Catalog enables cross-workspace data access, consistent governance, and simplified administration.


metastore------>catalog----------->schema------------->table , view,volume,model, function
metastore should be one across region

___________________________________________________________________________________________________________________
___
Given a nested JSON dictionary from an API, how would you flatten it and convert it into a Pandas DataFrame?


data = {
  "user": {
    "id": 1,
    "name": "Ashish",
    "location": {"city": "Noida", "country": "India"},
    "skills": [
      {"name": "Python", "level": "Advanced"},
      {"name": "SQL", "level": "Intermediate"}
    ]
  }
}

import pandas as pd
from pandas import json_normalize

# Flatten the nested JSON
df = json_normalize(
    data,
    record_path=['user', 'skills'],         # list to expand into rows
    meta=[
        ['user', 'id'], 
        ['user', 'name'], 
        ['user', 'location', 'city'], 
        ['user', 'location', 'country']
    ]
)

print(df)
________________________________________________________________________________________________________
_____________________________________________________________________________________________________________________
| Option                        | Purpose                         | Example                                          |
| ----------------------------- | ------------------------------- | ------------------------------------------------ |
| **mode**                      | How to handle bad rows          | `"mode", "FAILFAST"`                             |
|                               | `PERMISSIVE` â†’ keep bad rows    |                                                  |
|                               | `DROPMALFORMED` â†’ drop bad rows |                                                  |
|                               | `FAILFAST` â†’ stop immediately   |                                                  |
| **badRecordsPath**            | Store corrupted rows            | `"badRecordsPath", "/tmp/bad/"`                  |
| **columnNameOfCorruptRecord** | Name for corrupt column         | `"columnNameOfCorruptRecord", "_corrupt_record"` |

| Option         | Purpose                  | Example               |
| -------------- | ------------------------ | --------------------- |
| **nullValue**  | Treat this value as NULL | `"nullValue", "NULL"` |
| **nanValue**   | Value representing NaN   | `"nanValue", "NaN"`   |
| **emptyValue** | Replace empty strings    | `"emptyValue", None`  |

| Option        | Purpose                       | Example                |
| ------------- | ----------------------------- | ---------------------- |
| **quote**     | Defines quote char            | `"quote", "\"" `       |
| **escape**    | Escape quotes inside text     | `"escape", "\\"`       |
| **multiLine** | Allow multi-line fields       | `"multiLine", True`    |
| **encoding**  | File encoding (default UTF-8) | `"encoding", "UTF-16"` |
| Option              | Purpose                         | Example               |                        |
| ------------------- | ------------------------------- | --------------------- | ---------------------- |
| **header**          | First row is column names       | `"header", True`      |                        |
| **inferSchema**     | Detect data types automatically | `"inferSchema", True` |                        |
| **schema**          | Manually specify schema         | `.schema(my_schema)`  |                        |
| **sep / delimiter** | Column separator                | `"sep", "             | "`, `"delimiter", ";"` |
____________________________________________________________________________________________________________










_______________________________________________________________________
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("FlattenJSON").getOrCreate()

# Read JSON file
df = spark.read.option("multiline", "true").json("user_data.json")
df.printSchema()
______
from pyspark.sql.functions import col, explode

# Flatten nested struct fields
df_flat = df.select(
    col("user.id").alias("user_id"),
    col("user.name").alias("user_name"),
    col("user.location.city").alias("city"),
    col("user.location.country").alias("country"),
    explode(col("user.skills")).alias("skill")   # Explode array into multiple rows
)

df_flat.printSchema()
df_flat.show(truncate=False)
______

df_final = df_flat.select(
    "user_id", "user_name", "city", "country",
    col("skill.name").alias("skill_name"),
    col("skill.level").alias("skill_level")
)

df_final.show(truncate=False)
_____________________________________________________________________________________________________



    

