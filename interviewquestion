_____________________________________________________________________________________________

_____________________________________________________________________
df.rdd.getNumPartitions()

This is the BEST DataFrame-only way to see the number of partitions and how data is distributed across them

df \
    .withColumn("partitionId", spark_partition_id()) \
    .groupBy("partitionId") \
    .count() \
    .show()

_____________________________________________________________________
‚ÄúSpark follows a master-slave architecture with one Driver and multiple Executors running on a cluster
managed by a Cluster Manager.
The driver converts your code into tasks, the cluster manager allocates resources, 
and the executors run the tasks in parallel on partitions of data.‚Äù

Driver Program
Runs your main Python/Scala code
Creates SparkSession
Converts DataFrame operations into Logical Plan ‚Üí Physical Plan
Creates DAG (Directed Acyclic Graph)
Divides work into stages and tasks
Sends tasks to executors
Collects results from executor

Cluster Manager
Allocates resources (CPU, RAM) to your Spark job.

Executors
These are the workers running on cluster nodes.
Execute the tasks assigned by the driver
Keep data in memory
_______________________________________________________
difference between exist and 
z ordering vs liquid clustring
cache vs persist
1. Difference between client mode and cluster mode
where exactly your driver runs edge node / executor
2. what is partition skew, reasons for it. How to solve partition skew issues?
3. what is a broadcast join in apache spark.
4. what is the difference between partition and bucketing.
5. What are different types of joins in Spark
6. why count when used with group by is a transformation else its an action.
7. If your spark job is running slow how would you approach to debug it.
8. Difference between managed table & external table. When do you go about creating exernal tables.
9. Why we are not using mapreduce these days. what are similarities between spark and mapReduce.
10. How do you handle your pyspark code deployment, Explain about the CICD process.
11. Have you used caching in your project, when & where do you consider using it.
12. Explain about memory management in Apache spark.
13. difference between narrow and wide transformation
14. difference between dataframe and dataset
I
15. If some job failed with out of memory error in production, what will be you approach to debug that
16. what is DAG & how that helps.
Why did you use Databricks Workflows for orchestration
if we do not use inferschema=true what it is used
what is the pyspark and why it is used
difference between series and dataframe in pandas 
how i can know repartionning is needed when i am doing task in notebook
through spark ui and df.explain()
how  we do Mask of PII/Sensitive data 
when we do spark  job what happen

When we run a Spark job, Spark first creates a logical plan, 
then optimizes it, then converts it into physical execution plans. The job is divided into stages and tasks,
and tasks run in parallel on executors

q)what is better data set or dataframe and why
q)why we do repartion what problem it solves
DataFrames are untyped and give the best performance, while Datasets are typed and provide compile-time safety.
q) difference between managed table and external table

___________________________________
when to use cte and when to use subquery 
You need to reuse the same result multiple times
You want to break down a complex logic step-by-step

When to Use a Subquery
The logic is simple and used only once
You don‚Äôt need to reuse the result
_________________________________________________________
Every write creates a new version of the table, and Delta stores old Parquet files instead of deleting them.

SELECT * FROM delta.`/mnt/sales` VERSION AS OF 3;
df = spark.read.format("delta").option("versionAsOf", 3).load("/mnt/sales")

_______________________________________________
Clustered Index
It stores the data in sorted order physically.
The table itself is the index.
Only one clustered index per table (because data can be sorted only one way).
Think of it like a dictionary arranged alphabetically.
The words (data) are stored in the same order as the index.

Non-Clustered Index
It creates a separate structure (like another table).
Contains the index key + pointer to the actual data.
You can have multiple non-clustered indexes.
üëâ Think of it like a book‚Äôs index page at the back.
It doesn‚Äôt store the actual chapter, only page numbers pointing to it.

‚≠ê 2. One-Line Difference (Perfect for Interview)
Clustered Index	Non-Clustered Index
Rearranges the actual data	Creates a separate index structure
Only 1 per table	Many per table
Faster for range queries	Faster for specific point queries
‚≠ê 3. When the interviewer asks: "Give an example"
Example Table: Employee(id, name, city, salary)
If id is Clustered Index:

Data in table is stored like:
1, 2, 3, 4, 5 ‚Ä¶

Searching ID = 450 is super fast because data is physically sorted.

If city is Non-Clustered Index:

SQL creates a separate structure:
"Delhi" ‚Üí points to rows 5, 8, 12
"Mumbai" ‚Üí points to rows 1, 9

___________________________________________
PySpark is the Python API for Apache Spark.
It allows you to write Spark applications using Python instead of Scala or Java.
With PySpark, we can process large-scale data in a distributed manner across a cluster
_________
I used Databricks Workflows because it is natively integrated with the Databricks Lakehouse. 
It allows me to orchestrate notebooks, Delta Live Tables, SQL, and Python jobs in one place,
without needing an external tool. It supports scheduling, retries, alerts, dependency management, 
and gives full lineage and monitoring in one UI.

__________________________________________________
1. select count(*) from emp;
2. select count(1) from emp;
3. select count(distinct 2) from emp;
4. select 1 from emp;
5. select 1;
_____________________________________________
| Query                    | Meaning                        | Output Count     | Notes                  |
| ------------------------ | ------------------------------ | ---------------- | ---------------------- |
| **1. COUNT(*)**          | Count all rows                 | = number of rows | Standard & fastest     |
| **2. COUNT(1)**          | Count all rows                 | = number of rows | Same as COUNT(*)       |
| **3. COUNT(DISTINCT 2)** | Count distinct constant values | Always 1         | Only if table has rows |
| **4. SELECT 1 FROM emp** | Return 1 for each row          | = number of rows | Useful for EXISTS      |
| **5. SELECT 1**          | Return single row              | Always 1 row     | No table used          |

_______________________________________________________________________________________________
Why did you use Databricks Workflows for orchestration

I used Databricks Workflows because it is natively integrated with the Databricks Lakehouse. It allows me to orchestrate notebooks,
Delta Live Tables, SQL, and Python jobs in one place, without needing an external tool. It supports scheduling, retries, alerts, dependency management,
and gives full lineage and monitoring in one UI. This makes the pipeline more reliable and easier to maintain
__________________________________________________________
A nested field means a column that contains a STRUCT (record) inside it.
In other words, a column itself has multiple sub-columns.
 It is like a JSON object stored inside a single column.
{
  "name": "Ashish",
  "address": {
      "city": "Delhi",
      "pincode": 110001
  }
}
_____________________________
A repeated field means the column contains an array (list) of values or an array of STRUCTS.
{
  "name": "Ashish",
  "skills": ["Spark", "Python", "SQL"]
}

{
  "orders": [
     {"id": 1, "amount": 100},
     {"id": 2, "amount": 200}
  ]
}

______________________________________
Maximum Number of Partitions

.number of executors
‚úî number of cores
‚úî memory
‚úî cluster size
‚úî shuffle overhead
_______________________

__________________________________________________
how i can secure my etl piepine what are things you follow for the seccurity of your pipe,line

Data Security

a. Encryption
Data-at-rest: Enable encryption on storage layers
Masking & Tokenization
Mask PII/Sensitive data

Identity & Access Management (IAM)
Role-Based Access Contro

Secret/Key Management
Store passwords, API keys, tokens in:
Azure Key Vault

Service Principals
Use service accounts for automated jobs
Enable job-level ACLs (Databricks, Airflow, ADF).

c. Cluster Security
Use secure clusters:
No public IPs
Use IAM-based access
_______________________________
Service Principals are used so that automated systems like ETL pipelines, applications, or CI/CD processes can securely 
authenticate to cloud resources without using personal user accounts
________________________________
cdc is the process of capturing only changed data since pervious version
cdf is new feature which supports cdc in databrick delta lake env.
change data feed interally create view which have the following column(_change_Type, _Commit_Version, _Commit_timestamp)
(technique used to track and capture only the data that has changed ‚Äî inserts, updates, and deletes ‚Äî instead of moving the entire dataset. It helps build efficient, 
real-time or near-real-time data pipelines by reducing the amount of data that needs to be processed)

__________________________________________

______________________________________
A Star Schema is a type of data modeling used in data warehouses where data is organized into:
One central Fact table
Multiple surrounding Dimension tables
It looks like a star, where the fact table is in the center and dimension tables are the points around it.
Fact Table
Contains:
Measures / metrics (SalesAmount, Revenue, Quantity)
Foreign keys to dimension tables
Usually very large in size

Dimension Tables
Contain:
Descriptive attributes
Textual data

Star Schema
Dimensions are denormalized
Fewer joins, faster
Simpler model
Used for performance

Snowflake Schema
Dimensions are normalized
More joins, slower
More complex
Used for storage optimization

________________
A Snowflake Schema is a type of data warehouse modeling where dimension tables are normalized 
into multiple related tables.
It is basically an expanded version of the Star Schema, where the dimension tables break into sub-dimensions
it have much lower cost because it des not have data  redundancy so it save storage
_____________________________________________________
My name is Ashish Kumar. I‚Äôm from Deoghar, Jharkhand, and I completed my graduation from Raj Kumar Goel Institute of Technology, Ghaziabad.

I have around 2.9 years of experience and I‚Äôm currently working at Nagarro. I started my career as a Java backend developer, but later moved into Data Engineering.

Recently, I worked on a major migration project where we moved data from on-prem systems to Azure Databricks. My responsibilities included building ETL pipelines, performing data quality checks, migrating data from DB2 to ADLS using AzCopy, and implementing error-handling logic and validations
_____________________________________________________________________________________
how do we handle exception and logging in  project 
We handle exceptions using a centralized error-handling strategy.
For every step of the pipeline or code module, we wrap critical logic in try/except blocks,
log the error with complete details, send notifications, and ensure the system either retries

We always log:
Timestamp
Error message
Stack trace
Input parameters
Job/Task name
Tools used:
Databricks Logging
Log4j
CloudWatch

If a temporary failure happens (network timeout, cluster issue), auto-retry:
Using Databricks Workflows retry settings

If an exception occurs, we log the error into a Delta error_log table with metadata like file name, row number, exception type, and stack trace.
we manitain a table which contain error_date,error_code,source_name,impacted_field

___________________________________________________________________________

____________________________________________________
üü´ Bronze Layer (Raw Layer)
What it is:
This layer contains raw, unprocessed data exactly as it is coming from the source.
How to explain in interview:
‚ÄúThe Bronze layer is the raw data layer. We ingest data from multiple sources‚Äîfiles, 
APIs, databases, Kafka‚Äîinto Bronze without any transformation. It acts as the single source of truth.‚Äù

Characteristics:
Contains raw, uncleaned data
Schema may not be enforced
Includes duplicates, errors, null values
Used for reprocessing if needed

The Silver layer contains cleaned, validated, and structured data.

How to explain in interview:
The Silver layer is the cleaned and standardized layer. We remove duplicates, fix data types, handle nulls, apply quality checks, and create a reliable, analytics-ready dataset.‚Äù
Characteristics:
Data cleansing
Deduplication
Standardization (dates, types, structure)
Joins across data sources
Apply business rules/validations

Gold Layer (Business/Analytics Layer)
What it is:
The Gold layer contains aggregated, business-level curated data for reporting or ML.
How to explain in interview:
The Gold layer is the business-level curated layer. It contains aggregated, transformed data ready 
for dashboards, reporting, and ML models. This is the layer business users consume
Characteristics:
Aggregations, KPIs, business metrics
Dimension and fact tables (star schema)
Ready for BI tools (Power BI, Tableau)
Optimized for performance
Example:
Daily sales report
__________________________________________________________________
Why do we use this medalian architecture?
Ensures data quality step-by-step
Improves performance
Better governance and auditability
Easy to reprocess data from Bronze
____________________________________________________
view
A view in a database is a virtual table that is based on the result-set of an SQL query. 
Views are used to simplify complex queries, 
enhance security, and present data in a specific format
___________________________________________
A stored procedure is a reusable set of SQL statements stored in the database. Instead of writing queries again and again,
we put them in a procedure and call it with parameters. Since it‚Äôs precompiled, it improves performance 
and also helps in implementing business logic at the database level

CREATE PROCEDURE getCustomerOrders(IN customer_id INT)
BEGIN
   SELECT * FROM orders WHERE customer_id = customer_id;
END;

Why companies use stored procedures:
Faster execution (pre-compiled)
Centralized business logic
Reusability
Better security (limit access to tables)
____________________________
Materialized View 
A Materialized View (MV) is a pre-computed, stored result of a SQL query.
Unlike a normal view, which runs the query every time, a materialized view stores the data physically in the database.
It is used to improve performance for complex queries.
üìå Why Materialized Views are Used?
Fast performance for repeated heavy queries
Reduces load on source tables
Excellent for BI dashboards, reporting, analytics

CREATE MATERIALIZED VIEW sales_summary AS
SELECT region, SUM(amount) AS total_sales
FROM sales
GROUP BY region;

Materialized Views Are Usually Used in the GOLD Layer

Why?
Because the Gold layer contains aggregated, business-ready, analytics-ready data
Materialized views improve performance for:
Aggregations
Joins
KPIs
BI dashboards
Summary tables
___________________________________________________________
What can you run using Databricks Workflows?‚Äô
You can say:
Databricks notebooks (Python, SQL, Scala)
Delta Live Tables (DLT pipelines)
Spark JARs
SQL queries

Python scripts
‚ÄúDatabricks Workflows is the native orchestration tool inside Databricks.
It allows us to schedule and run end-to-end data pipelines such as notebooks,
Delta Live Tables, Python scripts, JARs, SQL queries, and ML workflows.

In my project, we used Databricks Workflows to orchestrate our daily ETL pipeline.
The workflow had multiple tasks ‚Äî ingest raw data into Bronze, clean it into Silver, aggregate it into Gold, and finally refresh dashboards.
Each task was a notebook, and dependencies ensured the sequence ran smoothly. We also added retries and email alerts for failure.‚Äù

Key Features
You can create workflows with:
Task dependencies
Built-in Scheduling
Supports:
Cron schedules
Retry, Timeout, Error Handling
Automatic retries
Email failure notifications

Monitoring & Logging
Job run history
Spark logs
_________________________________________

__________________________________________________
A trigger is an automatic action in the database that runs in response to an event such as:
INSERT
UPDATE
DELETE
Triggers run without being explicitly called. The database fires them automatically.
We use triggers for auditing, validations, maintaining logs, updating related tables, etc.

CREATE TRIGGER log_employee_changes
AFTER UPDATE ON employees
FOR EACH ROW
BEGIN
    INSERT INTO employee_audit(emp_id, old_salary, new_salary, changed_at)
    VALUES (OLD.id, OLD.salary, NEW.salary, NOW());
END;

______________________________________________
_A Delta Table is simply a table stored in the Delta Lake format.

‚úî What it provides:
ACID transactions (atomicity, consistency, isolation, durability)
Time travel (versioning)
Schema enforcement & schema evolution

DLT is a managed ETL/ELT framework built on top of Delta Lake.
You define pipelines, and Databricks automatically:
Schedules the pipeline
Manages the DAG (dependency graph)
Handles failures and retries
_________________________________________________________
ùêáùê®ùê∞ ùêà‚Äôùêù ùê©ùê´ùê®ùêúùêûùê¨ùê¨ ùêö ùüêùüé ùêìùêÅ ùêüùê¢ùê•ùêû ùê¢ùêß ùêÄùê©ùêöùêúùê°ùêû ùêíùê©ùêöùê´ùê§ ùêÉùêöùê¢ùê•ùê≤ - ùê¨ùê≠ùêûùê© ùêõùê≤ ùê¨ùê≠ùêûùê©:
20 TB daily isn‚Äôt a casual batch job.
We need to finish it in a few hours - not days - and do it every single day.
So the first question isn‚Äôt ‚Äúwhat code,‚Äù
it‚Äôs ‚Äúwhat cluster can handle this load reliably?‚Äù
I'll go with below setup

Worker nodes = 50   
Cores per node = 16   
Memory per node = 128 GB  

üí° That gives:
 ‚Ä¢ 50 √ó 16 = 800 cores
 ‚Ä¢ 50 √ó 128 = 6.4 TB total memory
But we can‚Äôt use all of that - Spark shares each node with the OS and Hadoop daemons.

Each node runs its own OS + NodeManager + Spark overhead.
So we leave 1 core + 8 GB RAM per node for system processes.
That leaves:
 ‚Ä¢ 15 cores and 120 GB usable memory per node
 ‚Üí 50 nodes √ó 15 cores = 750 cores total
 ‚Üí 50 nodes √ó 120 GB = 6 TB usable memory

Now the real Spark math starts.
We don‚Äôt want one big executor per node - too much GC overhead.
Nor do we want 16 tiny ones - too much scheduling overhead.
Thumb Rule: 3‚Äì5 cores per executor ‚Üí sweet spot = 4.
So per node:
15 cores √∑ 4 ‚âà 3 executors (1 core idle is fine).
Memory = 120 GB √∑ 3 = 40 GB per executor ‚Üí after 10 % overhead ‚âà 36 GB usable.
Each executor ‚Üí 4 cores + 36 GB RAM.
Executors per node = 3 ‚Üí 150 executors total.
üßÆ Total cluster usage by Spark =
 ‚Ä¢ 150 √ó 4 = 600 cores
 ‚Ä¢ 150 √ó 36 = 5.4 TB RAM
The rest (~25 %) stays reserved for system + shuffle overhead ‚Äî the safe zone.

Spark runs one task per core.
So 600 parallel tasks = 600 partitions processed at a time.
For 20 TB Parquet data (~512 MB per partition):
 ‚Ä¢ 20 TB / 0.5 GB ‚âà 40,000 partitions.
Waves = 40000 / 600 ‚âà 67 waves of tasks.
Each wave = a batch of 600 parallel tasks ‚Üí after 67 waves, job done 

Optimization Principles
1Ô∏è‚É£ Use columnar compression (Parquet + Snappy).
2Ô∏è‚É£ Partition by date or region for pruning.
3Ô∏è‚É£ Tune spark.sql.shuffle.partitions = 600 to match cores.
4Ô∏è‚É£ Broadcast small tables ‚Üí avoid heavy shuffles.
5Ô∏è‚É£ Compact tiny files ‚Üí 512 MB target size.
6Ô∏è‚É£ Checkpoint long DAGs ‚Üí no recomputation loops
______________________________________________________________
Databricks Jobs Are Slow ‚Äî Usually for These 6‚Äì7 Core Reasons 

1Ô∏è‚É£ Data Skew (Hot Keys)
Symptom: A few tasks take hours while others finish in seconds.
Cause: Uneven data distribution ‚Äî one key has too much data.
Fix: Add salting to hot keys, use repartition, or broadcast join when possible.
Check: Spark UI ‚Üí Stages ‚Üí task duration variance.

2Ô∏è‚É£ Memory Spill to Disk
Symptom: Stages show heavy disk I/O during shuffle.
Cause: Executor memory insufficient for shuffle/join operations.
Fix:
1. Increase spark.executor.memory
2. Reduce cores per executor
3. Enable Photon (reduces memory pressure)
4. Monitor spill metrics in Spark UI ‚Üí Stages.

3Ô∏è‚É£ Wrong Cluster Sizing
Symptom: Too slow or too costly.
Cause: Under-provisioned or over-provisioned clusters.
Fix:
1. Check CPU utilization in Spark UI ‚Üí Executors.
2. Scale up if CPU ~100%, scale down if mostly idle.
3. Use Autoscaling for variable workloads.

4Ô∏è‚É£ Inefficient Joins & Shuffles
Symptom: Long shuffle time and large shuffle read/write sizes.
Cause: Sort-merge joins or unnecessary shuffles.
Fix:
1. Use broadcast joins for <1GB lookup tables.
2. Enable AQE (spark.sql.adaptive.enabled=true) for dynamic join optimization.
3. Repartition intelligently before joins.

5Ô∏è‚É£ Cold Cache & Slow I/O
Symptom: First run slow, second run much faster.
Cause: Data read directly from cloud storage (S3/ADLS).
Fix:
1. Use Delta Lake with Liquid Clustering.
2. Run OPTIMIZE for compacted, co-located data.
3. Photon improves read speed 3‚Äì5x for Parquet/Delta.

6Ô∏è‚É£ Small Files Problem (a.k.a. Too Many Tiny Files)
Symptom: Jobs spend time opening thousands of small files.
Cause: Frequent writes/appends create small files.
Fix:
1. Use OPTIMIZE in Delta to compact files.
2. Use mergeSchema and autoOptimize options.
3. Combine input data at source or before writing to Delta.

7Ô∏è‚É£ Improper Partitioning or Missing Z-Ordering
Symptom: Full table scans despite filters.
Cause: Poor partition design or unclustered data.
Fix:
1. Partition on high-selectivity columns.
2. Use Liquid Clustering or Z-ORDER for frequently filtered columns.
3. Avoid over-partitioning (too many small partitions).

______________________________________________________________
1Ô∏è‚É£ What happens when you run a Spark job?
Spark creates a DAG, breaks it into stages, converts them into tasks, and sends those tasks to executors for execution.
The driver coordinates the whole process.

2Ô∏è‚É£ Driver vs Executors ‚Äî what‚Äôs the difference?
	‚Ä¢	Driver: creates the DAG, schedules stages, tracks metadata.
	‚Ä¢	Executors: actually run the tasks and store data in memory/disk.

3Ô∏è‚É£ What is a DAG?
A Directed Acyclic Graph of transformations.
It shows how your data flows from one step to the next, without cycles.
Spark uses it to optimize your job before execution.


4Ô∏è‚É£ What are Narrow vs Wide transformations?
	‚Ä¢	Narrow: each partition depends on a single parent partition (e.g., map, filter). ‚Üí No shuffle
	‚Ä¢	Wide: data must move across partitions (e.g., groupBy, join). ‚Üí Shuffle happens

This one is critical ‚Äî most performance issues come from wide transformations.


5Ô∏è‚É£ What is a Stage?
A group of tasks that can run without a shuffle.
A shuffle boundary creates a new stage.


6Ô∏è‚É£ What triggers a Shuffle?
Wide transformations like:
groupBy, agg, join, distinct, repartition, orderBy.
If Spark needs to move data across nodes ‚Üí shuffle.

7Ô∏è‚É£ What is a Task in Spark?
The smallest unit of work, run on executors.
Each task processes one partition of data.


8Ô∏è‚É£ How does Spark achieve fault tolerance?
Through lineage.
Instead of storing intermediate results, Spark knows how data was created and can recompute lost partitions if an executor fails.

9Ô∏è‚É£ What does a Cluster Manager do?
YARN / Kubernetes / Standalone:
It allocates resources (CPU, memory) to Spark applications
but does not schedule tasks ‚Äî the driver does that.

üîü What changes when AQE (Adaptive Query Execution) is enabled?
Spark can:
	‚Ä¢	Detect skew at runtime
	‚Ä¢	Change join strategy on the fly
	‚Ä¢	Merge/split shuffle partitions
Basically, Spark becomes smarter during execution, not just before it.
________________________________________
What is data modeling, and why is it important?

Answer: Data modeling is the process of creating a visual representation of the data structure and relationships within a database. It is important because it helps in understanding the data requirements, designing efficient databases, ensuring data integrity, and facilitating communication between stakeholders.

What are the different types of data models?

Answer: There are three main types of data models:
Conceptual data model: Represents high-level entities and relationships without considering implementation details.

Logical data model: Defines the structure of the data without considering how it will be physically implemented.

Physical data model: Specifies the physical implementation of the database, including tables, columns, indexes, etc.

What is normalization and denormalization?

Answer: Normalization is the process of organizing data in a database to reduce redundancy and dependency by dividing large tables into smaller ones and defining relationships between them. 

Denormalization, on the other hand, involves adding redundant data to improve query performance by reducing the need for joins.


What are surrogate keys and natural keys?

Answer: Surrogate keys are artificial keys generated solely for identifying records in a table, while natural keys are attributes that already exist in the real world and can uniquely identify records.

Explain the differences between OLTP and OLAP.
Answer: OLTP (Online Transaction Processing) is designed for transaction-oriented applications that involve frequent, short transactions, while OLAP (Online Analytical Processing) is optimized for complex queries and analysis of large volumes of data.

What is a star schema and snowflake schema?

Answer: A star schema is a data warehouse schema that consists of one or more fact tables referencing any number of dimension tables. In a snowflake schema, dimension tables are normalized into multiple related tables, resulting in a more normalized structure.

How do you handle slowly changing dimensions (SCDs) in data modeling?

Answer: Slowly changing dimensions are dimensions that change over time. They can be handled using various techniques such as Type 1 (overwrite), Type 2 (add new row), and Type 3 (add new column). The choice depends on the specific requirements of the business.

What is cardinality in data modeling?

Answer: Cardinality refers to the relationship between rows of two tables. It describes the number of instances of one entity that can be associated with a single instance of another entity.
_________________________________________________________________________________________________________
How would you process a large CSV file in chunks, filter only the required rows, and write the filtered data into a new file using Python?

# Step 1: Read CSV in chunks (distributed automatically by Spark)
df = spark.read.csv("s3://bucket-name/large_file.csv", header=True, inferSchema=True)

# Step 2: Filter only required rows (example condition)
# Example: select rows where 'country' is 'India' and 'sales' > 1000
filtered_df = df.filter((col("country") == "India") & (col("sales") > 1000))

# Step 3: Write filtered data into new CSV (or parquet) file
filtered_df.write.mode("overwrite").csv("s3://bucket-name/filtered_output/")

‚úÖ If You Want Manual Chunk Processing (Less Common in Spark)
If your CSV is extremely large and you want to control chunking manually:
df = spark.read.option("maxRecordsPerFile", 1000000).csv("path/to/large.csv", header=True, inferSchema=True)

___________________________________________________________________________________________________________________________
A table has start_date and end_date columns ‚Äî how would you calculate the total number of overlapping days across all rows?


SELECT 
    SUM(
        DATEDIFF(
            LEAST(a.end_date, b.end_date),
            GREATEST(a.start_date, b.start_date)
        ) + 1
    ) AS total_overlap_days
FROM events a
JOIN events b 
  ON a.id < b.id  -- avoid self-join duplicates
 AND a.start_date <= b.end_date
 AND b.start_date <= a.end_date;

________________________________________________________________________________________________________
How would you identify duplicate records in a table and delete only the duplicates while retaining one original record?

You can use ROW_NUMBER() to assign a unique rank to each duplicate group.
____________________________________________________________________________________
How do you remove duplicates and handle nulls in PySpark?

clean_df = df.dropDuplicates(['emp_id']).na.fill({'age': 0, 'city': 'Unknown'})
________________________________________________
How do you handle Slowly Changing Dimensions (SCD Type 2)?

Compare source and target using business keys.
Update old record's end_date and is_active=false.
Insert new record with start_date=current_date().
Example (Delta Merge):
MERGE INTO dim customer t
USING updates s
ON t.cust id = s.cust_id
WHEN MATCHED AND t.email <> s.email THEN
UPDATE SET t.is_active=false, t.end_date=current_date()
________________________________________________________________________________________
Design a simple data pipeline on AWS for daily data ingestion.

Ingest: S3 (landing zone) + EventBridge trigger
Process: AWS Glue job (PySpark ETL)
Store: S3 curated ‚Üí Redshift
Orchestrate: Step Functions / Lambda
Secure: IAM roles + KMS encryption
___________________________________________________________
Tell me about a time your pipeline failed in production.
What I said:
"Our ADF job failed due to missing files. I added file existence checks + retry logic ‚Üí reduced daily failures by 95%."
___________________________________________
How do you measure data pipeline performance? .

We measure data pipeline performance using key metrics like throughput, latency, and resource utilization.
Track throughput (rows/sec). 
Monitor failure rates via CloudWatch/ADF logs. 
We set up Grafana dashboards and alerts to visualize performance trends and detect anomalies early.

__________________________________________________________________

How do you ensure data accuracy and quality in ETL pipelines?

Perform schema validation before processing (column count, data types).
Apply data profiling and threshold checks (null%, duplicates).
Store invalid data separately in an error zone for audit.
Log validation results in a control table.
Automate alerts for any deviation beyond tolerance.
_________________________________________________________________________________________________________________
How do you optimize Delta Lake tables for performance and cost?

We optimize Delta tables using Z-Ordering, OPTIMIZE for file compaction, and VACUUM for cleanup.
With a good partitioning strategy and Parquet-based compression, we improved query performance by 65% while reducing storage cost

We optimize Delta Lake tables by using multiple techniques to improve performance and reduce cost.
We use Z-Ordering on high-selectivity columns like region and sale_date to improve query performance by clustering related data together.
We periodically run OPTIMIZE to compact small files into larger ones, reducing file overhead and improving read efficiency.
We also schedule VACUUM to remove old or unused files and free up storage space.
Data is stored in Parquet with Delta format, ensuring efficient compression and faster access.
We design proper partitioning strategies to balance between parallelism and metadata overhead.

__________________________________________________________________________________________________
What's the difference between Azure Synapse and Databricks when would you use each?

Databricks: Best for ETL, big data, and transformation using PySpark.
Synapse: Best for serving analytical queries to BI tools (e.g., Power BI).
In our project ‚Üí Databricks handled transformation, Synapse served reporting data.

Feature	                      Azure Databricks	                                      Azure Synapse Analytics
Purpose	                    Data engineering, ML, streaming	                       Data warehousing, analytics
Data type	                Unstructured / semi-structured	                        Structured
Engine	                 Apache Spark	                                          SQL-based MPP (Massively Parallel Processing)
Users	                  Data Engineers,                                         Data Scientists	Data Analysts, BI Developers
Integration	Works well with Kafka, ADLS, MLflow	                                works well with Power BI, SQL tools
__________________________________________________________________________________________________________
How do you reduce data storage and compute costs in Azure ?

For storage, we use lifecycle management policies to move old or infrequently used data to cool or archive tiers in Azure Data Lake or Blob Storage.
We compress data and store it in efficient formats like Parquet or Delta, which reduce both size and read costs.
For compute, we use auto-scaling and job clusters in Databricks so clusters run only when needed.

Use Delta Lake for storage efficiency and caching for faster reads.
Schedule jobs during off-peak hours to use lower-cost compute.
Consolidate small files (‚Äúsmall file problem‚Äù) in ADLS to reduce metadata overhead.

Delta Lake improves storage efficiency by storing data in the Parquet format, which is a highly compressed and columnar storage type.
It supports data versioning (time travel) using transaction logs instead of keeping multiple file copies, saving storage space
Features like data skipping and Z-ordering make queries faster by reading only the necessary data blocks.
It supports schema evolution without needing to reload data, which saves both time and resources.

File format: Delta Lake builds on top of Parquet ‚Üí efficient compression (Snappy, Gzip).
Transaction log (_delta_log): Stores metadata instead of duplicating data ‚Üí minimal overhead


______________________________________________________________________________


How do you process streaming data in Databricks or AWS?

In our project, we process streaming data in Databricks using Spark Structured Streaming.
The data is consumed from Kafka topics in real time using the Databricks readStream API.
We define a streaming DataFrame to process incoming data ‚Äî applying transformations, filters, and aggregations.
The processed data is then written to different sinks like Delta tables, Azure Data Lake, or another Kafka topic using writeStream.
We use checkpointing and watermarking to handle fault tolerance and late-arriving data.
The jobs are scheduled and monitored in Databricks workflows, ensuring reliable and continuous data processing.
This setup allows us to build real-time pipelines for analytics and dashboarding.

stream df.writeStream.format("delta")
.option("checkpoint Location", "/mnt/chkpt/")
.start("/mnt/output/")
___________________________________________________________
How do you implement data governance and lineage in Azure?

Use Azure Purview for data discovery, cataloging, and lineage tracking.
Store metadata centrally in Data Catalog (Glue Catalog in AWS).
Assign access via RBAC and Purview roles.

Tool for Governance: Azure Purview (now part of Microsoft Fabric governance).
For Data Lineage: Purview integrates with Data Factory and Databricks pipelines to trace the complete data flow.
For Access Control: Azure AD roles + managed identities.
For Data Security: Sensitivity labels, encryption, and Key Vault

Purview helps us discover, classify, and catalog data across Azure services like Data Lake, SQL, and Databricks.
It automatically captures data lineage, showing how data moves from source to transformation to reports.
We define data policies, access control, and sensitivity labels in Purview to ensure data security and compliance.
We also use Azure AD for role-based access and integrate with Key Vault for managing secrets.

_________________________________________________
walk me through the architecture of last  engineering project

sources: flat files
injestion: autoloader
processing : pyspark, delta
storage: adls
monitoring : log and alert 
i have implemented error profile reprocessing logic
written script for data validation
migrated huge volume of data
new source integrate 
clean and tranform the dataframe

______________
how do you handle ci/cd for databricks

In our project, we set up CI/CD for Databricks using Azure DevOps pipelines to move code across different environments like Dev, QA, and Prod.
We use ARM templates to automatically create and configure Databricks workspaces for each environment.
Our YAML pipeline builds the code and creates a .bundle file that contains notebooks, jobs, and all needed files.
This bundle is then deployed to the target environment using an automated release pipeline.
We manage environment-specific details like URLs and secrets using pipeline variables.
This setup helps us deploy faster, avoid manual errors, and keep all environments consistent.


_______________________________________________________________
Batch processing deals with large data sets in bulk ‚Äî it‚Äôs great for analytics and reporting where latency isn‚Äôt critical. Real-time processing, 
on the other hand, processes data continuously as it arrives ‚Äî useful when immediate insights or actions are required.
For example, a bank might use real-time processing to detect fraudulent transactions instantly, but use batch processing to generate end-of-day reports.
The trade-off is between latency and complexity ‚Äî batch is simpler and cheaper, while real-time gives instant insights but requires more complex infrastructure.

__________________________________________________________________________
How do you ensure the scalability of a data pipeline handling rapidly growing data volumes.

To ensure scalability of a data pipeline handling rapidly growing data volumes, 
I focus on building a distributed, decoupled, and elastic architecture ‚Äî one that can scale horizontally, process data in parallel, and handle spikes efficiently

(a) Use Distributed Processing Frameworks
I‚Äôd use distributed systems like Apache Spark.that can process large datasets in parallel across multiple nodes instead of relying on a single machine.
üîπ Example: Spark can automatically distribute the workload and scale horizontally when more data or nodes are added.

(b) Decouple Components
I ensure the pipeline components (ingestion, processing, storage, and serving) are loosely coupled using message queues or streaming systems like Kafka
üîπ This prevents one stage (like ingestion) from blocking another (like transformation).

Horizontal Scaling
Instead of scaling vertically (adding more CPU to one server), I prefer horizontal scaling ‚Äî adding more nodes or containers

Optimize Data Processing
I use partitioning, caching, and parallel I/O to speed up large data transformations.

___________________________________________________________________________________________________________________________
ETL (Extract, Transform, Load):
Data is first extracted from source systems, transformed into the required format on a separate processing server or ETL tool, and then loaded into the data warehouse.

ELT (Extract, Load, Transform):
Data is first extracted and loaded directly into the target data warehouse (often cloud-based), and the transformation happens inside the data warehouse using its processing power.

| Feature                     | ETL                                       | ELT                                                    |
| --------------------------- | ----------------------------------------- | ------------------------------------------------------ |
| **Transformation Location** | Before loading (external ETL tool/server) | After loading (inside target DB)                       |
| **Best for**                | On-premises data warehouses               | Cloud-based warehouses (Snowflake, BigQuery, Redshift) |
| **Performance**             | Limited by ETL server                     | Uses data warehouse‚Äôs high scalability                 |
| **Data Volume**             | Moderate                                  | Large/Big Data                                         |
| **Latency**                 | Batch processing                          | Near real-time or flexible processing                  |

Use ELT when:
You need to load large volumes of raw data quickly.
You want more flexibility for data analysts to run transformations on demand.

Use ETL when:
You need complex transformations before loading data.
The target system has limited processing capacity.

____________________________________________________________________________________________________________________________________________________________
Z-Ordering in Delta Lake is an optimization that reorders data within partitions based on multiple columns, improving data skipping and query performance.
For example, if my table is partitioned by date but queries often filter by customer_id, I can run OPTIMIZE table ZORDER BY (customer_id) to cluster related records together.
This way, Spark reads fewer files for those queries
Z-Ordering is especially useful for frequent filters on non-partition columns, multi-dimensional queries, and join keys, reducing I/O and improving performance significantly.‚Äù
Before Z-Order:
Files have random customer_id values ‚Üí Spark scans many files.
After Z-Order:
Each file contains a narrow range of customer_id values ‚Üí Spark skips irrelevant files easily.

‚öñÔ∏è 6Ô∏è‚É£ Performance Impact
Z-Ordering reduces the number of files read by Spark.

OPTIMIZE sales_table
Z-Ordering uses a multi-dimensional clustering technique
ZORDER BY (customer_id, product_id);
Under the hood:
Delta Lake combines files within each partition.
Then it reorders records based on the Z-order curve of the specified columns.
The reordered files make data skipping more effective.


--------------------------------------------------------------------------------------------------------------
T ‚Äî Time Travel (Data Versioning)
Delta automatically versions your table after every transaction (in _delta_log).
You can query older versions of your data using a timestamp or version number.
spark.read.format("delta").option("versionAsOf", 5).load("/delta/sales")
This feature allows rollback to previous versions

Schema Evolution:
Delta enforces a consistent schema between writes.
If new data doesn‚Äôt match the existing schema, Spark throws an error unless schema evolution is explicitly allowed.
Example:
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")
Delta prevents accidental data corruption by enforcing schema consistency ‚Äî you can‚Äôt write mismatched columns or types.
But if needed, Delta supports schema evolution for adding new columns

_______________________________________
Data skew happens when some partitions in a Spark job have significantly more data than others.
a few tasks take much longer while others finish quickly, leading to poor parallelism and slower job execution
Let‚Äôs say you‚Äôre joining on customer_id, and one customer (say ID = 12345) has millions of records, while others have only a few.

Why Does Data Skew Happen
Few dominant values in the key column.
Improper partitioning of data


üöÄ 4Ô∏è‚É£ Strategies to Resolve Data Skew
from pyspark.sql import functions as F
# Add random salt to skewed key
salted_df = df.withColumn("salted_key", 
                          F.concat(F.col("customer_id"), F.lit("_"), (F.rand()*5).cast("int")))
# Join using salted key
result = salted_df.join(other_df, salted_df.salted_key == other_df.salted_key)
‚ÄúI used key salting to distribute skewed keys across multiple partitions.
By adding a random salt, the heavy key gets split into smaller groups, balancing the workload.‚Äù

Avoid shuffle altogether by broadcasting the smaller dataset to all executors.
from pyspark.sql.functions import broadcast
result = large_df.join(broadcast(small_df), "customer_id")
‚ÄúInstead of shuffling both datasets, I broadcast the smaller one to every node ‚Äî this prevents skew from large shuffle operations




____________________________________________________________________________
REPARTITION
repartition() increases or decreases the number of partitions by shuffling all the data across the cluster.
How it works:
Performs a full shuffle ‚Äî data is redistributed across all partitions.
Each partition gets a roughly equal amount of data.
Can increase or decrease partitions.
Creates new partitions from scratch

When to use:
When you increase partitions to improve parallelism.
When you want even data distribution across partitions (to avoid skew).
performing a wide transformation (like join or groupBy).

‚öôÔ∏è 2Ô∏è‚É£ COALESCE
How it works:
Narrow transformation ‚Äî it simply merges adjacent partitions.
No shuffle, so it‚Äôs much faster than repartition.
Can only decrease the number of partitions

when to use
., before writing output files
fewer output files

| Feature                 | `repartition()`                | `coalesce()`             |
| ----------------------- | ------------------------------ | ------------------------ |
| **Shuffle**             | Yes (full shuffle)             | No shuffle               |
| **Performance**         | Slower (due to shuffle)        | Faster (no shuffle)      |
| **Use case**            | Increase or balance partitions | Decrease partitions      |
| **Transformation type** | Wide transformation            | Narrow transformation    |
| **Data distribution**   | More balanced                  | May be uneven            |
| **Flexibility**         | Can increase or decrease       | Only decrease            |
| **Fault tolerance**     | Higher                         | Lower (since no shuffle) |

how i coalesce does not do shuffle but repartion do

shuffling means moving data between executors or partitions across the cluster ‚Äî it‚Äôs expensive because it involves:
writing data to disk,
sending it over the network

Spark does not move data across executors.
Instead, it reuses existing partitions and merges adjacent partitions into fewer ones.
Spark just combines partition 1+2 and partition 3+4 ‚Äî without redistributing data.
So:
No network I/O
No shuffle

repartition() Works Internally
Redistribute all records evenly across the 8 partitions.
Data from every old partition can go to any new partition.
This ensures balanced data distribution and parallelism

______________________
_____________________________________________________________________
how Spark executes tasks in DAG (Directed Acyclic Graph) format

When we submit a Spark job, Spark first builds a logical plan of transformations.
Then, it converts this into a physical plan called a DAG ‚Äî a Directed Acyclic Graph ‚Äî that represents stages separated by shuffle boundaries.
Each stage is further divided into tasks based on partitions.
The DAG Scheduler submits these stages to the Task Scheduler, which runs the tasks on executors across the cluster.
Once all stages complete successfully, the job result is returned

DAG Scheduler: Divides the job into stages and creates the DAG.
Task Scheduler: Schedules tasks to executors.
Executor: Executes tasks and returns results.
Cluster Manager (like YARN, Mesos, or Standalone): Allocates resources.

Each node in the DAG represents a RDD/DataFrame.
Each edge represents a transformation (like map, filter, join, etc.).
No cycles ‚Äî because once data is processed, Spark doesn‚Äôt go back.
Spark divides transformations into two types:
Narrow transformations: (e.g., map, filter) ‚Üí data from one partition goes to one partition (no shuffle).
Wide transformations: (e.g., groupBy, join) ‚Üí data needs to be shuffled across nodes.
When Spark detects a wide transformation, it creates a new stage in the DAG.
‚ÄúSpark breaks the job into stages based on shuffle boundaries and forms a DAG of stages.‚Äù
_____________________________________________
Spark Core is the general execution engine for the Spark platform, responsible for tasks such 
as scheduling, distributing, and monitoring applications.
SparkContext is the entry point for any Spark application. It acts as a connection to the 
Spark cluster, allowing Spark jobs to be executed.

SparkSession is the unified entry point to work with DataFrames, Datasets, and SQL in 
Apache Spark. It replaces SQLContext and HiveContex

Excellent question, Ashish üëè ‚Äî this is **one of the most frequently asked Spark fundamentals** in interviews (especially for data engineering or Spark developer roles).

Let‚Äôs break it down clearly ‚Äî **definitions + examples + how they are related internally + a comparison table** üëá

---

## ‚öôÔ∏è **1Ô∏è‚É£ Executor**

* **Definition:**
  Executors are **JVM processes** launched on worker nodes in a Spark cluster that actually **run the tasks** of your Spark job.
  They are responsible for:

  * Executing your code (tasks)
  * Storing data in memory/disk (for caching)
  * Returning results to the driver

* **Key points:**

  * Executors live as long as your Spark application runs.
  * Each executor can run **multiple tasks in parallel**, depending on the number of **cores** allocated to it.

* **Example:**
  If your Spark app has 4 executors and each has 4 cores ‚Üí
  **Total 16 tasks can run in parallel.**

---

## üßÆ **2Ô∏è‚É£ Cores**

* **Definition:**
  Cores are the **processing units** (CPU threads) available to executors to run tasks in parallel.

* **Key points:**

  * Each core runs **one task at a time**.
  * More cores = more parallelism = faster execution (up to a point).

* **Example:**

  * Executor with 4 cores ‚Üí can process 4 tasks simultaneously.
  * Cluster with 10 executors √ó 4 cores = 40 tasks parallel execution.

---

## üß© **3Ô∏è‚É£ Stage**

* **Definition:**
  A **stage** is a **set of parallel tasks** that execute the same computation (like a map or reduce) on different data partitions.

* **Created When:**
  Spark divides a job into stages **based on shuffle boundaries**.
  (A shuffle happens when data must be redistributed across the cluster ‚Äî e.g., during `groupByKey`, `join`, or `reduceByKey`.)

* **Example:**

  ```python
  rdd.map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b)
  ```

  * Stage 1 ‚Üí Map stage
  * Stage 2 ‚Üí Reduce stage (after shuffle)

---

## üß± **4Ô∏è‚É£ Job**

* **Definition:**
  A **job** is triggered whenever you perform an **action** (like `count()`, `collect()`, `saveAsTextFile()`, etc.) on an RDD or DataFrame.

* **Key points:**

  * One action ‚Üí one job
  * A job is divided into one or more stages
  * Each stage has multiple parallel tasks

* **Example:**

  ```python
  df.count()
  df.show()
  ```

  ‚Üí Each of these actions will trigger a **separate Spark job**

---

## üîÑ **5Ô∏è‚É£ Transformation**

* **Definition:**
  Transformations are operations on RDDs or DataFrames that **define a new dataset** from an existing one.
  They are **lazy** ‚Äî Spark does not execute them immediately; it builds a DAG (Directed Acyclic Graph) instead.

* **Types:**

  * **Narrow transformations:** No shuffle (e.g., `map`, `filter`)
  * **Wide transformations:** Causes shuffle (e.g., `groupByKey`, `reduceByKey`)

* **Example:**

  ```python
  rdd2 = rdd1.map(lambda x: x*2)
  rdd3 = rdd2.filter(lambda x: x > 10)
  ```

  No computation happens yet ‚Äî Spark just builds the DAG.

---

## ‚ö° **6Ô∏è‚É£ Action**

* **Definition:**
  Actions **trigger execution** of the DAG built by transformations.
  They tell Spark to **actually perform the computation** and return a result to the driver or write to storage.

* **Example:**

  ```python
  rdd3.count()          # Returns number of elements
  rdd3.collect()        # Returns all elements to driver
  rdd3.saveAsTextFile() # Writes to storage
  ```

  Once you call an action ‚Üí Spark starts executing stages ‚Üí tasks ‚Üí on executors.

---

## üîó **7Ô∏è‚É£ Relationship Between All Components**

```
Driver Program
   ‚Üì
Transformations build DAG
   ‚Üì
Action triggers a Job
   ‚Üì
Job is split into Stages
   ‚Üì
Each Stage has multiple Tasks
   ‚Üì
Tasks are executed in parallel on Executors
   ‚Üì
Each Executor uses its allocated Cores
___________________________________



__________________
List Comprehension

A list comprehension is a concise way to create lists in Python ‚Äî
it allows you to write a single line of code instead of a multi-line for loop.
squares = [i * i for i in range(5)]
__________

A lambda function is a small, anonymous function defined without a name ‚Äî
typically used for short, simple operations that you don‚Äôt want to formally define using def.
lambda arguments: expression
square = lambda x: x * x
print(square(5))

___________________________________________________________
Described the hybrid approach of maintaining a current data table and a historical changes table.

In data warehousing, a hybrid approach combines the benefits of real-time updates and full historical tracking.
We maintain two tables ‚Äî one for current data and one for historical changes.

When an update happens, the old record is moved to the history table with effective start and end dates, and the current table is updated with the new value.
In Current Table, we update:

Customer_ID | Name   | Address | IsActive
-----------------------------------------
101          | Rajesh | Mumbai  | Y


‚úÖ In History Table, we insert the old record:

Customer_ID | Old_Address | Effective_Start | Effective_End | IsActive
---------------------------------------------------------------------
101          | Delhi       | 2024-01-01      | 2025-01-15     | N


____________________________________________________________________
scd
__________

Slowly Changing Dimensions, or SCDs, are techniques used in data warehousing to track and manage 
changes in dimension table attributes over time ‚Äî such as a customer‚Äôs address or an employee‚Äôs department

| **Type**                    | **Description**                                                                                             | **Example**                                                                                                              |
| --------------------------- | ----------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| **Type 0 ‚Äì Passive**        | No changes are tracked; once stored, values remain fixed.                                                   | Student‚Äôs admission major remains the same even if changed later.                                                        |
| **Type 1 ‚Äì Overwrite**      | Old data is replaced by new data; no history maintained.                                                    | Customer‚Äôs old address replaced by new one.                                                                              |
| **Type 2 ‚Äì New Record**     | A new record is created when data changes; full history preserved with effective dates or flags.            | Product‚Äôs price change creates a new record with new `start_date` and `end_date`.                                        |
| **Type 3 ‚Äì New Column**     | Adds a new column to store the previous value; only one level of history retained.                          | Employee‚Äôs previous role stored in ‚ÄúPrevious_Role‚Äù column.                                                               |
| **Type 4 ‚Äì History Table**  | Separate history table maintains old records; main table holds current data.                                | Old store details pushed to ‚ÄúStore_History‚Äù table when location changes.                                                 |
| **Type 6 ‚Äì Hybrid (1+2+3)** | Combination of Types 1, 2, and 3 ‚Äî overwriting, adding new records, and tracking previous columns together. | Salesperson changes region ‚Üí new record (Type 2), previous region column updated (Type 3), contact overwritten (Type 1). |

________________________________________________________________________________
FACT vs DIM

A fact table is the central table in a data warehouse that stores measurable data ‚Äî things you can count or sum, such as sales amount, units sold, or revenue.
It answers what happened in the business, while dimension tables explain who, what, when, and where.
For example, in a retail business, the fact table might have columns like Product_ID, Store_ID, Date_ID, and measures such as Units_Sold and Revenue. 
These foreign keys link to dimensions for deeper analysis.

There are three main types:
Transactional ‚Äî records every individual event, like each sale.
Snapshot ‚Äî captures the state at regular intervals, like daily inventory.
Accumulating ‚Äî tracks progress of a process, like order to delivery.
Accumulating fact table ‚Äî tracks processes with defined start and end points, such as an order from booking to delivery


A dimension table contains descriptive attributes about business entities ‚Äî like products, customers, stores, or dates.
It provides context to the numbers in a fact table, answering the who, what, where, when of a business event.
It gives context to the numbers in a fact table
For example, instead of seeing just Product_ID = 101, the dimension table tells you that it‚Äôs ‚ÄúiPhone 15‚Äù, belongs to the ‚ÄúSmartphones‚Äù category, and is made by ‚ÄúApple‚Äù.


| **Feature**           | **Fact Table**                                                          | **Dimension Table**                                                          |
| --------------------- | ----------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| **Purpose**           | Stores measurable, quantitative business data (facts or metrics).       | Stores descriptive, contextual information about business entities.          |
| **Type of Data**      | Numeric data (e.g., sales amount, quantity, profit).                    | Text or descriptive data (e.g., product name, customer name, location).      |
| **Foreign Key**       | Contains foreign keys referencing dimension tables.                     | Does **not** contain foreign keys from fact tables (only primary key).       |
| **Operations**        | Used for aggregations like SUM, AVG, COUNT during analysis.             | Used for filtering, grouping, and labeling facts in reports.                 |
| **Size**              | Usually very large (millions or billions of records).                   | Smaller in size compared to fact tables.                                     |
| **Refresh Frequency** | Updated frequently (as new transactions occur).                         | Changes rarely ‚Äî mostly when entity attributes change.                       |
| **Example**           | Sales_Fact (Date_ID, Product_ID, Customer_ID, Units_Sold, Revenue).     | Product_Dim (Product_ID, Product_Name, Category, Brand, Price).              |

______________________________________________________________________________________________________
An index is a database structure (like a lookup table) that helps the database find rows faster without scanning the entire table.
Think of it like an index in a book ‚Äî it points you to the page where information is located instead of reading every page.
Without an index ‚Üí Database performs a full table scan (checks every row).
With an index ‚Üí Database uses the index to quickly locate rows matching the condition.

Internally, traditional databases implement indexes using data structures like B-trees or hash tables.
A B-tree index maintains sorted keys and pointers to rows, allowing logarithmic time lookups.
Each insert or update operation must also maintain this index structure, which is why indexes speed up reads but can slow down writes
When you insert, update, or delete rows:
The DB must also update the index structure.
This is why write-heavy tables can get slower with too many indexes.
The indexed key is passed through a hash function.
The function returns a hash value that points directly to the data location.
Lookup time is O(1) on average (very fast for equality searches
____________________________________________________________


| Question                                                | Answer  | Explanation                                                     |
| ------------------------------------------------------- | ------- | --------------------------------------------------------------- |
| 1Ô∏è‚É£ Indexes improve the speed of data retrieval.        | ‚úÖ True  | Indexes make searches faster by avoiding full table scans.      |
| 2Ô∏è‚É£ Indexes slow down data insertion and updates.       | ‚úÖ True  | Because the index must be updated whenever data changes.        |
| 3Ô∏è‚É£ You can create an index on multiple columns.        | ‚úÖ True  | That‚Äôs called a **composite index**.                            |
| 4Ô∏è‚É£ Indexes reduce storage space.                       | ‚ùå False | They actually **consume extra space** in the database.          |
| 5Ô∏è‚É£ Indexes are automatically created on every column.  | ‚ùå False | You must explicitly create them (except for primary keys).      |
| 6Ô∏è‚É£ A primary key automatically creates a unique index. | ‚úÖ True  | Most databases do this under the hood.                          |
| 7Ô∏è‚É£ Too many indexes always improve performance.        | ‚ùå False | Too many indexes can **slow down writes** and use extra memory. |

Internally, traditional databases implement indexes using data structures like B-trees or hash tables.
A B-tree index maintains sorted keys and pointers to rows, allowing logarithmic time lookups.
Each insert or update operation must also maintain this index structure, which is why indexes speed up reads but can slow down writes
______________________________________________________________________________

WINDOW Functions: ÔÇß Provided use cases for RANK(), DENSE_RANK() for ordering data within partitions

Window functions are used to perform calculations across a set of rows that are related to the current row ‚Äî
without collapsing them into a single result like aggregation does

RANK() gives the same rank to ties, but skips the next number (notice both 400‚Äôs get rank 1, and the next rank would be 3).
DENSE_RANK() does not skip ranks after a tie.
So if two rows share rank 1, the next is 2 (not 3).





_______________________________________________________________________________________

SELF JOIN Applications: ÔÇß Discussed scenarios like finding manager-employee relationships within the same table

A SELF JOIN is when a table is joined to itself ‚Äî
you treat the same table as two different tables using aliases.
This is useful when rows within the same table are related to each other ‚Äî for example:

SELECT 
    e.emp_name AS employee,
    m.emp_name AS manager
FROM employees e
LEFT JOIN employees m
ON e.manager_id = m.emp_id;


________________________________________

HAVING vs WHERE:  Explained how WHERE filters rows before aggregation, whereas HAVING filters groups post-aggregation
explain this 

WHERE is used to filter individual rows before any grouping or aggregation happens.
HAVING is used to filter aggregated results after GROUP BY has been applied.


_____________________________________________________________________________________________________________________________________
Both Delta Lake and Parquet are file formats used in data lakes, but they serve different purposes.
Parquet is a storage format, while Delta is a storage layer built on top of Parquet that adds reliability, transaction support, and versioning

Parquet ‚Äì Data Storage Format
Parquet is a columnar storage format.
It‚Äôs highly efficient for read-heavy analytics workloads.
It supports compression and encoding schemes for better performance.
It has compression technique that why size is very low 
for coulmnar format if we are looking for aggregation we will choose only those particular column rest column we will not choose
so this is more optimized
Columnar formats store each column separately, so during aggregations we scan only the required columns instead 
of full rows. This reduces data read from storage and gives huge performance improvements


However, it is immutable ‚Äî once written, you can‚Äôt easily update or delete records.
It lacks ACID transactions and data versioning.
‚úÖ Example use case: Great for storing large, append-only datasets for analytics (like logs or historical data).


| Feature            | Parquet              | Delta Lake                           |
| ------------------ | -------------------- | ------------------------------------ |
| Storage Type       | Columnar file format | Storage layer built on Parquet       |
| ACID Transactions  | ‚ùå No                 | ‚úÖ Yes                                |
| Schema Enforcement | ‚ùå No                 | ‚úÖ Yes                                |
| Updates/Deletes    | ‚ùå Hard               | ‚úÖ Supported                          |
| Time Travel        | ‚ùå No                 | ‚úÖ Yes                                |
| Streaming Support  | ‚ùå No                 | ‚úÖ Yes                                |
| Metadata Handling  | Hive Metastore       | Delta transaction log (`_delta_log`) |
| Use Case           | Raw data storage     | Reliable data lake / Lakehouse       |

______________________________________________________________________________________________________

Delta Lake is an open-source storage layer that sits on top of Parquet files.
It adds features that Parquet alone doesn‚Äôt provide, such as:
ACID transactions (ensures reliability during concurrent writes)
Schema evolution and enforcement
Time travel (query old versions of data)
Merge, update, delete support
Streaming + batch unification
______________________________________________________________________________________________________


