Ingesting Data (Batch vs. Streaming)
Create Table (Batch Load - Non-Incremental)
CREATE OR REFRESH LIVE TABLE table_name
AS SELECT * FROM read_files(
  'path/to/volume/files',
  format => 'csv',
  header => true
);

____
Copy Into (Incremental Batch) Loads only new files from a directory into a Delta table. Note that this is usually run manually or via a workflow rather than inside a DLT pipeline.
COPY INTO delta.`/path/to/target_volume`
FROM '/path/to/source_volume'
FILEFORMAT = CSV
FORMAT_OPTIONS ('header' = 'true');
_____________
Autoloader (Streaming Incremental Load) The most powerful method for continuous ingestion.

SQL
CREATE OR REFRESH STREAMING LIVE TABLE table_name
AS SELECT * FROM cloud_files(
  'path/to/volume/files',
  'csv',
  map('header', 'true', 'inferColumnTypes', 'true')
);
______--
Dataset Types (Building Blocks)
Streaming Table (Live Feed) [31:36] Used for the "Landing" or "Bronze" layer to catch new data incrementally.

CREATE OR REFRESH STREAMING LIVE TABLE raw_data_table
COMMENT 'Ingesting raw data incrementally'
AS SELECT * FROM cloud_files(
  '/Volumes/catalog/schema/volume/folder',
  'csv',
  map('header', 'true', 'inferColumnTypes', 'true')
);
_____________
Materialized View (Snapshot/Aggregated)
CREATE OR REFRESH LIVE TABLE customer_summary
COMMENT 'Aggregated customer snapshot'
AS SELECT 
    customer_id, 
    sum(transaction_amount) as total_spent 
FROM live.raw_data_table
GROUP BY customer_id;
____________
Temporary View (Scratchpad)

CREATE OR REFRESH LIVE VIEW filtered_data_view
AS SELECT * FROM live.streaming_table_name
WHERE status = 'active';
____________________________



## 1Ô∏è‚É£ What are the three layers of the Lakeflow pipeline system?

**Answer:**
Lakeflow organizes data engineering into three layers:

1. **Connect (Ingestion):** Used to ingest data from external systems like Salesforce, SAP, files, or partner tools such as Fivetran.
2. **Transform (ETL / DLT):** This is where Declarative Pipelines live. We clean, transform, aggregate, and enforce data quality using SQL or Python.
3. **Orchestrate (Jobs):** Used to schedule pipelines, manage dependencies, triggers, alerts, and downstream tasks like dashboards or ML jobs.

---

## 2Ô∏è‚É£ What is the difference between Batch and Streaming ingestion?

**Answer:**

* **Batch processing** handles data in fixed intervals (daily/hourly). It is simpler but introduces latency.
* **Streaming processing** ingests data continuously as it arrives, enabling near real-time analytics but requiring more monitoring.

Lakeflow supports **both in a unified way** using the same pipeline.

---

## 3Ô∏è‚É£ What are the different ways to ingest data into DLT using SQL?

**Answer:**
There are three main ingestion methods:

1. **CREATE LIVE TABLE + read_files** ‚Üí Full batch reload
2. **COPY INTO** ‚Üí Incremental batch load (outside DLT)
3. **Autoloader (cloud_files)** ‚Üí Streaming incremental ingestion

Each method is chosen based on data volume, frequency, and latency needs.

---

## 4Ô∏è‚É£ When should you use `CREATE OR REFRESH LIVE TABLE`?

**Answer:**
This method is best for **small, static, or reference datasets**.
It performs a **full reload every time the pipeline runs**, meaning it is **not incremental**.

Example use cases:

* Country codes
* Product master data
* Lookup tables

---

## 5Ô∏è‚É£ Why is `CREATE TABLE` not incremental in DLT?

**Answer:**
Because `read_files()` reads **all files every time the pipeline runs**, DLT cannot track what has already been processed. As a result, the entire dataset is reloaded.

---

## 6Ô∏è‚É£ What is `COPY INTO` and how does it differ from DLT ingestion?

**Answer:**
`COPY INTO` is an **incremental batch ingestion** mechanism that loads **only new files** and skips already processed files.

Key difference:

* `COPY INTO` is **not automatically orchestrated by DLT**
* It must be run manually or scheduled via **Jobs / Workflows**

---

## 7Ô∏è‚É£ Why is `COPY INTO` usually not used inside DLT pipelines?

**Answer:**
Because DLT already provides **built-in incremental ingestion and checkpointing** using Autoloader.
`COPY INTO` is better suited for **one-time or scheduled batch ingestion**, not continuous declarative pipelines.

---

## 8Ô∏è‚É£ What is Autoloader and why is it preferred?

**Answer:**
Autoloader is a **streaming file ingestion mechanism** built on Spark Structured Streaming.

Key advantages:

* Automatically detects new files
* Handles schema drift
* Scales efficiently
* Fault-tolerant
* Incremental by default

This makes it the **recommended ingestion method** for most production pipelines.

---

## 9Ô∏è‚É£ What SQL keyword makes a table streaming in DLT?

**Answer:**
The keyword **`STREAMING`**.

```sql
CREATE OR REFRESH STREAMING LIVE TABLE table_name
```

Without `STREAMING`, the table becomes a materialized view.

---

## üîü What is the difference between `read_files` and `cloud_files`?

**Answer:**

| Feature         | read_files  | cloud_files          |
| --------------- | ----------- | -------------------- |
| Processing Type | Batch       | Streaming            |
| Incremental     | ‚ùå No        | ‚úÖ Yes                |
| Schema Drift    | ‚ùå Limited   | ‚úÖ Supported          |
| Use Case        | Static data | Continuous ingestion |

---

## 1Ô∏è‚É£1Ô∏è‚É£ Explain Lakeflow using the ‚ÄúPizza‚Äù analogy.

**Answer:**
DLT is declarative like ordering food at a restaurant.
You say **‚ÄúI want a pizza‚Äù (what)**, not how to knead the dough or bake it (how).
Similarly, in DLT, we define *what data we want*, and Databricks handles orchestration, retries, scaling, and checkpoints.

---

## 1Ô∏è‚É£2Ô∏è‚É£ What are the three dataset types in DLT?

**Answer:**
DLT provides three dataset types:

1. **Streaming Table**
2. **Materialized View**
3. **Temporary View**

Each serves a different role in the pipeline.

---

## 1Ô∏è‚É£3Ô∏è‚É£ What is a Streaming Table? (CCTV analogy)

**Answer:**
A streaming table behaves like a **CCTV camera**‚Äîit continuously captures new data as it arrives.

* Incremental
* Appends only new records
* Typically used in **Bronze / Landing layer**

---

## 1Ô∏è‚É£4Ô∏è‚É£ What is a Materialized View? (Photo album analogy)

**Answer:**
A materialized view is like a **photo album snapshot**.

* Stores computed results
* Refreshes when pipeline runs
* Represents the **current full state**
* Used in **Gold / Analytics layer**

---

## 1Ô∏è‚É£5Ô∏è‚É£ What is a Temporary View? (Rough sketch analogy)

**Answer:**
A temporary view is a **scratchpad**.

* Not persisted to storage
* Used for intermediate transformations
* Helpful for debugging and joins

---

## 1Ô∏è‚É£6Ô∏è‚É£ How do Streaming Tables behave across pipeline runs?

**Answer:**

* **First run:** Reads all existing data
* **Subsequent runs:** Reads **only new incoming records**

This makes them efficient and cost-effective.

---

## 1Ô∏è‚É£7Ô∏è‚É£ How do Materialized Views behave across pipeline runs?

**Answer:**
Materialized views **recompute the result** to represent the **current state of the data**, combining old and new records.

Example:

* 900 existing records
* 200 new records
* Result = 1100 total records

---

## 1Ô∏è‚É£8Ô∏è‚É£ Can a DLT pipeline mix batch and streaming tables?

**Answer:**
Yes.
DLT supports **unified batch and streaming pipelines**, where streaming tables can feed materialized views seamlessly.

---

## 1Ô∏è‚É£9Ô∏è‚É£ Where are Streaming Tables typically used in a Medallion Architecture?

**Answer:**
Streaming tables are typically used in the **Bronze / Landing layer**, where raw data is ingested incrementally and stored with minimal transformation.

---

## 2Ô∏è‚É£0Ô∏è‚É£ Give a one-line summary of Streaming Table vs Materialized View.

**Answer:**

* **Streaming Table:** Captures only new data incrementally
* **Materialized View:** Represents the complete current state of data

---




---

## 1Ô∏è‚É£ What problem do Lakeflow Declarative Pipelines (DLT) solve?

**Answer:**
Lakeflow Declarative Pipelines solve the operational complexity of traditional data pipelines. Earlier, engineers had to manually manage checkpoints, orchestration, backfills, schema changes, and infrastructure. DLT allows engineers to **declare what transformations they want**, and Databricks automatically handles checkpointing, incremental processing, dependency management, retries, and scaling.

---

## 2Ô∏è‚É£ Why are they called *Declarative* pipelines?

**Answer:**
They are called declarative because the engineer only specifies **what the final tables should look like**, not **how to execute each step**. Databricks automatically figures out execution order, parallelism, retries, and optimizations like incremental computation.

---

## 3Ô∏è‚É£ How did pipeline development work before DLT?

**Answer:**
Before DLT:

* Checkpoints had to be manually configured
* Full table recomputation was common even for small data changes
* DAGs and dependencies were manually managed
* Backfills and late data required custom logic
* Data quality checks were ad hoc
* Infrastructure and Spark tuning were manually handled

This resulted in brittle pipelines and high operational overhead.

---

## 4Ô∏è‚É£ How does DLT handle checkpointing and failures?

**Answer:**
DLT automatically manages checkpoints internally. If a pipeline fails, it restarts from the last successful checkpoint, ensuring **exactly-once processing** and avoiding duplicate or missing data without manual intervention.

---

## 5Ô∏è‚É£ What is incremental processing in Lakeflow?

**Answer:**
Incremental processing means DLT processes **only new or changed data**, instead of recomputing the entire dataset. This significantly improves performance and reduces compute cost.

---

## 6Ô∏è‚É£ How does DLT manage dependencies between tables?

**Answer:**
DLT automatically builds the **DAG (Directed Acyclic Graph)** by analyzing live table references in the code. If one table depends on another, Databricks ensures the correct execution order and parallelism without manual orchestration.

---

## 7Ô∏è‚É£ How does Lakeflow handle late-arriving data?

**Answer:**
Lakeflow natively supports late-arriving data by reprocessing only affected records using its incremental and checkpoint-based execution model. No separate backfill jobs are required.

---

## 8Ô∏è‚É£ How is schema evolution handled in DLT?

**Answer:**
DLT supports **automatic schema evolution** on Delta Lake. Schema changes do not break downstream pipelines, and Databricks handles compatibility safely without requiring manual fixes.

---

## 9Ô∏è‚É£ How does DLT ensure data quality?

**Answer:**
DLT allows defining **data quality rules (expectations)** directly in the pipeline using `EXPECT` or `CONSTRAINT`. Invalid records can be dropped, logged, or allowed based on configuration, ensuring data quality is enforced inline.

---

## üîü What role does Unity Catalog play in Lakeflow?

**Answer:**
Unity Catalog provides:

* Centralized governance
* Fine-grained access control
* End-to-end lineage tracking
* Auditing and data discovery

It integrates directly with DLT, making pipelines secure and compliant by default.

---

## 1Ô∏è‚É£1Ô∏è‚É£ What is the underlying storage layer for DLT?

**Answer:**
DLT is built on **Delta Lake**, which provides ACID transactions, schema evolution, time travel, and reliable streaming and batch unification.

---

## 1Ô∏è‚É£2Ô∏è‚É£ What execution engine does Lakeflow use?

**Answer:**
Lakeflow runs on **Apache Spark** and uses **Photon**, Databricks‚Äô vectorized execution engine, for faster SQL performance and optimized query execution.

---

## 1Ô∏è‚É£3Ô∏è‚É£ Explain the three-layer architecture of Lakeflow.

**Answer:**
Lakeflow follows a **three-layer architecture**:

1. **Connect (Ingestion Layer):** Ingests data from sources using connectors and Autoloader
2. **Declarative Pipelines (Transform Layer):** Applies transformations, data quality, and business logic
3. **Jobs (Orchestration Layer):** Schedules, monitors, and orchestrates pipeline execution

---

## 1Ô∏è‚É£4Ô∏è‚É£ What is Autoloader and where does it fit?

**Answer:**
Autoloader belongs to the **Connect (Ingestion)** layer. It incrementally ingests files from cloud storage, handles schema inference, schema evolution, retries, and deduplication automatically.

---

## 1Ô∏è‚É£5Ô∏è‚É£ How is orchestration handled in the new Databricks platform?

**Answer:**
Orchestration is handled through **Jobs (Workflows)**. Jobs allow scheduling pipelines, chaining tasks (like dashboards after pipelines), configuring retries, and setting alerts for failures or success.

---

## 1Ô∏è‚É£6Ô∏è‚É£ What languages can be used to build DLT pipelines?

**Answer:**
DLT pipelines can be built using **SQL or Python**. SQL is commonly preferred for declarative, transformation-heavy pipelines.

---

## 1Ô∏è‚É£7Ô∏è‚É£ How does DLT reduce operational overhead?

**Answer:**
DLT removes the need for:

* Manual cluster management
* Spark tuning
* Retry logic
* Dependency tracking
* Checkpoint configuration

Everything is handled by Databricks‚Äô managed or serverless infrastructure.

---

## 1Ô∏è‚É£8Ô∏è‚É£ What is the main benefit of using Jobs with DLT?

**Answer:**
Jobs ensure pipelines run **reliably and in the correct sequence**, support scheduling, monitoring, retries, and allow integration with dashboards, ML workflows, and downstream tasks.

---

## 1Ô∏è‚É£9Ô∏è‚É£ Can DLT support both batch and streaming?

**Answer:**
Yes. DLT supports **batch and streaming pipelines** using the same declarative code, powered by Delta Lake and Spark Structured Streaming.

---

## 2Ô∏è‚É£0Ô∏è‚É£ Give a one-line summary of Lakeflow Declarative Pipelines.

**Answer:**
Lakeflow Declarative Pipelines let data engineers **declare transformations and data quality rules**, while Databricks automatically manages execution, scaling, reliability, and governance.

---

If you want next:

* üî• **SQL-specific DLT interview questions**
* üî• **Scenario-based questions (banking / real-world pipelines)**
* üî• **Hands-on coding questions with DLT SQL**

Just tell me which one you want.
