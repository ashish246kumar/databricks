https://www.youtube.com/watch?v=aDCfzM7Cs7c&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=8
Read Data in Spark
Video mein bataya gaya hai ki data ko Spark mein kaise read karte hain. Iska ek core structure hota hai DataFrameReader API ke through.
1. General Format: Data read karne ke liye hum spark.read use karte hain, aur uske baad alag-alag options aate hain:
    ◦ .format(): Yahan hum file format batate hain, jaise CSV, JSON, ya Parquet. Agar hum .format() nahi dete hain, toh by default Spark parquet format samajhta hai.
    ◦ .option(): Isme hum key aur value pass karte hain (jaise header, mode). Ye optional field hai, hum de bhi sakte hain aur nahi bhi.
    ◦ .schema(): Agar humein manually batana hai ki column integer hoga ya string, toh hum schema define kar sakte hain.
    ◦ .load(): Yahan humein batana hota hai ki data kahan reside kar raha hai (data ka path/location).
2. Read Modes in Spark: Video mein explain kiya gaya hai ki data read karte waqt 3 tarike ke modes hote hain, aur interview mein isse question aate hi aate hain:
    ◦ Fail Fast: Jaise hi koi corrupted record (malformed record) mile, execution ko fail kar do.
    ◦ Drop Malformed: Ye corrupted record ko seedha drop kar dega.
    ◦ Permissive: Ye default mode hota hai. Agar koi corrupt record milega, toh ye us value ko null set kar dega.
_____________________________________________________________________________________
https://www.youtube.com/watch?v=7x9GXt6-AxU&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=7
**Spark Ecosystem ka Structure**
Video mein Spark ke structure ko do hisson mein baant kar samjhaya gaya hai:

*   **Low Level API (Spark Core):** Spark ka jo niche wala section hai, use "Spark Core" bolte hain. Isme RDD (Resilient Distributed Datasets) aata hai. Agar humein bilkul hi low level par jaakar kuch likhna hai, ya koi aisi cheez achieve karni hai jo High Level API se nahi ho pa rahi, toh wahan par hum RDD ka use karte hain.
*   **High Level API (Libraries):** Jo top mein hai, wo "Set of Libraries" hai jise "High Level API" bhi bolte hain. Isme Data Frame aur Spark SQL aata hai jo Spark Core ke upar run karta hai. Iske alava agar streaming karni hai, machine learning karni hai, ya graph wali cheezein karni hain (GraphX), toh uski libraries bhi available hain.
*   **Kaise Run hota hai:** Hum jo bhi kuch karne wale hain (High Level API mein), wo internally transform hoga Spark ke Core mein (RDD mein) aur phir Spark ke Computer Engine mein jayega.
*   **Languages:** Spark ne flexibility di hai ki aap apne code ko multiple languages mein likh sakte ho jaise Python, Java, R, aur Scala.

**Hardware aur Cluster Manager**
YouTuber ne samjhaya hai ki Spark na hi storage karta hai aur na hi Spark ka khud ka kuch cluster manager hota hai.
*   Sabse base par "Hardware" ya "Computer Machines" baithe hote hain.
*   In machines ko manage karne ke liye inke upar "Cluster Manager" baitha hota hai jo hardware se baat karta hai.
*   Cluster Manager ke kuch popular naam hain: YARN (Yet Another Resource Negotiator), Mesos, aur Standalone Cluster.

**Example (100 GB Calculation)**
Video mein process ko run karne ka example diya gaya hai ki kaise Spark Engine aur Cluster Manager interact karte hain:

1.  Spark Engine Cluster Manager ke paas jayega aur bolega ki humein "20 GB ki 4 Executor" chahiye aur "ek 20 GB ka Driver" chahiye.
2.  Total requirement calculation:
    *   Executors: $20 \times 4 = 80$ GB
    *   Driver: $20$ GB
    *   **Total = 100 GB**.
3.  Cluster Manager check karega ki uske paas 100 GB available hai ki nahi.
4.  Agar memory hai, toh wo process ko run karne ke liye bhejega. Agar nahi hai, toh wo bolega ki hum queue mein rakh dete hain jo "FIFO order" (First In First Out) mein chalta hai. Matlab jo pehla application run hoga, use memory pehle mil jayega.
______________________________________________________________________________
https://www.youtube.com/watch?v=mpMvOPhNtjA&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=5
Hadoop vs Spark

**1. Market mein Misconceptions (Galatfehmiyan)**
Video ke starting mein 3 main misconceptions batayi gayi hain:
*   **Hadoop Database nahi hai:** "Hadoop aisa koi database nahi hai yeh framework hai... Log Hive use karte hain to unhein lagta hai ki yeh relational database hai... lekin actual mein wo ek file system hota hai".
*   **Spark Speed:** "Spark jo hai wo 100 times faster hota hai lekin ye actual aisa nahi hota hai... yeh 100 times tak ja sakta hai, 'up to' hota hai".
*   **RAM Usage:** "Teesri misconception hai wo hai ki Spark jo hai wo RAM mein data ko process karta hai lekin Hadoop aisa nahi karta hai... Hadoop ko bhi RAM ki hi zarurat hoti hai".

**2. Performance Difference (Kaise kaam karte hain)**
*   **Hadoop Process:** "Hadoop is slower than Spark... Hadoop ko bhi RAM ki hi zarurat hoti hai... lekin usko baad mein usko write kar deta hai disk par... to jo RAM se disk write karne mein time jata hai wahan se slow ho jata hai". Example ke liye, Mapper load kiya, reduce kiya aur fir wapas disk pe write kiya. Fir agla process disk se read karega. Ye baar-baar Read/Write cycle chalta hai.
*   **Spark Process:** "Spark ke case mein kya ho raha hai ki... alag-alag executor isko read kar liya... write karega kisi apne executor ke... memory hoga jo ki basically RAM hai... to basically yeh kya kar raha hai ki yeh kabhi disk par gaya hi nahi... iski wajah se iska process fast ho gaya".
*   **Exception Case:** Agar data cluster mein wahin par fit ho ja raha hai (jaise 10GB ka data hai aur sirf count karna hai), toh "uss case mein... time ke case mein... 100 times faster ya 100 times slower hai aisa case to nahi aayega", hardley 1-2 times ka difference hoga,.

**3. Batch vs Streaming**
*   **Hadoop:** "Hadoop build kiya hai batch data processing ke liye... us samay... data bahut zyada volume mein generate hona shuru ho gaya hai... to wo batch mein process karne ke liye banaya gaya tha".
*   **Spark:** "Spark ke paas advantage hai... ye batch ko to handle kar hi sakta hai uske alawa ye streaming ko bhi handle kar sakta hai... kyunki humein jaldi-jaldi data ko process karna hai",.

**4. Ease of Use (Code likhna)**
*   **Hadoop:** "Hadoop mein kya hai ki code likhna bahut hi mushkil tha starting se hi... thoda tough hai likhna".
*   **Spark:** "Spark mein aur bhi easy bana diya gaya hai... Spark ne High Level API de rakha hai ki yahan se aap apne alag-alag language ke through likh sakte ho... Java, SQL, Python, R ye saare language ke through".

**5. Security**
*   **Hadoop:** "Security mein Hadoop ko zyada security hai". Ye "Kerberos authentication ka use karta hai" check karne ke liye ki user authorized hai ya nahi,. Folders access karne ke liye ye "ACLs (Access Control Lists)" use karta hai.
*   **Spark:** "Spark directly aisa koi humein security ke liye nahi deta hai... Spark jo hai hamara wo kya karta hai ki wo HDFS ka storage use kar leta hai... aur jaise hi YARN use karega apna resource negotiate karne ke liye to isko Kerberos mil jata hai".

**6. Fault Tolerance (Video wala Example)**
Manish ne Fault Tolerance samjhane ke liye ye example diya:

*   **Hadoop Example (Replication):**
    Man lo ek data hai "260 MB ka... 128 MB ka ek block size banta hai... to iske kitne banenge teen banenge... A, B aur C". Hadoop replication factor use karta hai (jaise ki 3). "A jo hai wo teen jagah pe store hoga alag-alag node pe... Machine 1, Machine 2 aur ye Machine 3".
    **Agar fail hua to:** "Maan lo ki hum data read kar rahe the Node 1 se... ab ye fail ho gaya... to A ko dekhega ki achha A ka data yahan pe bhi pada hua hai (dusre node par)... koi ek data ko read kar lega... iss tarah se hamara Hadoop fault tolerate hota hai",.

*   **Spark Example (DAG & Lineage):**
    "Spark kya karta hai ki Spark jo hai na wo DAG banata hai Directed Acyclic Graph". Iske paas RDD hota hai. "Isko pata hai ki process 1 ke baad fail hua hai... to process 1 ko banane ke liye kya laga tha kahan se data laga tha wo iske paas information hoti hai... wo us tarike se us data ka use karke wapas bana lega".

____________________________________________________________________________________
https://www.youtube.com/watch?v=N3vk5i_Hh78&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=3
why apache spark

**1. Apache Spark ke aane se pehle kya tha? (Databases)**
Video mein bataya gaya hai ki Spark ke aane se pehle hamare paas **Databases** hote the (jaise Oracle, Teradata, MySQL). Ye systems sirf **Structured Format** wale data ko store karte the.
*   **Example:** Structured data ka matlab hai **Tabular format** mein data, jisme fixed rows aur columns hote hain. Isko aap apne **Excel sheet** jaisa samajh sakte ho.

**2. Problem kahan shuru hui? (Data Formats and Internet)**
Internet ke invention ke baad data alag-alag formats mein generate hone laga:
*   **Files:** Text files, CSV files.
*   **Multimedia:** Images, Videos.
*   **Semi-Structured Data:** JSON ya YAML format (jahan fixed structure nahi hota, key-value pairs hote hain),.
Purane databases sirf tabular data handle kar pa rahe the, baaki formats (Unstructured aur Semi-structured) ko handle karne ke liye koi system nahi tha.

**3. Big Data aur "Three Vs"**
Yahan "Big Data" ke concept ko samjhaya gaya hai **Three Vs** ke through:
*   **Volume:** Sirf size (jaise 100GB ya 1TB) hone se wo Big Data nahi ban jata.
*   **Velocity:** Data kis speed se aa raha hai.
    *   **Example:** Agar 10TB data **per second** ya **per hour** aa raha hai, tab hum bol sakte hain ki Big Data ki problem hai.
*   **Variety:** Data ke alag-alag forms (Structured, Semi-structured, Unstructured). Aaj kal market mein **Unstructured data** sabse zyada produce hota hai.

**4. ETL vs. ELT (Data Handling ka change)**
Pehle hum **ETL** (Extract, Transform, Load) karte the (Data Warehouse concept). Lekin ab data ka volume aur velocity itna badh gaya hai ki hum **ELT** (Extract, Load, Transform) karte hain.
*   Iska matlab pehle hum source se data utha ke ek dump jagah (Data Lake) pe **Load** kar lete hain, aur baad mein jab time milta hai tab usko **Transform** karte hain,.

**5. Main Issues: Storage and Processing**
Itna zyada data aane se do badi problems aayi:
1.  **Storage:** Itna data store kahan karein?
2.  **Processing:** Data ko process karne ke liye RAM aur CPU chahiye,.

**6. Solution Approaches: Monolithic vs. Distributed**
In problems ko solve karne ke liye do options the:

*   **Monolithic Approach (Vertical Scaling):**
    *   Isme hum ek hi system ko bada banate hain (jaise hard disk ya CPU add karte jana).
    *   **Problem:** Iski ek limit hoti hai (Heat dissipation issue, performance drop). Ye **Expensive** hota hai aur isme **Single Point of Failure** hota hai (agar system fail hua to sab band),.

*   **Distributed Approach (Horizontal Scaling):**
    *   Isme hum multiple saste computers (commodity hardware) ko add karte hain.
    *   **Benefits:**
        *   Ye **Economical** (sasta) padta hai.
        *   **Unlimited Scaling:** Hum jitne chahein computers add kar sakte hain.
        *   **High Availability:** Agar ek machine fail ho gayi, to doosri machine kaam sambhal leti hai,.

**Conclusion:**
Isi distributed approach ki wajah se pehle **Hadoop** aaya aur uske baad **Spark** aaya taaki hum Big Data ki storage aur processing problems (Velocity, Variety, Volume) ko solve kar sakein.
____________________________________________________________________________________________________
https://www.youtube.com/watch?v=xW_GNBDW568&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=2
spark overview 

**What is Apache Spark?**
Video mein definition di gayi hai: "Apache Spark is a unified computing engine and set of libraries for parallel data processing on a computer cluster". Is definition ko samajhne ke liye YouTuber ne har term ko explain kiya hai:

**1. Unified**
*   **Explanation:** Unified ka matlab hota hai "sab cheez ko ek jagah la dena."
*   **Example:** Video mein example diya hai ki Data Engineer (pipeline banane wala), Data Analyst (sales analysis karne wala), aur Data Scientist (future predict karne wala)—agar ye teeno "ek hi software ya ek hi jagah pe kaam kar sakte hain, toh usi ko bolte hain Unified." Spark mein ye sab log ek hi computing engine par kaam kar sakte hain,.

**2. Computing Engine**
*   **Explanation:** Spark data ko store nahi karta hai. "Basically ye sara cheez RAM mein ho raha hota hai," jo ki permanent storage nahi hai.
*   **Example:** Jaise hamare laptop mein CPU task perform karta hai. Example diya ki "2+5 = 7... ye humein jo compute karne wali cheez hai wo kaun dega? Wo humein dega Spark." Agar 5 Terabyte ka data hai aur usme +1 karna hai, toh wo Spark compute karega. Storage ke liye S3, HDFS, ya RDBMS use hota hai kyunki Spark ke paas storage nahi hota,.

**3. Set of Libraries**
*   **Explanation:** Ye "set of code hota hai jo ki humein likh kar de diye jaate hain."
*   **Example:** Jaise aapne "Pandas" suna hoga, waise hi Spark mein libraries hoti hain jiska use karke hum aage ke kaam perform karte hain.

**4. Parallel Data Processing**
*   **Explanation:** Ek task ko multiple logon mein divide karke karwana.
*   **The Father-Son Example:** Yahan ek example explain kiya hai: "Maan lo ki ek baap hai, uske char bete hain. Ab baap ke paas total 10 task hain... toh wo kya karega do-do ya teen-teen task de dega do beton ko." Is tarah 10 task char beton mein divide ho gaye. "Charon bete apne apne independent task karenge aur shaam ko aakar result de denge." Agar baap akele karta toh time lagta, par beton ne mil kar jaldi kar diya. Same cheez Spark mein hoti hai jahan task alag-alag executors par bhej diya jata hai,.

**5. Computer Cluster**
*   **Explanation:** Spark "Master-Slave architecture" par kaam karta hai. "Ek machine jo ban gaya wo Master ban gaya aur baaki sare jo machines hain wo hamare ban gaye Slave (naukar)."
*   **Architecture Example:** Video mein assume kiya hai ki har computer (worker) ke paas "16GB ka RAM hai aur 1 Terabyte ka storage hai aur 4 core CPU hai."
*   **Process:** Agar Master ko 5 Terabyte data process karna hai, toh wo dekhega ki workers ke paas kitni capacity hai (jaise total 64GB ek baar mein). Master ka kaam hai data ko divide karna aur workers se kaam karwana,,.

Finally, video mein kaha gaya hai ki Spark ek "Unified Computing Engine hai... yaad rakhna storage nahi hota iske paas" aur ye parallel data processing karta hai computer clusters par.
_________________________________________________________
Spark SQL Engine
In this lecture (Lec-8), Manish Kumar explains the Spark SQL Engine (also known as the Catalyst Optimizer) and the four phases it uses to convert high-level Spark code (SQL, DataFrames, or DataSets) into low-level Java bytecode for execution on a cluster.

1. The Spark SQL Engine Flow
The video breaks down the conversion of your code into the following four sequential phases:

Phase 1: Analysis & The Catalog [07:27]
When you write a query, Spark first creates an Unresolved Logical Plan.
It then uses the Catalog (a repository of metadata like table names, column types, and file locations) to verify if the tables or columns you referenced actually exist.
If a column or table is missing, Spark throws an Analysis Exception at this stage [09:13].

Phase 2: Logical Planning / Optimization [10:35]
Once the plan is resolved, Spark applies logical optimizations.
Example: If you write SELECT * but only use two columns later, Spark optimizes the plan to fetch only those two specific columns from the source to save network bandwidth. It also combines multiple filter transformations into one [11:42].

Phase 3: Physical Planning [12:37]
Spark takes the optimized logical plan and generates multiple Physical Plans.
It uses a Cost-Based Model to evaluate these plans. It calculates which plan will use the least amount of CPU and memory.
Example: It decides whether to perform a Broadcast Join (sending a small table to all executors) or a standard shuffle join based on table sizes to avoid expensive data shuffling [12:54].
The best performing plan is selected as the Selected Physical Plan.

Phase 4: Code Generation [06:17]
The final physical plan consists of RDD transformations. Spark uses Whole-Stage Code Generation to convert these into highly optimized Java bytecode that runs on the cluster's executors [14:50].

2. Key Interview Questions Answered [01:40]
What is the Catalyst Optimizer? It is the engine that converts your high-level code into an optimized physical execution plan.
Why do we get an Analysis Exception? This occurs during the first phase if the metadata (table/column names) doesn't match the Catalog [10:06].
What is the Catalog? It is a storage for metadata (data about data), such as file sizes and schema information [08:17].
Is Spark SQL Engine a compiler? Yes, because it ultimately translates high-level Spark code into Java bytecode for the machine to execute [16:39].

3. Example: Analysis Error [09:19]
Manish shows a code snippet where he tries to select a column named name1 from a JSON file, but the actual column name is name. Because the Catalyst Optimizer checks the Catalog during the Analysis phase and finds no match, it throws an error immediately before any data is even processed._______________________________________________________________________
Database vs datalake vs data warehouse
_______________________________________________________________________________________________
. Core Concepts Explained
Database (OLTP):

Purpose: Primarily used for day-to-day transactions (Online Transactional Processing). It focuses on high performance for immediate tasks [01:33].
Data Type: Deals only with structured data (rows and columns) [02:43].
Recent Data: It holds only recent data (e.g., last 5-6 months) to maintain speed; it cannot efficiently store years of historical data [03:12].
Schema: Uses Schema on Write, meaning the structure is validated before data is saved. Any mismatch results in an error during the writing process [04:18].

Cost: The storage cost is high [06:23].
Data Warehouse (OLAP):
Purpose: Used for analytical processing (Online Analytical Processing) and finding insights from large amounts of historical data [07:51].
Source: It aggregates data from multiple databases through an ETL (Extract, Transform, Load) process [14:52].
Schema: Like a database, it uses Schema on Write and deals with structured data [12:41].
Cost: Storage is expensive, though generally cheaper than a primary transactional database [13:16].

Data Lake:
Purpose: Meant to store huge amounts of raw data for later analysis. It offers maximum flexibility for various teams to use the data as they need [18:04].
Data Type: Handles structured, semi-structured, and unstructured data (like log files or docs) in its original form [17:56].
Process: Uses ELT (Extract, Load, Transform). Data is loaded first, and transformations happen only when the data is read [19:00].
Schema: Uses Schema on Read, allowing users to define the structure at the time of analysis [21:01].
Cost: Highly cost-effective as it uses cheap storage hardware [20:16].

2. Examples Provided
Database Example:
Scenario: Swiping your card at an ATM or purchasing an item on Amazon. These are instant transactions where your current balance must be updated immediately [01:16].
Systems: Oracle, MySQL [04:06].
Data Warehouse Example:
Scenario: Amazon analysts wanting to see your purchase trends over the last 10 years to find insights. They move data from the transactional database to a warehouse to run complex queries without slowing down the main site [08:15].
Systems: Teradata [13:16].
Data Lake Example:
Scenario: Storing a raw .csv or log file. You load the file into the system as-is and later create a structure (like a Hive table) on top of it to visualize the data [21:10].
Systems: HDFS (Hadoop Distributed File System), Amazon S3 [19
__________________________________________________________
difference between Partitioning and Bucketing
Partitioning:
How it works: It divides data into multiple folders based on a specific column [03:02].
When to use: Use it when a column has low cardinality (limited distinct values), such as Country or Date [04:10].
Benefit: When you query for a specific value (e.g., Country = 'India'), the system only scans that specific folder instead of the entire dataset [03:21].

Bucketing:
How it works: It divides data into a fixed number of files using a hash function [06:21].
When to use: Use it when a column has high cardinality (many unique values), such as Customer ID or Zip Code [05:44].
Benefit: It distributes data evenly across a specified number of buckets (files) to manage large volumes of data that partitioning alone cannot handle efficiently [07:31].

2. Examples Provided
Partitioning Example [01:43]
Scenario: You have a table with billions of records including a Country column.
The Query: SELECT * FROM table WHERE Country = 'India'.
The Process: * Without partitioning, the system scans all 100 billion records.
With partitioning by Country, the system creates separate folders for India, USA, and UK.
The query goes directly to the India folder, drastically increasing speed [03:02].

Bucketing Example [06:41]
Scenario: You want to organize data by Customer ID (which has millions of unique values).
The Process:
You decide to create 4 buckets.
The system uses a hash function: Customer ID % 4.
If the result is 0, the data goes to Bucket 1; if it's 1, it goes to Bucket 2, and so on [07:07].
This results in exactly 4 files regardless of how many unique Customer IDs exist [07:17].


Partitioning:
SQL
CREATE TABLE zip_codes (...)
PARTITIONED BY (state STRING);
Bucketing:

SQL
CREATE TABLE zip_codes (...)
CLUSTERED BY (zip_code) INTO 10 BUCKETS;


_____________________________________________________________________
What are the differences between Managed Identity and Service Principal, and when should each be used?

Answer: * Managed Identity: * What it is: An identity automatically created and managed by Azure for specific resources
(like Azure Data Factory). [03:15, 01:10:35] * When to use: Use it whenever possible (recommended by Microsoft) 
for authenticating between Azure resources that support it. [13:18] * Advantages: It is highly secure and easy to manage because Azure handles the credentials,
removing the need for manual key rotation. [11:09, 11:18] * Disadvantages: It is less flexible because users have little control over its configuration, 
and it is not supported by all services (e.g., Azure DevOps). [12:31, 07:54]

Service Principal:

What it is: A security identity used by applications, services, and automation tools (created via Azure Entra ID/Active Directory) to access specific Azure resources. [03:51, 10:45]
When to use: Use it when Managed Identity is not supported, or when an external service (like Azure DevOps) needs to access Azure resources. [13:43, 07:39]
Advantages: It is highly flexible; you can create as many as needed and use them across various internal and external services. [12:49, 12:56]
Disadvantages: It is less secure and harder to manage because you must manually handle credentials (secret keys) and rotate them before they expire. [11:35, 11:55]

(
Azure services:

Managed Identity Example (ADF to Databricks):

When you create Azure Data Factory (ADF), Azure automatically creates an identity for it.

To give ADF access to Azure Databricks, you simply go to the Databricks access control and select the "Managed Identity" of that specific Data Factory.

Service Principal Example (Azure DevOps to Databricks):

If you want to use Azure DevOps to run a CI/CD pipeline in Databricks, you cannot use a Managed Identity because Azure DevOps doesn't have one in the same way.

Instead, you must create a "third-party" ID called a Service Principal in Azure Entra ID (Active Directory).

You generate a Client ID and a Secret Key for this Service Principal and then provide those credentials to Azure DevOps so it can "log in" and perform tasks in Databricks
)
________________________________________________________________
copying billion rows from a file to a database and identifies lines that are causing the problem. 
He suggests that the solution is to enable fault tolerance in the copy activity settings and check the "skip the incompatible rows" option. 
This will allow the compatible rows to be pushed into the database, while the incompatible rows can be found in the log
____________________________________________________
https://www.youtube.com/watch?v=C94-000jh6E
Edge Node (Gateway Node): 
An isolated machine outside the main cluster used for submitting Spark jobs. It serves security and organizational purposes:
Prevents direct user access to the cluster to avoid manual data corruption or security risks.
Handles authentication and authorization (e.g., via Kerberos) before jobs are submitted
Deployment Modes: The mode is determined by where the driver program runs.

Client Mode:
The driver runs on the edge node (client machine).
Logs are displayed directly on the user's screen, making it ideal for development and debugging.
Disadvantage: Higher network latency due to two-way communication between the edge node and the cluster.
Disadvantage: If the edge node is shut down or disconnected, the driver dies, and all executors are killed, stopping the process.

Cluster Mode:
The driver runs on one of the worker nodes inside the cluster.
Recommended for production workloads.
Advantage: Lower network latency since the driver and executors are within the same cluster.
Advantage: The edge node can be disconnected after submission without affecting the job, as the driver is already running in the cluster.

_______________________________________________________________________________
Question: How does Spark work? Can you quickly guide me through the Spark architecture? [00:01]

Answer: When you write a Spark application, the driver program gets executed. It follows several steps:
Schema Validation: It checks if the code is syntactically correct and matches the schema catalog. [00:15]
Unresolved Logical Plan: Once validated, it creates an unresolved logical plan. [00:26]
Logical Plan: A logical plan is created with the help of the Catalyst Optimizer, considering cost and roles. [00:39]
Physical Plan: Finally, a physical plan is created and then used for execution. [00:
______________________________________________________________
Question: What exactly is the difference between submitting a job in client mode or in cluster mode in Spark? [00:01]

Answer: - Client Mode: The driver program runs on the client machine [00:09]. This mode allows for interaction with the cluster results 
and is primarily used for development purposes, as it makes it easy to see the execution results from the cluster. [00:15]

Cluster Mode: The driver program runs on the cluster itself [00:25]. This mode is most commonly used for production environments. [00:31]
______________
