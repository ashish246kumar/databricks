Feature,Z-Order,Liquid Clustering
Stability,"Static. If you add data, you must re-run OPTIMIZE to Z-Order the new data.", Dynamic. It is incremental and clusters new data much more efficiently.
Maintenance,High. You have to manage file sizes and decide when to re-optimize.,Low.                 It handles data skew and changing data patterns automatically.
Flexibility,You can't easily change Z-Order columns without rewriting the table.                ,You can change clustering columns using ALTER TABLE without a full rewrite.
Overlap,"Files can still have overlapping data ranges (Min-Max), reducing pruning.",       "Significantly reduces file overlap, leading to better data skipping."
__________________
Data Lake vs Delta Lake vs DLT (Delta Live Tables)


1Ô∏è‚É£ Data Lake (Traditional Data Lake)
What it is

A storage architecture that stores raw data (structured, semi-structured, unstructured) at large scale, usually on low-cost object storage.

Examples

AWS S3

Azure Data Lake Storage (ADLS)

Google Cloud Storage

Key Characteristics

Stores data as-is (raw, curated, aggregated)

Supports schema-on-read

Cheap and scalable

No inherent transaction support

‚ùå Limitations

‚ùå No ACID transactions

‚ùå Data quality issues (duplicate, corrupt data)

‚ùå Schema drift is unmanaged

‚ùå Difficult updates/deletes

‚ùå Concurrency issues

Interview One-liner

‚ÄúA data lake is cheap and scalable storage, but without transactional guarantees or built-in reliability.‚Äù

2Ô∏è‚É£ Delta Lake
What it is

A storage layer on top of a data lake that adds reliability and performance using transaction logs.

üëâ Delta Lake does not replace the data lake ‚Äî it enhances it.

What Delta Lake Adds (This is critical)

‚úÖ ACID transactions

‚úÖ Schema enforcement & evolution

‚úÖ Time travel (versioning)

‚úÖ Upserts / Deletes / MERGE

‚úÖ Concurrent reads & writes

‚úÖ Data reliability

How it works (Important Interview Detail)

Stores data as Parquet files

Maintains a _delta_log transaction log

Every operation is recorded as a transaction

Problems it Solves
Problem in Data Lake	Delta Lake Solution
Partial writes	Atomic commits
Dirty reads	Snapshot isolation
Duplicate ingestion	MERGE
Schema drift	Enforced schemas
Debugging	Time travel
Interview One-liner

‚ÄúDelta Lake brings database-like reliability to a data lake using ACID transactions and a transaction log.‚Äù

3Ô∏è‚É£ DLT (Delta Live Tables)
What it is

A managed pipeline framework built on top of Delta Lake for ETL/ELT automation.

Think of it as:

Delta Lake + Pipeline orchestration + Data quality + Monitoring

What DLT Does (Very Important)

Declarative pipeline definitions

Automatic dependency management

Built-in data quality checks

Automatic retries & recovery

Pipeline monitoring & lineage

Key Concepts

Live Tables ‚Äì tables automatically updated

Streaming + Batch unified

Expectations ‚Äì data quality rules

Materialized views

Example (DLT Expectation)
CONSTRAINT valid_amount EXPECT (order_amount > 0) ON VIOLATION DROP ROW

‚ùå What DLT is NOT

‚ùå Not a storage layer

‚ùå Not a replacement for Spark

‚ùå Not just SQL ‚Äî it‚Äôs a pipeline framework

Interview One-liner

‚ÄúDLT is a managed ETL framework that uses Delta Lake to build reliable, observable, and quality-driven data pipelin
______________________________________
https://www.youtube.com/watch?v=bf7_ek6SqJs&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=19
---

# **Apache Parquet ‚Äì Interview Questions & Answers**

---

## **Q1. What is Parquet and why is it widely used in Data Engineering?**

**Answer:**
Parquet is a **columnar, binary file format** designed for **analytical workloads (OLAP)**.

It is widely used because:

* Data is written once and read many times
* Storage cost is reduced through compression
* Query performance is significantly faster than row-based formats

Parquet cannot be read directly using a text editor because it is a **binary format**.

---

## **Q2. What is the difference between row-based and column-based storage?**

**Answer:**

### **Row-Based Storage (CSV, Text)**

* Data is stored row by row
* Entire rows are read even if only one column is required
* Leads to unnecessary I/O
* Better suited for transactional systems (OLTP)

### **Column-Based Storage (Parquet)**

* Data is stored column by column
* Only required columns are read
* Reduces disk I/O and improves performance
* Ideal for analytical queries (OLAP)

---

## **Q3. Why is Parquet better for Big Data analytics?**

**Answer:**
In analytics, queries typically:

* Read only a few columns
* Scan large datasets

Parquet allows:

* Reading only required columns (**projection pruning**)
* Skipping irrelevant data using metadata (**predicate pushdown**)
* Faster query execution with lower storage cost

---

## **Q4. What is the difference between OLTP and OLAP systems, and how does Parquet fit in?**

**Answer:**

| Feature        | OLTP                   | OLAP               |
| -------------- | ---------------------- | ------------------ |
| Purpose        | Transactions           | Analytics          |
| Operations     | Insert, Update, Delete | Select, Aggregate  |
| Data Access    | Full row               | Few columns        |
| Storage Format | Row-based              | Columnar (Parquet) |

Parquet is designed for **OLAP systems**, not OLTP systems.

---

## **Q5. Is Parquet purely columnar? Explain its internal structure.**

**Answer:**
Parquet uses a **hybrid storage model**.

### **File Structure:**

1. **File** ‚Äì The main container
2. **Row Groups** ‚Äì Horizontal partitions of data

   * Default size is **128 MB**
   * Contains a subset of rows
3. **Column Chunks** ‚Äì Column-wise storage inside a row group
4. **Pages** ‚Äì Actual data storage units

   * Each page contains metadata (min, max, count)

This hybrid approach avoids excessive column jumping and improves performance.

---

## **Q6. What are Row Groups in Parquet and why are they important?**

**Answer:**
Row Groups are **horizontal partitions** of data.

Benefits:

* Allow partial file reads
* Enable skipping large chunks of data
* Improve performance during filtering
* Reduce disk I/O

Spark scans only the required row groups instead of the entire file.

---

## **Q7. What metadata does Parquet store?**

**Answer:**
Parquet is a **self-describing format**.

It stores metadata such as:

* Schema information
* Number of row groups
* Column statistics (min, max, null count)
* Encoding and compression details

This metadata is critical for query optimization.

---

## **Q8. How does Parquet reduce file size?**

**Answer:**
Parquet reduces file size using **encoding techniques** before applying compression.

### **1. Dictionary Encoding**

* Replaces repeated string values with integer keys
* Stores mapping in a dictionary

### **2. Run Length Encoding (RLE)**

* Compresses repeated values
* Example: `A, A, A, B, B` ‚Üí `A3, B2`

### **3. Bit Packing**

* Stores small integers using fewer bits
* Avoids using 32 or 64 bits unnecessarily

After encoding, compression algorithms like **Snappy or Gzip** are applied.

---

## **Q9. What is Projection Pruning in Parquet?**

**Answer:**
Projection pruning means **reading only the columns required by the query**.

Example:

```sql
SELECT name FROM employees
```

Parquet reads only the `name` column and ignores all others.

This significantly reduces disk I/O.

---

## **Q10. What is Predicate Pushdown in Parquet?**

**Answer:**
Predicate pushdown allows Spark to **filter data at the storage level** using metadata.

Before reading data:

* Spark checks min/max values in row group metadata
* If the filter condition cannot be satisfied
* The entire row group is skipped

Example:

```sql
SELECT * FROM table WHERE age < 18
```

If a row group has `min age = 22`, Spark skips it completely.

---

## **Q11. Why is Predicate Pushdown important?**

**Answer:**
Predicate pushdown:

* Avoids reading unnecessary data
* Saves disk I/O
* Improves query performance drastically
* Is one of the biggest advantages of Parquet

---

## **Q12. How does Parquet compare to CSV in performance?**

**Answer:**

| Aspect             | CSV       | Parquet       |
| ------------------ | --------- | ------------- |
| Format             | Row-based | Columnar      |
| Compression        | Poor      | Excellent     |
| Metadata           | None      | Rich metadata |
| Predicate Pushdown | ‚ùå         | ‚úÖ             |
| Projection Pruning | ‚ùå         | ‚úÖ             |
| Performance        | Slow      | Very fast     |

Parquet queries can be **multiple times faster** than CSV.

---

## **Q13. Can Parquet be used without Spark?**

**Answer:**
Yes, Parquet can be read using:

* Spark
* Hive
* Presto
* Trino
* Impala

However, it **cannot be read using a text editor** due to its binary format.

---

## **Q14. When should Parquet NOT be used?**

**Answer:**
Parquet should not be used when:

* Data requires frequent updates or deletes
* Low-latency transactional workloads are needed
* OLTP systems are involved

---

## **Final Interview Tip**

Interviewers expect you to explain:

* **Why Parquet exists**
* **How it stores data**
* **How it optimizes queries**

A strong answer always connects:

> Columnar Storage ‚Üí Metadata ‚Üí Pruning ‚Üí Performance

-______________________________--
https://www.youtube.com/watch?v=bf7_ek6SqJs&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=19
# **Apache Spark ‚Äì Partitioning & Bucketing (Interview Q&A)**

---

## **Q1. What is partitioning in Spark?**

**Answer:**
Partitioning in Spark means **dividing data into separate directories based on the values of one or more columns**.

When we write data using `partitionBy`, Spark creates **folder structures** such as:

```
column=value
```

Each folder contains only the data belonging to that specific column value.

---

## **Q2. Why do we need partitioning?**

**Answer:**
Partitioning is mainly used for **read-time optimization**.

Since big data systems follow the **‚ÄúWrite Once, Read Many‚Äù** principle, partitioning helps:

* Reduce data scanning
* Improve query performance
* Enable **partition pruning**

---

## **Q3. How does partition pruning work?**

**Answer:**
Partition pruning means Spark reads **only the required partitions** instead of scanning the entire dataset.

Example:

```sql
SELECT * FROM employees WHERE address = 'India'
```

Spark directly reads:

```
address=India/
```

and ignores other folders like `address=USA` or `address=Japan`, resulting in faster queries.

---

## **Q4. How do you implement partitioning while writing a DataFrame?**

**Answer:**
Partitioning is implemented using the `partitionBy` method:

```python
df.write
  .partitionBy("address")
  .format("parquet")
  .save("path")
```

Spark creates separate folders for each distinct value in the `address` column.

---

## **Q5. Can we partition by multiple columns?**

**Answer:**
Yes, Spark supports **multi-level partitioning**.

Example:

```python
df.write
  .partitionBy("address", "gender")
  .save("path")
```

This creates a hierarchical structure:

```
address=India/
  gender=Male/
  gender=Female/
```

---

## **Q6. Does the order of partition columns matter?**

**Answer:**
Yes, the order **matters significantly**.

* The first column becomes the **top-level folder**
* The second column becomes a **sub-folder**

The order should be chosen based on **most common filter conditions** used during reads.

---

## **Q7. When does partitioning fail or become inefficient?**

**Answer:**
Partitioning fails when applied to **high-cardinality columns**.

Example:

* Partitioning by `ID` with billions of unique values
* Results in millions or billions of small folders/files
* Causes the **small file problem**
* Degrades performance and metadata handling

---

## **Q8. What is bucketing in Spark?**

**Answer:**
Bucketing divides data into a **fixed number of files (buckets)** using a **hash function** on a column.

Unlike partitioning:

* Bucketing creates **files**, not folders
* Used when data is **high-cardinality or random**

---

## **Q9. When should we use bucketing instead of partitioning?**

**Answer:**
Bucketing should be used when:

* Partitioning is not feasible due to high cardinality
* Columns like **ID or primary key** are used
* Optimized joins and searches are required

---

## **Q10. How do you implement bucketing in Spark?**

**Answer:**
Bucketing is implemented using `bucketBy` along with `saveAsTable`:

```python
df.write
  .bucketBy(5, "id")
  .saveAsTable("employee_bucketed")
```

---

## **Q11. Why is `saveAsTable` mandatory for bucketing?**

**Answer:**
Bucketing metadata (bucket count and column) must be stored in the **Hive Metastore**.

* File-based saves (CSV/Parquet paths) do not store bucket metadata
* Without metadata, Spark cannot apply bucket optimizations

Hence, `saveAsTable` is mandatory.

---

## **Q12. What common mistake leads to too many bucket files?**

**Answer:**
If the DataFrame has many partitions:

* Spark creates **buckets per partition**
* This leads to **partition √ó bucket files**

Example:

* 200 partitions
* 5 buckets
* Results in **1000 files**

---

## **Q13. How do you ensure the correct number of bucket files?**

**Answer:**
You must **repartition the DataFrame to the same number as buckets** before writing:

```python
df.repartition(5)
  .write
  .bucketBy(5, "id")
  .saveAsTable("employee_bucketed")
```

This ensures exactly **5 bucket files** are created.

---

## **Q14. How does bucketing optimize joins?**

**Answer:**
Bucketing enables **shuffle-free joins** when:

* Both tables are bucketed
* On the same column
* With the same number of buckets

Spark directly joins:

```
Bucket 1 ‚Üî Bucket 1
Bucket 2 ‚Üî Bucket 2
```

This avoids expensive network shuffling and significantly improves join performance.

---

## **Q15. How does bucketing improve search performance?**

**Answer:**
Spark applies a **hash function** to the search key.

Example:

* Searching for an ID
* Spark calculates which bucket contains that ID
* Reads only that specific bucket file

Instead of scanning billions of records, Spark scans only a **small subset**, making searches extremely fast.

---

## **Q16. What is bucket pruning?**

**Answer:**
Bucket pruning allows Spark to:

* Identify the exact bucket file
* Read only the required bucket
* Skip scanning unnecessary files

This is similar to partition pruning but works at the **file level**.

---

## **Q17. Can we combine partitioning and bucketing?**

**Answer:**
Yes, Spark allows combining both.

Example:

* Partition by `date`
* Bucket by `id`

This provides:

* Partition pruning for date filters
* Bucket pruning and optimized joins on ID

---

## **Q18. What are the key differences between partitioning and bucketing?**

| Feature      | Partitioning            | Bucketing                           |
| ------------ | ----------------------- | ----------------------------------- |
| Best for     | Low-cardinality columns | High-cardinality columns            |
| Structure    | Folders (`key=value`)   | Fixed number of files               |
| Optimization | Partition pruning       | Shuffle-free joins, fast search     |
| Metadata     | Stored in path          | Stored in Hive Metastore            |
| Failure case | Too many small folders  | Too many files if not repartitioned |

---

## **Final Interview Tip**

Partitioning and bucketing are **design-time decisions**.
Choosing the wrong strategy can severely impact **performance, storage, and cost**.



_____________________________________________________
https://www.youtube.com/watch?v=a-YGzAZEt04&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=18
---

How to write dataframe to disk in spark 

---

### **Q1. How do you write a DataFrame to disk in Spark?**

**Answer:**
In Spark, a DataFrame is written to disk using the **DataFrameWriter API**, which is accessed via `df.write`.

The general syntax is:

```python
df.write
  .format("csv")
  .option("header", "true")
  .mode("overwrite")
  .save("path/to/output")
```

This API is used after reading and transforming data, when we want to persist the processed data to storage.

---

### **Q2. What is the general structure of the DataFrameWriter API?**

**Answer:**
The general structure of the DataFrameWriter API is:

1. **`df.write`** ‚Äì Entry point to the writer API
2. **`.format()`** ‚Äì Specifies the file format (CSV, Parquet, JSON, etc.)
3. **`.option()`** ‚Äì Used for format-specific configurations
4. **`.partitionBy()` / `.bucketBy()`** ‚Äì Used for optimization
5. **`.mode()`** ‚Äì Defines how to handle existing data
6. **`.save()`** ‚Äì Triggers the write operation

The `save()` method is an **action** and actually writes the data to disk.

---

### **Q3. What happens if you do not specify a format while writing a DataFrame?**

**Answer:**
If no format is specified, Spark uses **Parquet as the default format**.

Example:

```python
df.write.save("path")
```

This will write the data in **Parquet format**.

---

### **Q4. What are the different save modes available in DataFrameWriter?**

**Answer:**
Spark provides **four save modes** when writing a DataFrame:

1. **Append**
2. **Overwrite**
3. **ErrorIfExists**
4. **Ignore**

These modes define how Spark behaves when data already exists at the target location.

---

### **Q5. Explain Append mode in DataFrameWriter.**

**Answer:**
In **Append mode**:

* If files already exist at the destination path
* Spark **adds new data** to the existing files
* Existing data is **not modified or deleted**

This mode is commonly used in incremental data loads.

---

### **Q6. Explain Overwrite mode in DataFrameWriter.**

**Answer:**
In **Overwrite mode**:

* Spark **deletes all existing files** at the target location
* Writes only the new data

If the DataFrame contains 5 records, only those 5 records will exist after the write operation.

---

### **Q7. What is ErrorIfExists mode?**

**Answer:**
In **ErrorIfExists mode**:

* Spark checks if the output path already exists
* If it exists, Spark throws an error and **stops execution**
* No data is written

This mode is useful when overwriting data is not allowed.

---

### **Q8. What is Ignore mode in DataFrameWriter?**

**Answer:**
In **Ignore mode**:

* If the destination path already exists
* Spark **skips the write operation**
* No error is thrown and no new data is written

This mode is useful when we want to avoid both overwriting and appending data.

---

### **Q9. How does partitioning affect the number of output files?**

**Answer:**
Spark writes **one output file per partition**.

* If the DataFrame has **1 partition**, Spark writes **1 file**
* If the DataFrame has **N partitions**, Spark writes **N files**

Each output file is typically named:

```
part-00000
part-00001
```

---

### **Q10. How can you control the number of output files while writing a DataFrame?**

**Answer:**
The number of output files can be controlled by changing the number of partitions:

* Use **`repartition(n)`** to increase or rebalance partitions
* Use **`coalesce(n)`** to reduce partitions

Example:

```python
df.repartition(3)
  .write
  .format("csv")
  .save("path")
```

This will create **3 output files**.

---

### **Q11. Why does Spark create multiple files instead of a single file?**

**Answer:**
Spark is a **distributed processing engine**.

* Each partition is processed independently
* Each executor writes its own output file
* This design improves **parallelism and performance**

Writing a single file would reduce parallelism and cause a bottleneck.

---

### **Q12. How can you verify the output files after writing a DataFrame?**

**Answer:**
In Databricks, output files can be verified using:

```python
dbutils.fs.ls("path")
```

This command lists all files written to the destination directory.

---

### **Q13. Is writing a DataFrame to disk a transformation or an action?**

**Answer:**
Writing a DataFrame is an **action**.

* Spark executes all lazy transformations
* Data is finally written to disk only when `save()` is called

---

### **Q14. What are common interview topics related to DataFrame writing?**

**Answer:**
Common interview questions include:

* Save modes in DataFrameWriter
* Difference between `partitionBy` and `bucketBy`
* Impact of partitions on output files
* Default file format in Spark
* Difference between `repartition` and `coalesce`

---

### **Final Interview Tip**

Understanding **how Spark writes data**, **save modes**, and **partition behavior** is very important because these concepts directly impact **performance, storage layout, and data correctness** in production systems.

If you want, I can next prepare:

* **PartitionBy vs BucketBy (deep interview explanation)**
* **Repartition vs Coalesce comparison**
* **Spark file formats interview Q&A**

Just tell me üëç

_______________________________
https://www.youtube.com/watch?v=vWSQIIiS8sg&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=16
RDD
---

### **Q1. What is an RDD in Apache Spark?**

**Answer:**
RDD stands for **Resilient Distributed Dataset**.
It is the **core data abstraction** of Apache Spark.

An RDD is:

* A **distributed collection of data**
* Stored across multiple nodes in a **cluster**
* Processed **in parallel in memory**

Internally, **all Spark operations are ultimately executed on RDDs**, even when we use DataFrames or Datasets.

---

### **Q2. Why is RDD called ‚ÄúResilient‚Äù?**

**Answer:**
RDD is called **Resilient** because it is **fault tolerant**.

If a partition of an RDD is lost due to node or executor failure:

* Spark uses **lineage information**
* Lineage tells Spark how the RDD was created
* Spark **recomputes the lost partitions automatically**

This self-recovery mechanism makes RDD resilient.

---

### **Q3. Why is RDD called ‚ÄúDistributed‚Äù?**

**Answer:**
RDD is distributed because its data is **spread across multiple machines** in a cluster.

Example:

* If we have 500 MB of data
* And the block size is 128 MB
* The data will be split into **4 partitions**
* Each partition is processed by a different executor

This distribution enables **parallel processing** and improves performance.

---

### **Q4. What does ‚ÄúDataset‚Äù mean in RDD?**

**Answer:**
Dataset refers to the **actual data** being processed.

An RDD is not just metadata; it contains:

* Real data
* Which is processed using transformations and actions

---

### **Q5. Are RDDs mutable or immutable? Explain.**

**Answer:**
RDDs are **immutable**.

Once an RDD is created, it **cannot be modified**.
Any transformation applied to an RDD creates a **new RDD**.

Example:

* Applying a `filter` creates RDD1
* Applying another filter on RDD1 creates RDD2
* Original RDD remains unchanged

Immutability helps Spark achieve fault tolerance and consistency.

---

### **Q6. What is Lazy Evaluation in RDD?**

**Answer:**
RDD transformations are **lazily evaluated**.

This means:

* Transformations are not executed immediately
* Spark waits until an **action** (like `count`, `collect`, or `show`) is called

Before execution:

* Spark builds a **DAG (Directed Acyclic Graph)**
* This DAG represents the **lineage** of transformations

Execution starts only when an action is triggered.

---

### **Q7. How does RDD provide fault tolerance?**

**Answer:**
RDD provides fault tolerance using **lineage information**.

If an RDD or its partition fails:

* Spark traces back the lineage
* Identifies the transformations used to create the RDD
* Recomputes the missing data automatically

This avoids the need for data replication.

---

### **Q8. What are the disadvantages of using RDD?**

**Answer:**

1. **No Automatic Optimization**

   * Spark does not optimize RDD operations automatically
   * The developer must manage performance manually

2. **‚ÄúHow to‚Äù Instead of ‚ÄúWhat to‚Äù**

   * RDD requires developers to specify *how* to process data
   * DataFrames allow specifying *what* result is required

3. **Complex Code**

   * Operations like `groupBy` and `join` are verbose and complex in RDD
   * Much simpler in DataFrames or SQL

4. **Performance Limitations**

   * Spark cannot reorder or optimize RDD operations
   * Execution follows the developer-defined order

---

### **Q9. Explain the performance difference between RDD and DataFrame.**

**Answer:**

Example:
Operations ‚Äî `reduceByKey` followed by `filter`

* **RDD**

  * Spark first performs a full shuffle
  * Then applies the filter
  * This is computationally expensive

* **DataFrame**

  * Spark applies the filter first
  * Reduces the dataset size
  * Then performs shuffle
  * This improves performance

Reason: **Catalyst Optimizer**, which works only with DataFrames/Datasets.

---

### **Q10. Why should we avoid using RDD in most cases?**

**Answer:**
RDDs should generally be avoided because:

* They lack automatic optimization
* Code is more complex
* Performance is usually lower compared to DataFrames

Modern Spark applications prefer **DataFrames and Datasets**.

---

### **Q11. When should we use RDD?**

**Answer:**
RDDs are useful when:

1. **Processing Unstructured Data**

   * Text files
   * Logs
   * Raw data without schema

2. **Type Safety is Required**

   * RDD provides compile-time type checking
   * DataFrames detect type errors at runtime

3. **Full Low-Level Control is Needed**

   * Custom transformations
   * Fine-grained performance tuning

---

### **Q12. Are RDDs type-safe?**

**Answer:**
Yes, RDDs are **type-safe**.

Type mismatches are detected at **compile time**, reducing runtime failures.

---

### **Q13. What is the relationship between RDD, DataFrame, and Dataset?**

**Answer:**

* DataFrames and Datasets are built **on top of RDDs**
* Internally, Spark converts them into RDDs during execution
* Understanding RDDs helps in understanding Spark internals

---

### **Q14. What is the current industry preference: RDD or DataFrame?**

**Answer:**
The industry strongly prefers:

* **DataFrames**
* **Datasets**

Because they are:

* Optimized
* Easier to use
* Suitable for both engineers and analysts

---

### **Final Interview Tip**

Even though RDDs are used less in real projects today,
**interviewers expect strong knowledge of RDDs** because they form the **foundation of Apache Spark**.







____________________________________________________________________________________________________________________
For each product, calculate the sales performance based on the average of the last 3 months (Rolling/Moving Average

# Define Window
rolling_window = Window.partitionBy("product_id") \
    .orderBy("sales_date") \
    .rowsBetween(-2, 0) # Current row + 2 rows before

# Calculate Rolling Average
df_rolling = product_df.withColumn("rolling_avg", F.avg("sales").over(rolling_window))

# Optional: Filter out first 2 months using row_number to ensure accurate 3-month data
rn_window = Window.partitionBy("product_id").orderBy("sales_date")
df_final_rolling = df_rolling.withColumn("rn", F.row_number().over(rn_window)) \
    .filter(F.col("rn") > 2)

df_final_rolling.show()
___________________________
Given a dataset of office entry/exit scans, identify employees who spent less than 8 hours in the office on any given day so an automated email can be sent to them. [21:51]

# 1. Prepare Timestamps
df_ts = emp_df.withColumn("full_timestamp", 
    F.to_timestamp(F.concat(F.col("date"), F.lit(" "), F.col("time")), "dd-MM-yyyy HH:mm"))

# 2. Define Window (Partition by Name AND Date)
emp_window = Window.partitionBy("emp_name", "date") \
    .orderBy("full_timestamp") \
    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)

# 3. Get Entry (First) and Exit (Last)
df_hours = df_ts.withColumn("entry_time", F.first("full_timestamp").over(emp_window)) \
    .withColumn("exit_time", F.last("full_timestamp").over(emp_window))

# 4. Calculate Difference in Hours
df_final = df_hours.withColumn("hours_worked", 
    (F.col("exit_time").cast("long") - F.col("entry_time").cast("long")) / 3600) \
    .filter(F.col("hours_worked") < 8)

df_final.distinct().show()
____________________________________________
For each product, find the difference between its sales in the very first month it was sold and its most recent (latest) month's sales
from pyspark.sql import Window
import pyspark.sql.functions as F

# Define the Window
window_spec = Window.partitionBy("product_id") \
    .orderBy("sales_date") \
    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)

# Calculate First and Latest Sales
df_result = product_df.withColumn("first_sales", F.first("sales").over(window_spec)) \
    .withColumn("latest_sales", F.last("sales").over(window_spec)) \
    .withColumn("sales_diff", F.col("latest_sales") - F.col("first_sales"))

df_result.show()

from pyspark.sql import Window
import pyspark.sql.functions as F

# Window for first sale
w_first = Window.partitionBy("product_id").orderBy("sales_date")

# Window for latest sale
w_last = Window.partitionBy("product_id").orderBy(F.col("sales_date").desc())

df = (
    product_df
    .withColumn("rn_first", F.row_number().over(w_first))
    .withColumn("rn_last", F.row_number().over(w_last))
)

first_sales = (
    df.filter(F.col("rn_first") == 1)
      .select("product_id", F.col("sales").alias("first_sales"))
)

latest_sales = (
    df.filter(F.col("rn_last") == 1)
      .select("product_id", F.col("sales").alias("latest_sales"))
)

result = (
    first_sales
    .join(latest_sales, "product_id")
    .withColumn("sales_diff", F.col("latest_sales") - F.col("first_sales"))
)

result.show()
________________________

_________________________________________________
https://www.youtube.com/watch?v=2Eb9mE7pfgs&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=39

# **Driver Out Of Memory (OOM) in Spark ‚Äì Interview Q&A**

---

## **1. What is Driver OOM in Spark?**

**Answer:**
Driver OOM happens when the **Spark Driver‚Äôs JVM heap or overhead memory** exceeds its allocated limit.
Since the Driver is responsible for **task scheduling, metadata, and collecting results**, large result sets or excessive objects can easily crash it.

---

## **2. How is Driver memory configured in Spark?**

**Answer:**
Driver memory is configured using two parameters:

```text
spark.driver.memory          ‚Üí JVM Heap
spark.driver.memoryOverhead  ‚Üí Non-JVM memory
```

Total Driver container size =

```
Driver Memory + Memory Overhead
```

If this **container limit** is crossed, the Driver crashes with OOM.

---

## **3. What is Driver Memory Overhead and why is it important?**

**Answer:**
Driver Memory Overhead is **non-JVM memory** used for:

* Python processes (PySpark)
* Native libraries
* Serialization buffers
* Container management
* OS-level memory

**Rule:**

```
memoryOverhead = max(10% of driver memory, 384MB)
```

If overhead is insufficient, the Driver can fail even if JVM heap looks free.

---

## **4. Why does `df.show()` work but `df.collect()` causes Driver OOM?**

**Answer:**

### **`collect()`**

* Fetches **all partitions from all executors**
* Loads entire dataset into Driver JVM heap
* If total data > Driver memory ‚Üí **OOM**

Example:

* 3 partitions √ó 500MB = **1.5GB**
* Driver memory = **1GB**
* Result ‚Üí **java.lang.OutOfMemoryError: Java heap space**

---

### **`show()`**

* Fetches **only enough rows** (e.g., 20)
* Usually reads **one partition**
* Fits easily into Driver memory

‚úÖ That‚Äôs why `show()` succeeds while `collect()` fails.

---

## **5. Where does the Driver run in a Spark cluster?**

**Answer:**
The Driver runs inside an **Application Master container** on a worker node (or client machine in client mode).
This container has a **hard physical memory limit**, just like a phone storage limit.

Exceeding it ‚Üí Driver killed ‚Üí Application fails.

---

## **6. What happens internally when `collect()` is called?**

**Answer:**

* Executors send all partition results to the Driver
* Driver stores results in JVM heap
* No spilling mechanism for Driver results
* Large datasets ‚Üí Heap overflow ‚Üí OOM

üìå Unlike executors, the Driver **cannot spill collected results to disk**.

---

## **7. What are the most common causes of Driver OOM in Spark?**

### **1Ô∏è‚É£ Using `collect()` on large datasets**

**Reason:** Entire dataset moves to Driver memory
**Fix:** Avoid `collect()`, use `show()`, `take()`, or write to storage

---

### **2Ô∏è‚É£ Broadcast Joins**

**Reason:**

* Driver must load all broadcast tables first
* Multiple small tables can add up

Example:

* 5 tables √ó 50MB = **250MB**
* Driver already busy ‚Üí OOM

**Fix:**

* Ensure enough Driver memory
* Lower broadcast threshold
* Avoid unnecessary broadcasts

---

### **3Ô∏è‚É£ Excessive Objects / Overhead Issues**

**Reason:**

* Too many objects
* Heavy Python processing
* Low `spark.driver.memoryOverhead`

**Fix:**

* Increase memory overhead
* Optimize object creation
* Avoid unnecessary driver-side logic

---

## **8. How do you identify Driver OOM from logs?**

**Answer:**
Typical errors:

* `java.lang.OutOfMemoryError: Java heap space`
* `Container killed by YARN for exceeding memory limits`
* Driver lost / Application failed

Spark UI usually shows Driver lost before executors.

---

## **9. How do you prevent Driver OOM in Spark?**

**Answer:**

* Avoid `collect()` on large datasets
* Use `take()`, `show()`, or write to files
* Increase `spark.driver.memory`
* Increase `spark.driver.memoryOverhead`
* Be cautious with broadcast joins
* Minimize driver-side data processing


____________________________________
https://www.youtube.com/watch?v=b2hO1oJf9nA&list=PLTsNSGeIpGnGkpfKMf7ilFmzfx6AjMKyT&index=19
executor out of memory spark | spark memory management 

### **Potential Interview Questions**
The YouTuber highlights several "famous" questions addressed in the video:
*   **Why do we get OOM** even when data can be spilled to disk?
*   How does Spark manage **internal storage**?
*   How are **tasks split** inside an executor?
*   What is the use of **Overhead Memory** (usually 10%)?
*   What are the different **Memory Managers** in Spark?
Got it üëç
Below is the **same content rewritten cleanly in Interview Q&A format**, crisp enough to speak in an interview, but still technically solid. You can treat this as a **ready-to-revise cheat sheet** for Spark memory management & Executor OOM questions.

---

## **1. Why do we get Executor Out Of Memory (OOM) even though Spark can spill data to disk?**

**Answer:**
Spark spills data to disk only for **certain operations**, but **not all memory usage is spillable**.

The most common reason for OOM is **data skew**.

For example, during a **join**, all records of the same key must come to **one executor**. If a single key holds **3GB of data** but the **execution memory pool is only ~2.9GB**, Spark **cannot partially spill that key**.
As a result, the executor exceeds its physical memory limit and crashes with OOM.

üëâ Spilling helps, but **Spark still needs enough physical memory per task**.

---

## **2. How does Spark manage Executor memory internally?**
When you request an executor, Spark divides memory in **multiple layers**:
 **Step 1: Container Memory**

```
spark.executor.memory + spark.executor.memoryOverhead
```

Example:

* Executor memory = **10GB**
* Overhead (10%) = **1GB**
* **Total container size = 11GB**

If Spark exceeds **11GB**, the container is killed ‚Üí **OOM**.

---

## **3. What is Overhead Memory and why is it needed?**

**Answer:**
**Overhead memory** is **non-JVM memory** used for:

* PySpark processes
* JVM native memory
* Container management
* Serialization buffers
* OS-level memory usage

By default:

```
spark.executor.memoryOverhead = max(10% of executor memory, 384MB)
```

Without enough overhead memory, executors fail even when JVM memory looks fine.

---

## **4. How is Executor JVM memory divided internally?**

**Answer:**
Executor JVM memory is split into **three parts**:

### **1. Reserved Memory (Fixed ‚Äì ~300MB)**

* Used by Spark‚Äôs internal system objects
* Executor must be at least **1.5√ó reserved memory**
* Otherwise Spark won‚Äôt start

---

### **2. User Memory (~40% of remaining)**

Used for:

* UDFs
* User-defined objects
* RDD metadata
* Broadcast variables

This memory **cannot be spilled**.

---

### **3. Spark Memory (~60% of remaining)**

Used for **DataFrame and SQL operations**

---

## **5. What is Spark Memory and how is it divided?**

**Answer:**
Spark Memory is split into **two pools**:

### **1. Storage Memory**

* Used for `cache()` / `persist()`
* Stores cached DataFrames & RDDs
* Uses **LRU eviction** when full

### **2. Execution Memory**

* Used for:

  * Shuffles
  * Joins
  * Aggregations
  * Hash tables

Execution memory is **short-lived but critical**.

---

## **6. What are Spark Memory Managers?**

**Answer:**

### **Static Memory Manager (Spark < 1.6)**

* Fixed boundaries between storage & execution
* Inefficient
* Causes frequent OOM

### **Unified Memory Manager (Spark ‚â• 1.6)**

* Flexible boundaries
* Execution can **borrow memory** from storage if needed
* Much better memory utilization

---

## **7. If Unified Memory Manager exists, why does OOM still happen?**

**Answer:**
Because **physical memory limits still apply**.

Even with borrowing:

* A **single task** cannot exceed executor memory
* Large joins, skewed keys, or huge hash maps still cause OOM
* Borrowing does **not increase total memory**

---

## **8. How are tasks split inside an executor?**

**Answer:**
Tasks are split based on **cores per executor**.
* 1 core = 1 task at a time
* 4 cores = 4 parallel tasks

Each task consumes:
* Execution memory
* User memory
* Overhead memory

More parallel tasks ‚Üí **more memory pressure**.

---

## **9. How many cores should an executor have and why?**

**Answer:**
**Recommended: 3‚Äì5 cores per executor**
Reason:
* Too few cores ‚Üí underutilization
* Too many cores ‚Üí high memory contention

With more than 5 cores:
* Many tasks run in parallel
* Memory overhead increases
* Leads to **Executor OOM**

---

## **10. What are common real-world reasons for Executor OOM?**
* Data skew
* Large joins without repartitioning
* Excessive caching
* Too many cores per executor
* Insufficient memory overhead
* Heavy UDF usage
* Large broadcast variables

---

## **11. How do you prevent Executor OOM in Spark?**

**Answer:**

* Handle **data skew** (salting, repartitioning)
* Increase executor memory or overhead
* Reduce cores per executor
* Avoid unnecessary caching
* Use broadcast joins wisely
* Monitor Spark UI (Storage & Executors tab)
_________________
https://www.youtube.com/watch?v=EEq9JxGGEZE&list=PLTsNSGeIpGnGkpfKMf7ilFmzfx6AjMKyT&index=20&t=856s
spark submit
Spark Submit is a command-line tool used to run Spark applications. It works by taking all necessary files and JARs, packaging them, and triggering the application on the cluster. 
The cluster can be located in several places: Standalone, Local mode, Kubernetes, or YARN (which is the most common

Dynamic Allocation: Setting spark.dynamicAllocation.enabled=true allows Spark to release resources that are idle so other processes can use them.
‚Ä¢ Executor Limits: You can set minExecutors (e.g., 1) and maxExecutors (e.g., 10) to prevent a single job from consuming all cluster memory.
‚Ä¢ Broadcast Settings: You can adjust autoBroadcastJoinThreshold and broadcastTimeout. While he mentions a 3600-second (1-hour) timeout for extreme cases, he typically recommends starting
____________________________________________________________________
https://www.youtube.com/watch?v=_PA86Npk1ck&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=46
dynamic resource allocation in spark 

### 1. Introduction & Interview Questions
Sabse pehle unhone bataya ki interview mein is topic se kya questions aate hain:
*   Dynamic Resource Allocation kya hota hai?
*   Resource Manager resources kaise allocate karta hai?
*   Kaun-kaun si techniques hain (Static vs Dynamic)?
*   Challenges kya hain aur process fail kyun ho sakta hai?
*   Dynamic Allocation kab avoid karna chahiye?
*   Configurations kaun si lagti hain?

### 2. The Scenario (Problem Statement with Example)
Unhone ek real-world example se samjhaya ki Dynamic Allocation ki zarurat kyun padi:

*   **User 1 (Heavy Job):** Ek developer ne `spark-submit` command chalayi. Usne configuration rakhi:
    *   **Executors:** 49 (Max executors).
    *   **Executor Memory:** 20 GB.
    *   **Cores:** 4 per executor.
    *   **Driver Memory:** 20 GB.
    *   **Calculation:** $49 \times 20$ GB = 980 GB + 20 GB Driver = **Total 1000 GB** use kar liya.
*   **Cluster Condition:** Maan lijiye poore cluster ki capacity hi 1000 GB RAM aur 196 Cores thi. Iska matlab User 1 ne poora cluster full kar diya.
*   **User 2 (Small Job):** Ab ek dusra developer aata hai jise sirf **25 GB** chahiye (20 GB executor + 5 GB driver).
*   **Problem:** Resource Manager (RM) **FIFO (First In First Out)** pe kaam karta hai. Kyunki cluster full hai, RM bolega "Wait karo". User 2 ka job ho sakta hai 5 minute ka ho, lekin use User 1 ke 5 ghante ke job ke khatam hone ka wait karna padega.

Development environment mein ye aksar hota hai ki log resource lekar baith jate hain par kaam nahi kar rahe hote, jisse baaki log block ho jate hain.

### 3. Static vs. Dynamic Resource Allocation
Yahan do techniques aati hain:
*   **Static Resource Allocation:** Ek baar jo memory mil gayi (e.g., 100 GB), wo poore time (pure application duration tak) reserve rehti hai, chahe aap use karein ya na karein.
*   **Dynamic Resource Allocation:** Agar aapka process resource use nahi kar raha, to use release (khali) kar diya jata hai taaki dusre use kar sakein.

### 4. How Dynamic Allocation Works (Release & Acquire)
Is process ko enable karne ke liye `spark.dynamicAllocation.enabled = true` set karna padta hai (by default ye false hota hai).

*   **Releasing Resources:**
    *   Maan lo shuru mein load karte waqt 1000 GB chahiye tha.
    *   Phir filter lagaya, data kam ho gaya, ab sirf 500 GB ki zarurat hai.
    *   Dynamic allocation mein bacha hua **500 GB release** kar diya jayega.
    *   Phir aur data kam hua, to aur 250 GB release ho jayega.

*   **Acquiring Resources (On Demand):**
    *   Agar aage chalkar koi Join operation aata hai aur wapas memory ki zarurat padti hai, to Spark Resource Manager se demand karega.
    *   **Risk/Challenge:** Resource Manager kisi se jabardasti resource khali nahi karwa sakta. Agar wo release kiya hua 500 GB kisi aur process ne le liya hai, to aapke process ko wapas resource nahi milega aur **process fail ho sakta hai**.

### 5. Solutions & Configurations (Detailed)
Process fail hone se bachane ke liye kuch configurations set ki jati hain:

*   **Min & Max Executors:**
    *   `spark.dynamicAllocation.minExecutors`: Default 1 hota hai, par hume isse badhana chahiye (jaise ki example mein **5 ya 20** set kar sakte hain). Iska matlab hai ki chahe kaam ho ya na ho, itne executors hamesha aapke paas rahenge taaki process starve na ho.
    *   `spark.dynamicAllocation.maxExecutors`: Maximum kitne executors hum le sakte hain (example mein **49** tha).

*   **External Shuffle Service / Shuffle Tracking:**
    *   **Challenge:** Jab executor khali (kill) kiya jata hai, to uske disk pe rakha hua **Shuffle Data (Map output/exchange data)** bhi chala jata hai. Agar ye data chala gaya, to Join ke time recalculate karna padega.
    *   **Solution:** `spark.shuffle.service.enabled` (ya shuffle tracking) ko true karte hain. Ye service `Worker Node` pe alag se chalti hai aur data ko hold karti hai, bhale hi executor kill ho jaye. Isse recalculation nahi karni padti.

### 6. Timing & Request Strategy (Bahut Important Detail)
Video mein ye bhi explain kiya gaya hai ki resource kab chhoda jata hai aur kab maanga jata hai:

*   **Kab Release karega (Idle Timeout):** Agar koi executor **60 seconds** (default) tak idle hai (koi kaam nahi kar raha), to usko release kar diya jayega. Aap is config ko change bhi kar sakte hain (jaise video mein 45 sec kiya tha).
*   **Kab Request karega (Backlog Timeout):** Agar pending tasks ko resource nahi mil raha aur wo **1 second** se zyada wait kar rahe hain, to new executors ki demand shuru hogi.
*   **Kaise Maangega (Two-Fold Strategy):**
    *   Aisa nahi hai ki seedha 500 GB maang lega.
    *   Ye **exponentially (Two-Fold)** maangta hai: Pehle **1** executor maangega, agar kaam nahi bana to **2** maangega, phir **4**, phir **8**, phir **16**... is tarah se badhta jayega.

### 7. When to Avoid Dynamic Allocation?
Interview mein ye pucha jata hai.
*   **Production Critical Jobs:** Agar aap production mein koi critical job chala rahe hain jahan aap SLA (Time) miss nahi kar sakte, wahan Dynamic Allocation **avoid** karna chahiye.
*   Wahan **Static Allocation** use karein taaki resources guaranteed rahein aur resource na milne ki wajah se job fail ya delay na ho.

### 8. Summary of Configurations Mentioned
Video mein dikhaye gaye `spark-submit` mein ye settings thin:
*   `spark.dynamicAllocation.enabled`: true
*   `spark.shuffle.service.enabled`: true (ya shuffle tracking)
*   `spark.dynamicAllocation.minExecutors`: (e.g., 5)
*   `spark.dynamicAllocation.maxExecutors`: (e.g., 49)
*   `spark.dynamicAllocation.executorIdleTimeout`: (e.g., 60s)

spark-submit \
--name YourAppName \
--master yarn \
--deploy-mode cluster \
--conf spark.dynamicAllocation.enabled=true
--conf spark.dynamicAllocation.minExecutors=5
+
20
--conf spark.dynamicAllocation.maxExecutors=49
++
--conf spark.shuffleTracking.enabled=true
--conf spark.dynamicAllocation.executorialeTimecut=45s \
--conf spark.scheduler.backlogTimeout=2s \
--conf spark.executor.memory=20g
\2
--conf spark.executor.cores=4
\
--conf spark.driver.memory=20g \
--py-files python_dependencies.zip \
main.py
________________________________
cache vs persist
https://www.youtube.com/watch?v=Pw1vn82wEBE&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=45



# **Apache Spark ‚Äì Cache & Persist (Interview Questions & Answers | Hinglish)**---

## **Q1. What is caching in Spark?**

**Answer:**
Caching Spark ki ek **optimization technique** hai jisme hum **intermediate results** ko memory mein store kar dete hain taaki unko baar-baar re-calculate na karna pade.

## **Q2. Spark mein cached data memory ke kis part mein store hota hai?**

**Answer:**
Spark Executor memory ke 3 parts hote hain:

1. User Memory
2. Spark Memory
3. Reserved Memory

Spark Memory ke andar do pools hote hain:

* **Storage Memory Pool** ‚Üí Cache/Persist yahin hota hai
* **Execution Memory Pool** ‚Üí Join, shuffle, aggregation yahin hoti hai

üëâ Cached data **sirf Storage Memory Pool** mein store hota hai.

---
 **Q3. Agar Execution memory ko space chahiye ho toh kya hota hai?**

**Answer:**
Execution pool, Storage pool se memory **LRU (Least Recently Used)** fashion mein **evict** kar sakta hai. Matlab jo data least recently use hua hai, wo pehle remove hoga.
## **Q4. Caching ki zarurat kyun padti hai? (Why cache?)**

**Answer:**
Spark mein:

* DataFrames **immutable** hote hain
* Operations **lazy evaluation** follow karte hain

Agar ek DataFrame (`df`) ko multiple jagah reuse kiya ja raha hai (joins, transformations), toh bina cache ke Spark:

* Har action pe DAG ke through data **re-calculate** karega
* Isse time aur resources waste hote hain
 `df.cache()` lagane se data **long-lived Storage Memory** mein aa jata hai aur re-calculation avoid hota hai.


# **Q6. Agar Storage Memory 2GB hai aur data 2.5GB ka ho toh Spark kya karega?**

**Answer:**
Ye bahut common interview question hai:

* Spark **kabhi half partition cache nahi karta**
* Agar 12 partitions fit ho rahe hain aur 13th aadha fit ho raha hai ‚Üí wo store nahi hoga
* Jo partitions fit nahi honge:

  * Ya toh disk pe jayenge (agar storage level allow karta ho)
  * Ya phir unko dobara **re-calculate** kiya jayega
* Agar koi cached partition **lost** ho jaye, Spark DAG se usse wapas calculate kar leta hai

---

## **Q7. Cache aur Persist mein difference kya hai?**

**Answer:**
Short answer: **Persist main method hai, Cache uska shortcut hai.**

| Cache                       | Persist               |
| --------------------------- | --------------------- |
| Wrapper method              | Actual implementation |
| Fixed default storage level | Custom storage level  |
| Simple use                  | More control          |

`cache()` internally call karta hai:

persist(MEMORY_AND_DISK)
## **Q8. Spark ka default storage level kya hota hai?**
**Answer:**
Default storage level hota hai:
**MEMORY_AND_DISK**
## **Q9. MEMORY_ONLY storage level ka behaviour kya hai?**

**Answer:**

* Data sirf **RAM** mein hota hai
* Data **deserialized form** mein rehta hai

‚úÖ Fast processing
‚ùå High memory consumption
‚ùå Agar memory full ho gayi toh data re-calculate hota hai

## **Q10. MEMORY_AND_DISK storage level kaise kaam karta hai?**

**Answer:**
 Pehle RAM mein store karta hai
* Agar RAM full ho gayi ‚Üí bacha hua data **Disk** pe chala jata hai
* Disk pe data **serialized form** mein hota hai

‚ùå Disk I/O slow hota hai
‚ùå CPU usage zyada hota hai due to serialization / deserialization

-## **Q11. DISK_ONLY storage level kab use karte hain?**

**Answer:**

* Jab memory bilkul limited ho
* Data sirf disk pe store hota hai
* Performance slow hoti hai
  Generally production mein avoid kiya jata hai

## **Q12. Serialized (_SER) storage levels kya hote hain?**

**Answer:**
Example: `MEMORY_ONLY_SER`

* Data **byte stream (serialized)** form mein hota hai
* Memory kam lagti hai (compression benefit)
 Memory efficient
‚ùå CPU intensive (deserialize karna padta hai)

üìå Mostly **Java/Scala** mein effective, Python mein itna clearly visible nahi hota

---

## **Q13. Replication (_2) storage levels ka use kya hai?**

**Answer:**
Example: `MEMORY_ONLY_2`

* Same data ki **2 copies** alag-alag worker nodes pe store hoti hain
* Agar ek node down ho gaya, toh doosre se data mil jata hai

‚úÖ Fault tolerance
‚ùå Memory usage double

---

## **Q14. Cache ke baad show() aur count() mein kya difference dikhta hai?**

**Answer:**

* `df.show()`:

  * Spark sirf **required partitions** cache karta hai
  * Kyunki show ko sirf top rows chahiye hoti hain

* `df.count()`:

  * Spark **saare partitions** cache karta hai
  * Kyunki count ke liye poora data scan hota hai

üëâ Spark intelligent hai, wo unnecessary caching avoid karta hai

---

## **Q15. Serialized aur Deserialized data ka size difference kyun hota hai?**

**Answer:**

* **Deserialized (Memory)** data readable objects hota hai ‚Üí size zyada
* **Serialized (Disk)** data compressed byte stream hota hai ‚Üí size kam

Isiliye UI mein disk size kam aur memory size zyada dikhta hai

---

## **Q16. Cached data ko memory se kaise hataate hain?**

**Answer:**
Simple:

```scala
df.unpersist()
```

Ye data ko Storage Memory Pool se remove kar deta hai

---

## **One-Line Interview Summary**

> **Cache Spark ke intermediate results ko Storage Memory mein store karta hai taaki lazy evaluation ke wajah se baar-baar DAG re-run na ho, aur persist hume storage level choose karne ka control deta hai.**

---

Agar chaho toh main:

* üîπ **30-second short answers**
* üîπ **Tricky follow-up interview questions**
* üîπ **Spark UI based explanation**
* üîπ **Real production scenarios**
__________________________________________________________________________
Delta Tables Liquid Clustering and Deletion Vectors
https://www.youtube.com/watch?v=EveYBUZP__4&t=115s
Got it üëç
Here‚Äôs your content **converted cleanly into interview-style Question & Answer format**, with crisp wording, strong keywords, and practical explanations‚Äîperfect for Delta Lake / Databricks interviews.

---

# **Interview Questions & Answers: Delta Tables ‚Äì Deletion Vectors & Liquid Clustering**

---

## **Q1. What are Deletion Vectors in Delta Lake?**

**Answer:**
Deletion Vectors are a Delta Lake optimization that allows row-level deletes **without rewriting the underlying Parquet files**. Instead of physically removing rows, Delta marks them as deleted using a vector (flag). During reads, only unflagged rows are returned.

---

## **Q2. Why are Deletion Vectors needed? What problem do they solve?**

**Answer:**
In traditional Parquet-based storage:

* Deleting even **one row** forces the **entire Parquet file to be rewritten**
* This causes high I/O, slower deletes, and performance overhead

Deletion Vectors solve this by:

* Avoiding file rewrites for small deletes
* Improving delete and update performance significantly

---

## **Q3. How do Deletion Vectors work internally?**

**Answer:**

1. A delete operation marks rows using a **deletion vector**
2. The original Parquet file remains unchanged
3. Delta metadata stores which rows are deleted
4. During reads, Delta skips flagged rows
5. When `OPTIMIZE` runs, files are rewritten and deleted rows are permanently removed

---

## **Q4. What happens when Deletion Vectors are disabled?**

**Answer:**
When Deletion Vectors are disabled:

* Any delete operation rewrites the entire Parquet file
* Delta history shows **files removed and new files added**
* This is slower and less efficient

---

## **Q5. How can you check if Deletion Vectors are enabled?**

**Answer:**
You can run:

```sql
DESCRIBE EXTENDED table_name;
```

Look for the table property:

```text
delta.enableDeletionVectors = true
```

---

## **Q6. What Delta / Databricks versions are required for Deletion Vectors?**

**Answer:**

* **Delta Lake:** 2.3.0+
* **Databricks Runtime:** 12.2 LTS+

---

## **Q7. What is Liquid Clustering in Delta Lake?**

**Answer:**
Liquid Clustering is a modern clustering mechanism that **automatically organizes new data** based on specified clustering columns‚Äî**without requiring table rewrites** like partitioning or Z-ordering.

---

## **Q8. How is Liquid Clustering different from Z-Ordering?**

**Answer:**

| Feature                         | Z-Ordering | Liquid Clustering |
| ------------------------------- | ---------- | ----------------- |
| Requires OPTIMIZE rewrite       | Yes        | No                |
| Applies to existing data        | Yes        | Incremental       |
| Maintenance overhead            | High       | Low               |
| Works well for high cardinality | Yes        | Yes (better)      |
| Automatic for new data          | No         | Yes               |

---

## **Q9. When should Liquid Clustering be used?**

**Answer:**
Liquid Clustering is ideal when:

* Filtering on **high-cardinality columns**
* Data grows rapidly
* There is data skew
* Query access patterns change frequently
* You want **low-maintenance optimization**

---

## **Q10. What versions are required for Liquid Clustering?**

**Answer:**

* **Delta Lake:** 3.1.0+
* **Databricks Runtime:** 13.3 LTS+

---

## **Q11. How do you enable Liquid Clustering on an existing table?**

**Answer:**

```sql
ALTER TABLE table_name
CLUSTER BY invoice_number;
```

To remove clustering:

```sql
ALTER TABLE table_name
CLUSTER BY NONE;
```

---

## **Q12. How do you create a new table with Liquid Clustering?**

**Answer:**

```sql
CREATE TABLE sales (
  ...
)
USING DELTA
CLUSTER BY (invoice_number);
```

---

## **Q13. Does Liquid Clustering rewrite existing data?**

**Answer:**
No.
Liquid Clustering does **not rewrite existing data**. It automatically clusters **new incoming data**, reducing maintenance overhead compared to Z-Ordering.

---

## **Q14. What is an important limitation of Liquid Clustering?**

**Answer:**
Clustering columns must be present within the **first 32 columns** of the Delta table schema.

---

## **Q15. How does Liquid Clustering improve query performance?**

**Answer:**

* Data is physically organized based on clustering columns
* Queries with filters on clustered columns read fewer files
* Faster scans and reduced I/O
* No need for frequent OPTIMIZE operations

---

## **Q16. How do Deletion Vectors and Liquid Clustering work together?**

**Answer:**

* **Deletion Vectors** optimize delete and update operations
* **Liquid Clustering** optimizes query performance and data layout

---

___________________________________________________________________
Data Skipping and Z-Ordering in Delta Lake Tables
https://www.youtube.com/watch?v=kUMAmZQIMfA&t=604s

Absolutely üëç
Here‚Äôs the same content **converted into a clean, interview-ready Question & Answer format**, with crisp explanations and keywords that interviewers look for.

---

## **Interview Questions & Answers: Data Skipping and Z-Ordering (Delta Lake)**

---

### **Q1. What is data skipping in Delta Lake?**

**Answer:**
Data skipping is an optimization technique in Delta Lake that avoids scanning unnecessary files during query execution. Delta Lake stores **min/max statistics** for columns at the file level. When a query is executed, Delta uses this metadata to **skip files** that cannot possibly satisfy the query condition.

---

### **Q2. Why is partitioning not suitable for high-cardinality columns like `order_id`?**

**Answer:**
High-cardinality columns have a large number of unique values. Partitioning on such columns would create **too many partitions**, leading to:

* Small file problems
* Metadata overhead
* Poor query performance

Hence, partitioning is avoided for columns like `order_id`, `invoice_number`, or `transaction_id`.

---

### **Q3. What problem does Z-Ordering solve?**

**Answer:**
Z-Ordering helps optimize queries on **high-cardinality, frequently filtered columns** where partitioning is not feasible. Without Z-Ordering, Spark must scan all files to locate a specific value. Z-Ordering **clusters related data together**, enabling effective data skipping.

---

### **Q4. How does Z-Ordering work internally?**

**Answer:**
When a table is Z-Ordered on a column:

1. Data is **sorted based on the Z-Order column(s)**.
2. Sorted data is written into Parquet files sequentially.
3. Delta Lake records **min/max values per file** in metadata.
4. During queries, Delta reads **only the files whose ranges match the filter condition**, skipping the rest.

Example:
If File-2 contains `order_id` range **105‚Äì108**, searching for `order_id = 106` will scan only File-2.

---
What are the min/max invoice numbers per file?": This investigative query proves that data is overlapping across files.

SELECT min(invoice_number), max(invoice_number), _metadata.file_name FROM SalesDelta GROUP BY _metadata.file_name ORDER BY min(invoice_number)
### **Q5. What is multi-dimensional Z-Ordering?**

How do I set the maximum file size for optimization?": 
* spark.conf.set("spark.databricks.delta.optimize.maxFileSize", 64 * 1024 * 1024
___________

**Answer:**
Multi-dimensional Z-Ordering allows Z-Ordering on **multiple columns** (e.g., `Country`, `InvoiceNo`).
Delta uses a space-filling curve to interleave column values so that related rows stay physically close, improving filtering performance when **all Z-Order columns are used in queries**.

---

### **Q6. What happens if Z-Ordering is used incorrectly?**

**Answer:**
If queries do not filter on all Z-Ordered columns:

* More files may be scanned
* Performance can degrade instead of improving

Example:
Z-Ordering by `(Country, InvoiceNo)` but filtering only by `InvoiceNo` may increase file scans.

**Key rule:**
üëâ Z-Order only on columns that are **frequently used together in filters**.

---

### **Q7. How does Z-Ordering improve query performance in practice?**

**Answer:**
In a retail dataset example:

* Before Z-Ordering:

  * Files read: **16**
  * Query time: **~3 seconds**
* After `OPTIMIZE ZORDER BY InvoiceNo`:

  * Files read: **1**
  * Query time: **~0.5 seconds**

This shows significant improvement due to data skipping.

---

### **Q8. Why was `spark.databricks.delta.optimize.maxFileSize` changed in the example?**

**Answer:**
The default file size is **1 GB**, which is too large for small datasets. Reducing it (e.g., to **64 MB**) ensures:

* More files are created
* Z-Ordering effects become visible
* Better demonstration of data skipping behavior

---

### **Q9. What is the best strategy: Partitioning or Z-Ordering?**

**Answer:**
The best approach is **combining both**:

* **Partition** on low-cardinality columns (e.g., `Country`)
* **Z-Order** on high-cardinality columns (e.g., `InvoiceNo`)

Example:

```sql
OPTIMIZE SalesDelta
WHERE Country = 'Australia'
ZORDER BY InvoiceNo;
```

This ensures:

* Only **1 partition** is scanned
* Only **1 file** is read

---

### **Q10. Does Auto Compaction perform Z-Ordering automatically?**

**Answer:**
No.
Auto Compaction only **merges small files into larger ones**. It does **not** Z-Order data.
Z-Ordering must be **explicitly triggered** using the `OPTIMIZE ... ZORDER BY` command.

---

### **Q11. When should Z-Ordering be used?**

**Answer:**
Z-Ordering should be used when:

* The column has **high cardinality**
* The column is **frequently used in WHERE filters**
* Partitioning is not feasible

---

### **One-Line Interview Summary**

> **Partition low-cardinality columns, Z-Order high-cardinality columns, and rely on Delta metadata for data skipping to minimize file scans and improve query performance.**

---

____________________________________________________________________________________
https://www.youtube.com/watch?v=lI8SryM3Dqk
---

## **Q: What is Change Data Capture (CDC)? Explain its types and how it is handled.**

**Answer:**

**Change Data Capture (CDC)** is a data engineering concept used to **identify and capture changes in data over time** from a source system so that only changed data is processed in downstream systems like data lakes or data warehouses.
Instead of reprocessing the full dataset every time, CDC helps handle **inserts, updates, and deletes efficiently**.

---

## **Types of Data Changes in CDC**

There are **three types of changes** that can occur in a source system:

1. **Insert** ‚Äì A new record is added.
2. **Update** ‚Äì An existing record is modified.
3. **Delete** ‚Äì A record is removed.

**Example:**

* Day 1: IDs ‚Üí 1, 2, 3
* Day 2:

  * ID 1 ‚Üí Address updated (Update)
  * ID 2 ‚Üí Removed (Delete)
  * ID 4 ‚Üí Added (Insert)

---

## **CDC Load Scenarios**

### **1. Full Load**

* Source sends the **entire dataset every time**.
* **Handling:** Overwrite the previous data.
* **Pros:** Simple to implement.
* **Cons:** Inefficient for large datasets.

üëâ Updates, inserts, and deletes are automatically reflected.

---

### **2. Incremental Load (With Flags)**

* Source sends **only changed records** along with a flag:

  * `I` ‚Üí Insert
  * `U` ‚Üí Update
  * `D` ‚Üí Delete

**Handling:**

* **Insert:** Append new records.
* **Update:** Locate existing records using primary key and update them.
* **Delete:** Either remove records or mark them as deleted.

üëâ This is the **most efficient and preferred** CDC approach.

---

### **3. Incremental Load (Without Flags)**

* Source sends changed data **without specifying the change type**.

**Handling:**

* Perform a **Left Outer Join** between incoming data and existing data using the primary key.

  * **No match found:** Insert
  * **Match found with column differences:** Update
* Column-level comparison is done only on **business-relevant columns** to avoid performance issues.

‚ö†Ô∏è **Limitation:** Deletes **cannot be identified**, because missing records are not explicitly provided.

---

## **Storage Strategies: Slowly Changing Dimensions (SCD)**

### **SCD Type 1 ‚Äì No History**

* Only the **latest data** is stored.
* Old values are overwritten.
* **Use case:** When history is not required.

üìå Example: Address updated from Pune ‚Üí Bangalore (Pune is lost).

---

### **SCD Type 2 ‚Äì Full History**

* Maintains **historical versions** of records.
* New row is inserted for every change.

**Common Columns:**

* `start_date`
* `end_date`
* `is_active` / `is_latest`

**Example:**

| ID | Address   | Start Date | End Date | Active |
| -- | --------- | ---------- | -------- | ------ |
| 1  | Pune      | 01-Jul     | 02-Jul   | N      |
| 1  | Bangalore | 02-Jul     | NULL     | Y      |

üëâ Current data can be fetched using `is_active = 'Y'`.

---

## **Handling Deletes in CDC**

1. **Soft Delete**

   * Record is not removed.
   * A flag like `delete_flag = 'Y'` is set.
   * **Preferred** for audit and history.

2. **Hard Delete**

   * Record is physically removed.
   * Used only when mandated by business or compliance.

---


-- ============================================================================
-- CDC SCENARIO C: INCREMENTAL LOAD WITHOUT FLAGS (SQL IMPLEMENTATION)
-- ============================================================================

-- Step 1: Create sample tables
-- ============================================================================

-- Existing table (your current data lake/warehouse table)
CREATE TABLE existing_table (
    id INT PRIMARY KEY,
    name VARCHAR(100),
    address VARCHAR(100),
    salary DECIMAL(10, 2)
);

-- Insert sample existing data
INSERT INTO existing_table (id, name, address, salary) VALUES
(1, 'Alice', 'Pune', 50000),
(2, 'Bob', 'Mumbai', 60000),
(3, 'Charlie', 'Delhi', 55000);

-- Staging table (incoming data from source)
CREATE TABLE staging_table (
    id INT PRIMARY KEY,
    name VARCHAR(100),
    address VARCHAR(100),
    salary DECIMAL(10, 2)
);

-- Insert incoming data (Day 2)
INSERT INTO staging_table (id, name, address, salary) VALUES
(1, 'Alice', 'Bangalore', 50000),  -- Address changed (UPDATE)
(3, 'Charlie', 'Delhi', 65000),    -- Salary changed (UPDATE)
(4, 'Diana', 'Kolkata', 58000);    -- New record (INSERT)


-- Step 2: Identify INSERTS (records in staging but not in existing)
-- ============================================================================

SELECT 
    s.*,
    'INSERT' AS change_type
FROM staging_table s
LEFT JOIN existing_table e ON s.id = e.id
WHERE e.id IS NULL;

-- Result: ID 4 (Diana) is a new insert


-- Step 3: Identify UPDATES (records that exist in both but have different values)
-- ============================================================================

SELECT 
    s.id,
    s.name,
    s.address,
    s.salary,
    'UPDATE' AS change_type,
    -- Show what changed
    CASE WHEN s.name != e.name THEN 'name' ELSE '' END AS name_changed,
    CASE WHEN s.address != e.address THEN 'address' ELSE '' END AS address_changed,
    CASE WHEN s.salary != e.salary THEN 'salary' ELSE '' END AS salary_changed
FROM staging_table s
INNER JOIN existing_table e ON s.id = e.id
WHERE 
    s.name != e.name OR
    s.address != e.address OR
    s.salary != e.salary;

-- Result: ID 1 (address changed), ID 3 (salary changed)


-- Step 4: APPLY CHANGES - Method 1 (Using MERGE - SQL Server, Oracle, PostgreSQL 15+)
-- ============================================================================

MERGE INTO existing_table AS target
USING staging_table AS source
ON target.id = source.id
WHEN MATCHED AND (
    target.name != source.name OR
    target.address != source.address OR
    target.salary != source.salary
) THEN
    UPDATE SET
        target.name = source.name,
        target.address = source.address,
        target.salary = source.salary
WHEN NOT MATCHED THEN
    INSERT (id, name, address, salary)
    VALUES (source.id, source.name, source.address, source.salary);


-- Step 5: APPLY CHANGES - Method 2 (Using separate INSERT and UPDATE)
-- ============================================================================

-- Insert new records
INSERT INTO existing_table (id, name, address, salary)
SELECT s.id, s.name, s.address, s.salary
FROM staging_table s
LEFT JOIN existing_table e ON s.id = e.id
WHERE e.id IS NULL;

-- Update existing records
UPDATE existing_table e
SET 
    e.name = s.name,
    e.address = s.address,
    e.salary = s.salary
FROM staging_table s
WHERE e.id = s.id
AND (
    e.name != s.name OR
    e.address != s.address OR
    e.salary != s.salary
);


-- Step 6: COMPLETE CDC PROCESS with audit columns

________________________________________________
https://www.youtube.com/watch?v=rYbk-Y8Mx1g
Legacy Hive Metastore "Before Unity catalog we used to have one default catalog
which is called Hy metast store and we used to save all our structured data assets under this Hy meta store". 
"In order to use this we need a compute". 
"If you don't specify any schema while creating a table table would go directly under this default schema".
Managed Tables "What exactly is a manage table it means both your metadata which is your schema and your data would be managed by The Meta store"-.
"Once you drop your manage table both your meta data which which is your schema and your data would be gone". 
"This is the manage location where datab stores the data for a manage table until you define the location explicitly for an external table". "Whenever you create a manage table in data brakes the default table that is created is off form Delta".
External Tables "This time I've defined a location where your data would be stored". 
"This is external table okay and we have defined a location here 
and this is the location where the table data would be stored".

create schema brunch
create table br.EMP ...
select star from brunch.EMP
create table emp_EXT location '...dbfs/temporary/directory/EMP_EXT...'

____________________________________________________________-
https://www.youtube.com/watch?v=rYbk-Y8Mx1g
unity catalog

**On Governance and Unity Catalog**
*   "when we talk about governance it means we need to make our data secure available and accurate".
*   "Unity catalog is first of all an open source unified governance".
*   "this unified means you have to Define it once and you can secure it everywhere".
*   "it provides security it provides auditing it provides lineage capability it provides data Discovery".
*   "without Unity catalog you have to manage each databas workspace individually".
*   "with unity catalog you can maintain everything from a single place".

**On the Object Model**
*   **Metastore:** "in the object model topmost is the metastore and it is used to maintain your metadata". "metadata is your data about data it means it captures your schema it captures your Access Control List It captures your permissions and all sort of things". "one region should have one meta store".
*   **Catalog:** "catalog is the data securable object". "catalogs are basically used to maintain our data assets".
*   **Schemas:** "you can Define schemas in order to maintain those into different schemas".
*   **Tables and Views:** "in order to maintain structured data you can use tables and Views".
*   **Volumes:** "volumes are basically some file systems that can be used to maintain either structured data or unstructured data or semi-structured data".
*   **Non-data securable objects:** "storage credentials external location share recipient provider connection and clean rooms... are used to maintain nonata securable objects".

**On Three Name Space and the Example**
*   **The explanation:** "three name space means you have to Define catalog schema and the table name in order to access it".
*   **The specific example:** "so if you have a catalog called Dev and if you have a schema 
called Bron and if you have a table called sales then your table name would be something like dev. br. sales".

       METASTORE
            (Topmost Object) [1]
                     |
       ______________|______________
      |                             |
      |                             |
NON-DATA SECURABLE            DATA SECURABLE
    OBJECTS                      OBJECTS
 ("Left Hand Side")             (Main Focus)
      |                             |
      |-- Storage Credentials [2]   |-- CATALOG [2]
      |-- External Location [2]     |   |
      |-- Share [2]                 |   |-- SCHEMA [3]
      |-- Recipient [2]             |       |
      |-- Provider [2]              |       |-- Tables [3]
      |-- Connection [2]            |       |-- Views [3]
      |-- Clean Rooms [2]           |       |-- Volumes [3]
                                    |       |-- ML Models [3]
                                    |       |-- Functions [3]
_________________________________________________________________________________________________
https://www.youtube.com/watch?v=VDZ7erjooU0&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=15
Spark SQL Engine (Catalyst Optimizer)**


**Potential Interview Questions Discussed:**
Video ke start mein bataya gaya hai ki interview mein kya puchha ja sakta hai:
*   Catalyst Optimizer ya Spark SQL Engine kya hai aur kaise kaam karta hai?
*   **Analysis Exception** error kyu aata hai aur isko kaise resolve kiya jata hai?
*   **Catalog** kya hota hai?
*   **Physical Planning** ya **Spark Plan** kya hai? (Dono term same hain).
*   Kya Spark SQL Engine ek **Compiler** hai? (Answer: Yes, kyunki ye code ko Java Byte Code mein convert karta hai).
*   Spark SQL Engine mein kitne **Phases** involve hote hain? (Answer: 4 phases).

**The 4 Phases of Spark SQL Engine**
Youtuber ne explain kiya ki ye engine **4 phases** mein kaam karta hai:
1.  **Analysis**
2.  **Logical Planning**
3.  **Physical Planning**
4.  **Whole Stage Code Generation**

In phases ko detail mein aur examples ke saath is tarah samjhaya gaya hai:

**1. Analysis Phase aur Catalog (Example ke saath)**
*   **Concept:** Jab hum code likhte hain to wo "Unresolved Logical Plan" hota hai. Yahan **Catalog** ka use hota hai.
*   **Catalog kya hai:** Catalog ke paas **Metadata** hota hai, matlab "Data ka data." Jaise file ka naam kya hai, kab load kiya gaya, size kya hai, aur table/files exist karte hain ya nahi.
*   **Example (Analysis Exception):**
    Youtuber ne ek example dikhaya jahan unhone ek JSON file ko read kiya.
    Unhone code likha: `df.select("name1")`.
    Lekin actual data mein column ka naam tha **"name"** ("name1" wahan tha hi nahi).
    Is wajah se code "fat gaya" aur **Analysis Exception** error aaya. Catalog ne check kiya aur bataya ki ye column exist nahi karta.
    Jab sab sahi hota hai tab ye **"Resolved Logical Plan"** banta hai.

**2. Logical Planning / Optimization (Example ke saath)**
*   **Concept:** Yahan **Logical Optimization** chalta hai. Spark lazy evaluation ka use karke code ko optimize karta hai.
*   **Example:**
    Agar aapne query likhi `Select * from table` (saare columns lao), lekin baad mein uske upar aap sirf **2 columns** use kar rahe ho.
    To Optimizer samajh jata hai ki user ne pehle pure columns select kiye hain jiski requirement nahi hai. Wo code ko change kar dega taaki shuru se hi sirf wo 2 columns aayein. Isse network se pura data uthane ka bada task bach jata hai.
    Isi tarah agar multiple filters (transformations) hain, to unhe combine kar diya jata hai.
    Isse **"Optimized Logical Plan"** banta hai.

**3. Physical Planning (Example ke saath)**
*   **Concept:** Logical plan se multiple physical plans banaye jate hain. Yahan **Cost Based Model** apply hota hai (kisme memory aur CPU kam lagega). Jo sabse best hota hai use **"Best Physical Plan"** kehte hain.
*   **Example (Broadcast Join):**
    Agar aapne **Join** lagaya hai aur ek table bada hai aur ek table chhota hai.
    Physical planning mein decide hota hai ki kya karein. Example diya ki chhote wale table ko saare executors ke paas bhej dete hain (Broadcast kar dete hain).
    Executors ko bola jata hai "tumhare paas chhota table hai, ise join kar lo."
    Isse data ko **Shuffle** (idhar-udhar move) nahi karna padta, jo ki ek expensive operation hai. Ye cost bacha leta hai.

**4. Whole Stage Code Generation**
*   End mein, jo Best Physical Plan hota hai wo **RDDs** (Resilient Distributed Datasets) mein convert hota hai.
*   Ye code alag-alag **Executors** ke paas bheja jata hai run karne ke liye.
*   Youtuber ne confirm kiya ki Spark SQL Engine ek compiler hai kyunki ye finally code ko **Java Byte Code** mein convert kar raha hai.

__________________________________________________________________________________
https://www.youtube.com/watch?v=M0Kx205dxmM&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=14
How to read json file in pyspark

1. JSON Kya Hota Hai (What is JSON) JSON ka full form hota hai JavaScript Object Notation. Ye ek key-value pair ka data hai.
‚Ä¢ Example: Agar hum tabular format ki baat karein jahan ID aur Name hai, toh JSON mein hum curly bracket { se start karte hain. Pehle key likhte hain (column name) double quote mein, phir colon :, aur phir value.
    ‚ó¶ Jaise: {"id": 1, "name": "manish"}.
‚Ä¢ Semi-structured Data: Iska advantage ye hai ki ye semi-structured hai. Maan lo ki ek ka salary hai aur ek ka nahi hai, ya kisi ka ek extra column "Nominee" add karna hai. Tabular format mein humein pura column add karna padta aur baaki jagah null value hoti, jisse space zyada lagta. Lekin JSON mein hum sirf specific record mein wo key-value add kar sakte hain.
2. Line-Delimited vs. Multi-Line JSON Spark mein JSON do type ke aate hain:
‚Ä¢ Line-Delimited: Har ek line mein ek record hota hai. Spark by default line-delimited JSON hi accept karta hai. Isme performance fast hoti hai kyunki wo line-by-line read karta hai.
‚Ä¢ Multi-Line: Agar teen-chaar line mila ke ek JSON record banta hai, toh use multi-line JSON kehte hain. Iske liye humein option("multiline", "true") set karna padta hai.
    ‚ó¶ Performance: Multi-line mein performance thodi degrade milegi kyunki usko pure object ko ek baar mein read aur write karna padta hai.
3. Interview Questions aur Examples
‚Ä¢ Question: Agar 3 keys hain aur ek record mein 4 keys aa gaye toh kya hoga?
    ‚ó¶ Example: Humare paas Name, Age, Salary teen keys hain, lekin ek record mein humne ek extra key Gender de diya.
    ‚ó¶ Explanation: Jab hum isko read karenge, toh Spark ek Gender column generate karega. Jahan-jahan usko value nahi mila wahan null set kar dega, aur jahan value mila (jaise "Male"), wahan value set kar dega.
‚Ä¢ Question: Corrupted JSON (Invalid File) ke case mein kya hoga?
    ‚ó¶ Example: Ek record mein open bracket { hai lekin closing bracket } humne jaanbujh ke hata diya.
    ‚ó¶ Explanation: Spark error throw nahi karega balki uss specific row ko corrupted record batayega. Wo ek column bana dega jahan aapko wo data show karega ki ye data corrupt hai.

_____________________________________________________________
data lake vs delta lake vs data warehouse
https://www.youtube.com/watch?v=t6i6fQilAm8&t=81s
**What is Delta Lake?**
The speaker defines Delta Lake as an "open format storage layer that delivers reliability, security and performance on your data lake". It acts as a "single home for structured semi-structured and unstructured data".

To explain this, the video compares three components:

*   **Data Warehouse:** This is a traditional concept for storing huge amounts of data. However, it has limitations. It accepts "only structured data". Also, the computing resources (RAM, processor, storage) are "tightly coupled," meaning you cannot increase one component without exponentially increasing the others.
*   **Data Lake:** This solution "came to picture" to solve data warehouse problems. You can dump "huge amount of data" and it accepts any format (structured, semi-structured, or unstructured). It is "limitlessly scalable" with minimal effort. However, it has "poor support to asset [ACID] transactions" and is not suitable for DML operations.
*   **Delta Lake:** This is the "combination of best features" from the data warehouse and data lake. It is a storage layer "sitting on top of the data lake". It supports ACID transactions, scales easily, and ensures that if a process fails, it "will not leave the system corrupted".

**The Example Explained in the Video**
The speaker uses an example regarding **Schema on Write** to show how a Data Warehouse handles data validation compared to Data/Delta Lake.

*   **The Scenario:** "Let's say we are having one table it's containing employee id employee name and salary but the incoming record is having one extra field maybe date of joining".
*   **Data Warehouse Result:** Because the record is not compatible with the schema, "on right it would be discarded it would be removed".
*   **Delta Lake Result:** There is a concept of "schema evaluation," which means "even though there is a mismatch with the schema still it would be accepted".

**Key Features Highlighted**
*   **ACID Transactions:** The video explains that Data Warehouse supports these (Atomic, Consistency, Isolated, Durable), Data Lake has poor support, but Delta Lake "supports acid transactions very easily".
*   **Handling Corruption:** In a Data Lake, if a process fails while doing transformations, it leaves the system in a "corrupted state". Delta Lake, similar to a Data Warehouse, "would not leave the system in corrupt status".
*   **Metadata:** Delta Lake manages metadata. While the Data Lake contains raw data (like CSV or Parquet files), Delta Lake captures metadata "like what is the name of the column what is the data type" and statistics (min/max values) to perform better.
_________________________________
database vs data lake vs data warehouse
https://www.youtube.com/watch?v=Nn7CFCQLKOs&t=938s
### **Database**
*   **Explanation:** A database is "a system which holds data". It deals with "majorly transactional data," which is data "used on day-to-day basis". It is "best meant for oltp purpose" (Online Transactional Processing). It deals with "structured data" that "has certain format it's in the form of rows and columns". It deals with "recent data," perhaps "last five months data six months data," but "cannot hold years of data" because the "cost to store the data in database is high". It follows "schema on write," meaning "while writing itself it will give you an error if there is any mismatch".
*   **Examples:**
    *   "If you go to a bank and swipe your card and you withdraw some 5000 rupees then it's a transaction which is happening".
    *   "You go to a grocery store you purchase something that's a transaction".
    *   "You are purchasing something from amazon that's a transaction".
    *   "One good database" examples are "oracle" and "mysql".

### **Data Warehouse**
*   **Explanation:** A data warehouse "holds more data than a database and is mainly used for analytical purpose". It is for "olap online analytical processing". It is a place which will "keep lot of your historical data" to "find the insights". We migrate data here because if we run "complex queries" for analysis on a database, "day-to-day transactions will become slow". It deals with "structured data" and follows "schema on right" (similar to a database). It follows an "ETL process" (extract, transform, load), which "restricts our flexibility" because "before loading it to our data warehouse itself we need to get the data to a form which is desired". The storage cost is "high but lesser than your database".
*   **Examples:**
    *   "Amazon people want to find some insights around your purchase trends and they want to analyze your last 10 years data".
    *   "One good example of a data warehouse" is "teradata".

### **Data Lake**
*   **Explanation:** A data lake is meant to "get insights from huge amounts of data," but the "data is present in raw format". It can be "structured or... unstructured". It follows a "elt process" (extract, load, and then transform). We can "keep the data in its raw form" and "think of transformation only when required later". It is "cost effective" because it uses "cheap hardware or cheap storage," so you can keep "tens of years of data here freely". It follows "schema on read," meaning "first you put a file... next what i do is i create a structure on top of it". It gives "enough flexibility... to keep all your raw data" and "later whenever you want to use it... feel free to use".
*   **Examples:**
    *   "Hdfs" and "amazon s3" are "good examples of data lakes".
    *   "If i have a log file doc file... we can directly have this file in raw form".
    *   Putting a file like "employee.csv" and creating a "hive table on top of it".
________________________________________
https://www.youtube.com/watch?v=M0Kx205dxmM&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=14

___________________________________________________________________
https://www.youtube.com/watch?v=JHm7BkRp3T8&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=13
Dag and lazy evaluaton

**1. DAG (Directed Acyclic Graph) Kya Hai**
*   DAG ka full form hota hai **Directed Acyclic Graph**. Iska matlab hai ki ye **cycle mein nahi jayega, ye loop mein kabhi nahi jayega**, ye directed hoga aur graph hoga.
*   Jitne bhi humne transformations likhe hain, ye usi ka ek graph banata hai ki pehle ye kaam karenge uske baad ye kaam karenge.
*   **Har ek action pe ek job banega aur har ek job ka apna DAG hoga**,.
*   Visualisation mein jo boxes hote hain wo vertices hote hain aur jo arrow hote hain wo edges hote hain.

**2. Lazy Evaluation**
*   Lazy Evaluation ka matlab hai ki **jab tak hum action ko hit nahi karenge tab tak ye jitni bhi transformations hain wo nahi hogi**.
*   Code line by line run karne ki koshish nahi karta. Ye shanti se baitha hua hai aur wait kar raha hai ki jab tum action hit karoge tabhi hum kaam karna shuru karenge.
*   Ye aisa isliye karta hai taaki ye code ko **optimize** karega.

**3. Example: Flight Data Analysis**
Video mein **Flight CSV data** ka example use kiya gaya hai. Isme teen main columns dikhaye gaye: Destination Country Name, Origin Country Name, aur Count.

Youtuber ne is example ke through Lazy Evaluation aur Optimization explain kiya:

*   **Step 1 (Transformations):** Unhone code ko chote-chote chunk mein chalaya. Pehle `repartition` kiya, fir `filter` lagaya (Narrow dependency), fir `groupBy` lagaya (Wide dependency),.
*   **Observation:** Jab tak sirf transformations run kiye (jaise filter lagana), tab tak **Spark job ka count nahi badha** (wo 2 ka 2 hi raha). Isse pata chala ki ye Lazy Evaluation kar raha hai,.
*   **Step 2 (Action):** Jaise hi last line (Action) ko hit kiya, job create hua aur count 2 se 4 ho gaya.
*   **Internal Optimization:** Example mein do filters lagaye gaye the:
    1.  Destination Country Name = **United States**.
    2.  Origin Country Name = **India** ya **Singapore**.
*   Ideally, pehle United States wala filter hona chahiye tha fir dusra code run hona chahiye tha. Lekin Spark ne **in dono step ko combine kar diya** aur ek baar mein hi calculate kar liya.
*   Isse ye fayda hua ki usko do baar processing karne ki zarurat nahi padi.

**4. Spark UI**
*   Jo hum application ke status ko dekhte hain (Jobs, Stages, DAG), usi ko bolte hain **Spark UI**,.
*   Yahan par hum dekh sakte hain ki kaise code optimize hua hai aur SQL query kaise bani hai,.

(
Lazy evaluation facilitates internal code optimisation in Spark by allowing the system to wait until
an 'action' is triggered before executing any transformations, rather than processing code line-by-line immediately.
Instead of running each step as it encounters it, Spark sits quietly 
and builds a plan‚Äîspecifically a Directed Acyclic Graph (DAG)‚Äîwhich represents the sequence of jobs required.
This delay allows Spark to view the entire chain of transformations holistically, 
enabling it to combine steps and remove redundancies before touching the data.
)
_____________________________________________________________________
https://www.youtube.com/watch?v=ce-F5lyFeyk&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=12
transformation and action

**1. Transformation aur Action kya hote hain**
*   **Transformation:** Iska matlab hum koi bhi data pe kuch bhi **processing** karenge. Jaise agar hum filter kar rahe hain ya select kar rahe hain columns ko, toh usko transformation bolte hain,.
*   **Action:** Action matlab jab hum bolte hain ki kuch kaam humko chahiye, jaise ki `.show` ya `.count`. Jab tak action hit nahi karoge, Spark data ko scan nahi karega kyunki **Spark lazy evaluation** hai. Woh wait karega aur dekhega ki action hit hua toh hi woh kaam karega,.

**2. Types of Transformations**
Transformations do type ke hote hain:

*   **Narrow Dependency Transformation:** Aise transformations jisme data movement ki requirement nahi hoti between partitions. Isme ek executor akele apna kaam kar lega, usko dusre executor ya partition pe depend nahi rehna padta,,.
*   **Wide Dependency Transformation:** Jahan pe ek executor ko dusre executor se baat karke cheezein sort out karni padegi. Isme data ki movement hoti hai ek jagah se dusri jagah, aur jab ye movement hoti hai toh ye **expensive operation** ho jata hai. Hamesha Wide Dependency se bachna chahiye. Examples hain: `Group By`, `Join`, `Distinct`.

**3. Examples jo Video mein explain kiye gaye (Employee Data)**
YouTuber ne ek CSV file ka example liya jisme Employee ka data hai (ID, Name, Income, Age) aur do questions solve kiye concept samjhane ke liye,.

*   **Example 1: Narrow Dependency (Filter)**
    *   **Question:** "Show me the employee who is less than 18." (Aise employee dikhao jinki age 18 se kam hai).
    *   **Explanation:** Maan lo data ke do partition hain aur do executor hain. Executor 1 apne data mein check karega aur Executor 2 apne data mein. Dono apna-apna result return kar denge. Kahin pe bhi **data ko move karne ki zarurat nahi padi**. Isliye ye **Narrow Dependency** hai,.

*   **Example 2: Wide Dependency (Group By)**
    *   **Question:** "Find out the total income of each employee." (Kyunki ek employee ke multiple source of income ho sakte hain),.
    *   **Explanation:** Agar hum `Group By ID` karte hain, aur maan lo ID '2' Executor 1 ke partition mein bhi hai aur Executor 2 ke partition mein bhi hai. Agar data shift nahi karenge toh answer galat aayega.
    *   Sahi answer ke liye **Data ki Shuffling** hogi. Matlab same ID wale data ko same partition mein move karna padega taaki total sum calculate ho sake. Jab data idhar se udhar network mein move hoga, toh ye **expensive transformation** (Wide Dependency) kehlayega,.

**4. Driver Out of Memory aur Jobs**
*   Jab bhi action hit hota hai (jaise `.collect`), toh executor sara data process karke **Driver** ko bhejta hai.
*   Agar tumne zyada data maang liya (maan lo Driver 10GB ka hai aur 15GB data maang liya), toh **Driver Out of Memory Exception** aayega aur job fail ho jayega.
*   Jab bhi action hit hoga, tab tab naya **Job** create hoga.
__________________________________________
https://www.youtube.com/watch?v=wXGhaNGA7KA&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=11
Handling courupted record

**1. Potential Interview Questions**
Video ki shuruat mein bataya gaya hai ki is topic se related ye questions pooche ja sakte hain:
*   Kya aapne kabhi corrupted record ko handle kiya hai?
*   Humein kaise pata chalega ki record corrupted hai?
*   Agar hum alag-alag "Read Mode" mein read kar rahe hain to corrupted record ke saath kya hoga?
*   Bad records ko hum kaise print karwa sakte hain?
*   Corrupted records ko kahan store karte hain? (Kyunki agar hazaar records corrupt hain to hum print nahi karwaenge, store karenge).

**2. Corrupted Record ka Example**
Unhone CSV aur JSON ke examples diye hain:
*   **JSON:** Agar koi record hai aur uska last mein closing (bracket) nahi hai, to wo 'bad record' ho jayega kyunki wo properly close nahi hua.
*   **CSV (Main Example):** Ek table dikhaya gaya hai jisme 6 columns hain. Isme third record ke "Address" column mein "Bangalore, India" likha hai. Kyunki CSV ka matlab "Comma Separated Value" hota hai, to ye Bangalore aur India ko alag-alag consider kar lega. Is wajah se 'India' agle column 'Nominee' mein chala jayega aur data shift ho jayega. To ye ek bad record ban jayega.

**3. Different Read Modes ke Results**
Video mein practical karke dikhaya gaya ki same CSV file ko alag-alag mode mein read karne par kya aata hai:
*   **Permissive Mode:** Isme paanchon ke paanchon records aa jayenge. Jo record galat hoga, Spark usko "Null" set kar dega lekin error throw nahi karega.
*   **Drop Malformed:** Ye corrupted record ko drop kar dega. Example mein 2 record galat the (ek Bangalore wala aur ek Kolkata wala), to total 3 records hi aayenge.
*   **Fail Fast:** Isme 0 record aayega kyunki jaise hi bad record milega, wo process ko fail kar dega.

**4. Bad Records ko Print karna**
Bad records ko print karne ke liye "Manual Schema" define karna padta hai:
*   Schema mein ek extra column `_corrupt_record` (underscore corrupt underscore record) banana padta hai jiska type String hota hai.
*   Jab isko run karte hain, to corrupted data us column mein aa jata hai. Pura data dekhne ke liye `truncate=False` ka use karte hain.

**5. Corrupted Records ko Store karna**
Agar bahut saare records corrupted hain, to unhe store karne ke liye `badRecordsPath` option ka use karte hain:
*   Value mein wo path dete hain jahan aap store karna chahte hain. Ye ek directory bana deta hai aur wahan JSON format mein file store hoti hai.
*   Is command ko chalate waqt 'mode' specify karna allowed nahi hota.
*   Baad mein us JSON file ko read karke dekh sakte hain ki kis file aur kis reason ki wajah se record corrupt hua.

df = spark.read.format("csv") \
    .schema(employee_schema) \
    .option("header", "true") \
    .option("badRecordsPath", bad_records_path) \
    .load("dbfs:/FileStore/tables/employees.csv")
employee_schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("name", StringType(), True),
    # ... (other columns like address, nominee etc.)
    StructField("_corrupt_record", StringType(), True) # This column captures the bad data
])

# Read using the schema
df = spark.read.format("csv") \
    .schema(employee_schema) \
    .option("header", "true") \
    .load("dbfs:/FileStore/tables/employees.csv")

___________________________________________________________
https://www.youtube.com/watch?v=U3sgM-ohLVE&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=10
Schema in Spark

**Introduction & Interview Questions**
Starting mein hi humne interview questions discuss kar liye taaki humein idea lag jaye ki agar hum ye padhte hain toh interview mein kis tarah ke questions aa sakte hain. Possible questions ye hain:
1.  **How to create schema in PySpark?** (Manual schema kaise create karte hain).
2.  **What are other ways?** (Kya iske alawa koi aur tarika hai schema create karne ka?).
3.  **StructType aur StructField** ka concept kya hota hai?.
4.  Agar header pehle se pada hua hai aur hum header ko false kar dein, yaani **rows skip karna hai**, toh wo kaise karenge?.

**Key Concepts Explained**

*   **Schema create karne ke do tarike hain:**
    1.  **StructType aur StructField** ke through.
    2.  **DDL String** ke through.

*   **StructType & StructField:**
    *   **StructType:** Ye Dataframe ke structure ko define karta hai. Ye basically collection of StructFields hai.
    *   **StructField:** Ye column ki information rakhta hai. Jaise ki column ka naam, uska datatype (kya wo integer hai ya string hai), aur kya wo value null ho sakti hai (nullable true/false).

**Example 1: Basic Data Frame (Laptop Screen Explanation)**
Youtuber ne concept samjhane ke liye ek chhota sa Data Frame banaya jisme teen columns the: **ID, Name, aur Age**.

1.  **Method 1 (StructType & StructField):**
    Unhone `pyspark.sql.types` se `StructType` aur `StructField` import kiya.
    Code kuch aisa likha:
    ```python
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("name", StringType(), True),
        StructField("age", IntegerType(), True)
    ])
    ```
    Isme list pass kiya kyunki multiple columns the.

2.  **Method 2 (DDL String):**
    Ye method bahut aasan hai. Isme bas string pass karna hai column name aur type ke saath.
    Example:
    ```python
    ddl_schema = "id integer, name string, age integer"
    ```
    Youtuber ne kaha ki ye "bacchon wala kaam" hai agar pehla method samajh aa gaya toh.

**Example 2: Flight Data (Databricks Notebook Implementation)**
Phir unhone same cheez ko Databricks notebook par replicate kiya using **Flight Data CSV**.

*   **Scenario:** Unhone `header` ko `false` kar diya kyunki unhe apna manual schema banana tha.
*   **Schema Definition:** Unhone `my_schema` define kiya jisme columns the:
    *   `DEST_COUNTRY_NAME` (StringType)
    *   `ORIGIN_COUNTRY_NAME` (StringType)
    *   `count` (IntegerType).

*   **Issue (Null Values):**
    Jab data read kiya, toh `count` column mein `null` aa raha tha. Kyunki data mein pehla row header tha ("count" likha hua), aur schema mein wo Integer tha. String "count" integer nahi ho sakta, isliye Spark ne usko null kar diya.

*   **Solution (Skip Rows):**
    Agar humein header khud se hatana hai (kyunki humne `header=false` kiya hai lekin file mein header hai), toh hum `.option("skipRows", 1)` use kar sakte hain.
    Jaise hi `skipRows` 1 kiya, pehla row skip ho gaya aur sahi data (integers) aane laga.

Youtuber ne end mein ye bhi bataya ki ye sab options (jaise `skipRows`, `mode`) samajhne ke liye Spark documentation padhna chahiye.
___________________________________________________________________________
https://www.youtube.com/watch?v=xJVonk4yxJY&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=9

spark architecture
**1. Cluster Setup (Example Scenario)**
Sabse pehle unhone Cluster ka example diya hai. 
*   "Cluster matlab kya? Bahut sare computers ko ek saath connect kar do network ke through."
*   **Example Setup:** Unhone 10 machines ka example liya hai.
    *   "20 core per machine."
    *   "Har ek machine me 100 GB RAM laga hua hai."
    *   Total calculate karein toh: "200 core" aur "1000 GB (1 TB)" RAM ka cluster hai.

**2. Master-Slave Architecture**
Cluster kaise kaam karta hai, isko unhone "Malik aur Naukar" ke example se samjhaya hai:
*   Cluster mein ek "Master" hota hai (Malik) aur baaki "Slaves" hote hain (Naukar).
*   Master node par "Resource Manager" install hota hai.
*   Slave nodes ko "Worker Node" ya "Node Manager" bolte hain.

**3. Application Submission Flow (Step-by-Step)**
Developer jab code submit karta hai, toh process kaise chalta hai:
*   **Request:** Developer ek application run karne ko bolta hai aur demand karta hai: "Hamein Driver de do 20 GB ka, aur Executors 25 GB ka aur number of Executors 5 aur CPU core de do 5".
*   **Driver Allocation:** Ye request "Resource Manager" ke paas jati hai. Wo dekhta hai ki Driver chahiye, toh wo kisi ek Worker Node (jaise example mein `w5` liya) ko bolta hai: "Please create container with 20 GB of RAM".
*   **Application Master:** Worker node (`w5`) apne 100GB mein se 20GB ka ek Container banata hai. Is container ke andar "Spark Application" run hota hai. Spark mein is container ko "Application Master" bolte hain.

**4. PySpark Internal Working**
Container ke andar kya hota hai, ye bahut important part hai:
*   Agar hum Python (PySpark) use kar rahe hain, toh bhi internally ek "JVM ka main method" create hota hai kyunki Spark core Scala (Java based) mein likha hai.
*   Python code pehle convert hota hai aur Java wrapper ke through chalta hai. Is JVM main method ko "Application Driver" bolte hain.
*   Agar humne Java/Scala mein code likha hai toh seedha Application Driver banta hai, lekin Python mein "PySpark Driver" aur "Application Driver" dono hote hain.

**5. Executor Allocation**
Jab Driver set ho jata hai, tab wo baaki resources mangta hai:
*   Application Driver wapas "Resource Manager" ke paas jata hai aur bolta hai ki "5 Executors, 25-25 GB ke aur 5 Core ke saath" chahiye.
*   Resource Manager dekhta hai kaunse Workers free hain. Example mein usne `w2, w3, w4, w7, w8` ko choose kiya aur wahan 5 containers create kar diye.
*   Ab Driver in containers (Executors) ko data aur tasks bhejta hai.

**6. Python UDF (User Defined Function) ka Masla**
YouTuber ne Python UDF use karne se mana kiya hai aur uska reason bataya hai:
*   Agar humne application mein "User Defined Function (UDF)" likha hai, toh Executer ke andar sirf JVM se kaam nahi chalega.
*   Wahan "Python Worker" ki zarurat padegi runtime environment mein.
*   Isliye bola jata hai ki UDF mat use karo, kyunki "Python Worker" aane se performance slow ho jati hai. In-built functions use karoge toh performance zyada milega.

**Summary:**
Process simple hai: Developer Resource Manager se Driver mangta hai -> Driver create hota hai -> Driver Resource Manager se Executors mangta hai -> Executors milte hain -> Kaam shuru hota hai. Jab kaam complete ho jata hai, Driver aur Executors band ho jate hain.
_____________________________________________________________________________
https://www.youtube.com/watch?v=aDCfzM7Cs7c&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=8
Read Data in Spark
Video mein bataya gaya hai ki data ko Spark mein kaise read karte hain. Iska ek core structure hota hai DataFrameReader API ke through.
1. General Format: Data read karne ke liye hum spark.read use karte hain, aur uske baad alag-alag options aate hain:
    ‚ó¶ .format(): Yahan hum file format batate hain, jaise CSV, JSON, ya Parquet. Agar hum .format() nahi dete hain, toh by default Spark parquet format samajhta hai.
    ‚ó¶ .option(): Isme hum key aur value pass karte hain (jaise header, mode). Ye optional field hai, hum de bhi sakte hain aur nahi bhi.
    ‚ó¶ .schema(): Agar humein manually batana hai ki column integer hoga ya string, toh hum schema define kar sakte hain.
    ‚ó¶ .load(): Yahan humein batana hota hai ki data kahan reside kar raha hai (data ka path/location).
2. Read Modes in Spark: Video mein explain kiya gaya hai ki data read karte waqt 3 tarike ke modes hote hain, aur interview mein isse question aate hi aate hain:
    ‚ó¶ Fail Fast: Jaise hi koi corrupted record (malformed record) mile, execution ko fail kar do.
    ‚ó¶ Drop Malformed: Ye corrupted record ko seedha drop kar dega.
    ‚ó¶ Permissive: Ye default mode hota hai. Agar koi corrupt record milega, toh ye us value ko null set kar dega.
_____________________________________________________________________________________
https://www.youtube.com/watch?v=7x9GXt6-AxU&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=7
**Spark Ecosystem ka Structure**
Video mein Spark ke structure ko do hisson mein baant kar samjhaya gaya hai:

*   **Low Level API (Spark Core):** Spark ka jo niche wala section hai, use "Spark Core" bolte hain. Isme RDD (Resilient Distributed Datasets) aata hai. Agar humein bilkul hi low level par jaakar kuch likhna hai, ya koi aisi cheez achieve karni hai jo High Level API se nahi ho pa rahi, toh wahan par hum RDD ka use karte hain.
*   **High Level API (Libraries):** Jo top mein hai, wo "Set of Libraries" hai jise "High Level API" bhi bolte hain. Isme Data Frame aur Spark SQL aata hai jo Spark Core ke upar run karta hai. Iske alava agar streaming karni hai, machine learning karni hai, ya graph wali cheezein karni hain (GraphX), toh uski libraries bhi available hain.
*   **Kaise Run hota hai:** Hum jo bhi kuch karne wale hain (High Level API mein), wo internally transform hoga Spark ke Core mein (RDD mein) aur phir Spark ke Computer Engine mein jayega.
*   **Languages:** Spark ne flexibility di hai ki aap apne code ko multiple languages mein likh sakte ho jaise Python, Java, R, aur Scala.

**Hardware aur Cluster Manager**
YouTuber ne samjhaya hai ki Spark na hi storage karta hai aur na hi Spark ka khud ka kuch cluster manager hota hai.
*   Sabse base par "Hardware" ya "Computer Machines" baithe hote hain.
*   In machines ko manage karne ke liye inke upar "Cluster Manager" baitha hota hai jo hardware se baat karta hai.
*   Cluster Manager ke kuch popular naam hain: YARN (Yet Another Resource Negotiator), Mesos, aur Standalone Cluster.

**Example (100 GB Calculation)**
Video mein process ko run karne ka example diya gaya hai ki kaise Spark Engine aur Cluster Manager interact karte hain:

1.  Spark Engine Cluster Manager ke paas jayega aur bolega ki humein "20 GB ki 4 Executor" chahiye aur "ek 20 GB ka Driver" chahiye.
2.  Total requirement calculation:
    *   Executors: $20 \times 4 = 80$ GB
    *   Driver: $20$ GB
    *   **Total = 100 GB**.
3.  Cluster Manager check karega ki uske paas 100 GB available hai ki nahi.
4.  Agar memory hai, toh wo process ko run karne ke liye bhejega. Agar nahi hai, toh wo bolega ki hum queue mein rakh dete hain jo "FIFO order" (First In First Out) mein chalta hai. Matlab jo pehla application run hoga, use memory pehle mil jayega.
______________________________________________________________________________
https://www.youtube.com/watch?v=mpMvOPhNtjA&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=5
Hadoop vs Spark

**1. Market mein Misconceptions (Galatfehmiyan)**
Video ke starting mein 3 main misconceptions batayi gayi hain:
*   **Hadoop Database nahi hai:** "Hadoop aisa koi database nahi hai yeh framework hai... Log Hive use karte hain to unhein lagta hai ki yeh relational database hai... lekin actual mein wo ek file system hota hai".
*   **Spark Speed:** "Spark jo hai wo 100 times faster hota hai lekin ye actual aisa nahi hota hai... yeh 100 times tak ja sakta hai, 'up to' hota hai".
*   **RAM Usage:** "Teesri misconception hai wo hai ki Spark jo hai wo RAM mein data ko process karta hai lekin Hadoop aisa nahi karta hai... Hadoop ko bhi RAM ki hi zarurat hoti hai".

**2. Performance Difference (Kaise kaam karte hain)**
*   **Hadoop Process:** "Hadoop is slower than Spark... Hadoop ko bhi RAM ki hi zarurat hoti hai... lekin usko baad mein usko write kar deta hai disk par... to jo RAM se disk write karne mein time jata hai wahan se slow ho jata hai". Example ke liye, Mapper load kiya, reduce kiya aur fir wapas disk pe write kiya. Fir agla process disk se read karega. Ye baar-baar Read/Write cycle chalta hai.
*   **Spark Process:** "Spark ke case mein kya ho raha hai ki... alag-alag executor isko read kar liya... write karega kisi apne executor ke... memory hoga jo ki basically RAM hai... to basically yeh kya kar raha hai ki yeh kabhi disk par gaya hi nahi... iski wajah se iska process fast ho gaya".
*   **Exception Case:** Agar data cluster mein wahin par fit ho ja raha hai (jaise 10GB ka data hai aur sirf count karna hai), toh "uss case mein... time ke case mein... 100 times faster ya 100 times slower hai aisa case to nahi aayega", hardley 1-2 times ka difference hoga,.

**3. Batch vs Streaming**
*   **Hadoop:** "Hadoop build kiya hai batch data processing ke liye... us samay... data bahut zyada volume mein generate hona shuru ho gaya hai... to wo batch mein process karne ke liye banaya gaya tha".
*   **Spark:** "Spark ke paas advantage hai... ye batch ko to handle kar hi sakta hai uske alawa ye streaming ko bhi handle kar sakta hai... kyunki humein jaldi-jaldi data ko process karna hai",.

**4. Ease of Use (Code likhna)**
*   **Hadoop:** "Hadoop mein kya hai ki code likhna bahut hi mushkil tha starting se hi... thoda tough hai likhna".
*   **Spark:** "Spark mein aur bhi easy bana diya gaya hai... Spark ne High Level API de rakha hai ki yahan se aap apne alag-alag language ke through likh sakte ho... Java, SQL, Python, R ye saare language ke through".

**5. Security**
*   **Hadoop:** "Security mein Hadoop ko zyada security hai". Ye "Kerberos authentication ka use karta hai" check karne ke liye ki user authorized hai ya nahi,. Folders access karne ke liye ye "ACLs (Access Control Lists)" use karta hai.
*   **Spark:** "Spark directly aisa koi humein security ke liye nahi deta hai... Spark jo hai hamara wo kya karta hai ki wo HDFS ka storage use kar leta hai... aur jaise hi YARN use karega apna resource negotiate karne ke liye to isko Kerberos mil jata hai".

**6. Fault Tolerance (Video wala Example)**
Manish ne Fault Tolerance samjhane ke liye ye example diya:

*   **Hadoop Example (Replication):**
    Man lo ek data hai "260 MB ka... 128 MB ka ek block size banta hai... to iske kitne banenge teen banenge... A, B aur C". Hadoop replication factor use karta hai (jaise ki 3). "A jo hai wo teen jagah pe store hoga alag-alag node pe... Machine 1, Machine 2 aur ye Machine 3".
    **Agar fail hua to:** "Maan lo ki hum data read kar rahe the Node 1 se... ab ye fail ho gaya... to A ko dekhega ki achha A ka data yahan pe bhi pada hua hai (dusre node par)... koi ek data ko read kar lega... iss tarah se hamara Hadoop fault tolerate hota hai",.

*   **Spark Example (DAG & Lineage):**
    "Spark kya karta hai ki Spark jo hai na wo DAG banata hai Directed Acyclic Graph". Iske paas RDD hota hai. "Isko pata hai ki process 1 ke baad fail hua hai... to process 1 ko banane ke liye kya laga tha kahan se data laga tha wo iske paas information hoti hai... wo us tarike se us data ka use karke wapas bana lega".

____________________________________________________________________________________
https://www.youtube.com/watch?v=N3vk5i_Hh78&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=3
why apache spark

**1. Apache Spark ke aane se pehle kya tha? (Databases)**
Video mein bataya gaya hai ki Spark ke aane se pehle hamare paas **Databases** hote the (jaise Oracle, Teradata, MySQL). Ye systems sirf **Structured Format** wale data ko store karte the.
*   **Example:** Structured data ka matlab hai **Tabular format** mein data, jisme fixed rows aur columns hote hain. Isko aap apne **Excel sheet** jaisa samajh sakte ho.

**2. Problem kahan shuru hui? (Data Formats and Internet)**
Internet ke invention ke baad data alag-alag formats mein generate hone laga:
*   **Files:** Text files, CSV files.
*   **Multimedia:** Images, Videos.
*   **Semi-Structured Data:** JSON ya YAML format (jahan fixed structure nahi hota, key-value pairs hote hain),.
Purane databases sirf tabular data handle kar pa rahe the, baaki formats (Unstructured aur Semi-structured) ko handle karne ke liye koi system nahi tha.

**3. Big Data aur "Three Vs"**
Yahan "Big Data" ke concept ko samjhaya gaya hai **Three Vs** ke through:
*   **Volume:** Sirf size (jaise 100GB ya 1TB) hone se wo Big Data nahi ban jata.
*   **Velocity:** Data kis speed se aa raha hai.
    *   **Example:** Agar 10TB data **per second** ya **per hour** aa raha hai, tab hum bol sakte hain ki Big Data ki problem hai.
*   **Variety:** Data ke alag-alag forms (Structured, Semi-structured, Unstructured). Aaj kal market mein **Unstructured data** sabse zyada produce hota hai.

**4. ETL vs. ELT (Data Handling ka change)**
Pehle hum **ETL** (Extract, Transform, Load) karte the (Data Warehouse concept). Lekin ab data ka volume aur velocity itna badh gaya hai ki hum **ELT** (Extract, Load, Transform) karte hain.
*   Iska matlab pehle hum source se data utha ke ek dump jagah (Data Lake) pe **Load** kar lete hain, aur baad mein jab time milta hai tab usko **Transform** karte hain,.

**5. Main Issues: Storage and Processing**
Itna zyada data aane se do badi problems aayi:
1.  **Storage:** Itna data store kahan karein?
2.  **Processing:** Data ko process karne ke liye RAM aur CPU chahiye,.

**6. Solution Approaches: Monolithic vs. Distributed**
In problems ko solve karne ke liye do options the:

*   **Monolithic Approach (Vertical Scaling):**
    *   Isme hum ek hi system ko bada banate hain (jaise hard disk ya CPU add karte jana).
    *   **Problem:** Iski ek limit hoti hai (Heat dissipation issue, performance drop). Ye **Expensive** hota hai aur isme **Single Point of Failure** hota hai (agar system fail hua to sab band),.

*   **Distributed Approach (Horizontal Scaling):**
    *   Isme hum multiple saste computers (commodity hardware) ko add karte hain.
    *   **Benefits:**
        *   Ye **Economical** (sasta) padta hai.
        *   **Unlimited Scaling:** Hum jitne chahein computers add kar sakte hain.
        *   **High Availability:** Agar ek machine fail ho gayi, to doosri machine kaam sambhal leti hai,.

**Conclusion:**
Isi distributed approach ki wajah se pehle **Hadoop** aaya aur uske baad **Spark** aaya taaki hum Big Data ki storage aur processing problems (Velocity, Variety, Volume) ko solve kar sakein.
____________________________________________________________________________________________________
https://www.youtube.com/watch?v=xW_GNBDW568&list=PLTsNSGeIpGnFiErPovNizG_2IP2RvrgnK&index=2
spark overview 

**What is Apache Spark?**
Video mein definition di gayi hai: "Apache Spark is a unified computing engine and set of libraries for parallel data processing on a computer cluster". Is definition ko samajhne ke liye YouTuber ne har term ko explain kiya hai:

**1. Unified**
*   **Explanation:** Unified ka matlab hota hai "sab cheez ko ek jagah la dena."
*   **Example:** Video mein example diya hai ki Data Engineer (pipeline banane wala), Data Analyst (sales analysis karne wala), aur Data Scientist (future predict karne wala)‚Äîagar ye teeno "ek hi software ya ek hi jagah pe kaam kar sakte hain, toh usi ko bolte hain Unified." Spark mein ye sab log ek hi computing engine par kaam kar sakte hain,.

**2. Computing Engine**
*   **Explanation:** Spark data ko store nahi karta hai. "Basically ye sara cheez RAM mein ho raha hota hai," jo ki permanent storage nahi hai.
*   **Example:** Jaise hamare laptop mein CPU task perform karta hai. Example diya ki "2+5 = 7... ye humein jo compute karne wali cheez hai wo kaun dega? Wo humein dega Spark." Agar 5 Terabyte ka data hai aur usme +1 karna hai, toh wo Spark compute karega. Storage ke liye S3, HDFS, ya RDBMS use hota hai kyunki Spark ke paas storage nahi hota,.

**3. Set of Libraries**
*   **Explanation:** Ye "set of code hota hai jo ki humein likh kar de diye jaate hain."
*   **Example:** Jaise aapne "Pandas" suna hoga, waise hi Spark mein libraries hoti hain jiska use karke hum aage ke kaam perform karte hain.

**4. Parallel Data Processing**
*   **Explanation:** Ek task ko multiple logon mein divide karke karwana.
*   **The Father-Son Example:** Yahan ek example explain kiya hai: "Maan lo ki ek baap hai, uske char bete hain. Ab baap ke paas total 10 task hain... toh wo kya karega do-do ya teen-teen task de dega do beton ko." Is tarah 10 task char beton mein divide ho gaye. "Charon bete apne apne independent task karenge aur shaam ko aakar result de denge." Agar baap akele karta toh time lagta, par beton ne mil kar jaldi kar diya. Same cheez Spark mein hoti hai jahan task alag-alag executors par bhej diya jata hai,.

**5. Computer Cluster**
*   **Explanation:** Spark "Master-Slave architecture" par kaam karta hai. "Ek machine jo ban gaya wo Master ban gaya aur baaki sare jo machines hain wo hamare ban gaye Slave (naukar)."
*   **Architecture Example:** Video mein assume kiya hai ki har computer (worker) ke paas "16GB ka RAM hai aur 1 Terabyte ka storage hai aur 4 core CPU hai."
*   **Process:** Agar Master ko 5 Terabyte data process karna hai, toh wo dekhega ki workers ke paas kitni capacity hai (jaise total 64GB ek baar mein). Master ka kaam hai data ko divide karna aur workers se kaam karwana,,.

Finally, video mein kaha gaya hai ki Spark ek "Unified Computing Engine hai... yaad rakhna storage nahi hota iske paas" aur ye parallel data processing karta hai computer clusters par.
_________________________________________________________
Spark SQL Engine
In this lecture (Lec-8), Manish Kumar explains the Spark SQL Engine (also known as the Catalyst Optimizer) and the four phases it uses to convert high-level Spark code (SQL, DataFrames, or DataSets) into low-level Java bytecode for execution on a cluster.

1. The Spark SQL Engine Flow
The video breaks down the conversion of your code into the following four sequential phases:

Phase 1: Analysis & The Catalog [07:27]
When you write a query, Spark first creates an Unresolved Logical Plan.
It then uses the Catalog (a repository of metadata like table names, column types, and file locations) to verify if the tables or columns you referenced actually exist.
If a column or table is missing, Spark throws an Analysis Exception at this stage [09:13].

Phase 2: Logical Planning / Optimization [10:35]
Once the plan is resolved, Spark applies logical optimizations.
Example: If you write SELECT * but only use two columns later, Spark optimizes the plan to fetch only those two specific columns from the source to save network bandwidth. It also combines multiple filter transformations into one [11:42].

Phase 3: Physical Planning [12:37]
Spark takes the optimized logical plan and generates multiple Physical Plans.
It uses a Cost-Based Model to evaluate these plans. It calculates which plan will use the least amount of CPU and memory.
Example: It decides whether to perform a Broadcast Join (sending a small table to all executors) or a standard shuffle join based on table sizes to avoid expensive data shuffling [12:54].
The best performing plan is selected as the Selected Physical Plan.

Phase 4: Code Generation [06:17]
The final physical plan consists of RDD transformations. Spark uses Whole-Stage Code Generation to convert these into highly optimized Java bytecode that runs on the cluster's executors [14:50].

2. Key Interview Questions Answered [01:40]
What is the Catalyst Optimizer? It is the engine that converts your high-level code into an optimized physical execution plan.
Why do we get an Analysis Exception? This occurs during the first phase if the metadata (table/column names) doesn't match the Catalog [10:06].
What is the Catalog? It is a storage for metadata (data about data), such as file sizes and schema information [08:17].
Is Spark SQL Engine a compiler? Yes, because it ultimately translates high-level Spark code into Java bytecode for the machine to execute [16:39].

3. Example: Analysis Error [09:19]
Manish shows a code snippet where he tries to select a column named name1 from a JSON file, but the actual column name is name. Because the Catalyst Optimizer checks the Catalog during the Analysis phase and finds no match, it throws an error immediately before any data is even processed._______________________________________________________________________
Database vs datalake vs data warehouse
_______________________________________________________________________________________________
. Core Concepts Explained
Database (OLTP):

Purpose: Primarily used for day-to-day transactions (Online Transactional Processing). It focuses on high performance for immediate tasks [01:33].
Data Type: Deals only with structured data (rows and columns) [02:43].
Recent Data: It holds only recent data (e.g., last 5-6 months) to maintain speed; it cannot efficiently store years of historical data [03:12].
Schema: Uses Schema on Write, meaning the structure is validated before data is saved. Any mismatch results in an error during the writing process [04:18].

Cost: The storage cost is high [06:23].
Data Warehouse (OLAP):
Purpose: Used for analytical processing (Online Analytical Processing) and finding insights from large amounts of historical data [07:51].
Source: It aggregates data from multiple databases through an ETL (Extract, Transform, Load) process [14:52].
Schema: Like a database, it uses Schema on Write and deals with structured data [12:41].
Cost: Storage is expensive, though generally cheaper than a primary transactional database [13:16].

Data Lake:
Purpose: Meant to store huge amounts of raw data for later analysis. It offers maximum flexibility for various teams to use the data as they need [18:04].
Data Type: Handles structured, semi-structured, and unstructured data (like log files or docs) in its original form [17:56].
Process: Uses ELT (Extract, Load, Transform). Data is loaded first, and transformations happen only when the data is read [19:00].
Schema: Uses Schema on Read, allowing users to define the structure at the time of analysis [21:01].
Cost: Highly cost-effective as it uses cheap storage hardware [20:16].

2. Examples Provided
Database Example:
Scenario: Swiping your card at an ATM or purchasing an item on Amazon. These are instant transactions where your current balance must be updated immediately [01:16].
Systems: Oracle, MySQL [04:06].
Data Warehouse Example:
Scenario: Amazon analysts wanting to see your purchase trends over the last 10 years to find insights. They move data from the transactional database to a warehouse to run complex queries without slowing down the main site [08:15].
Systems: Teradata [13:16].
Data Lake Example:
Scenario: Storing a raw .csv or log file. You load the file into the system as-is and later create a structure (like a Hive table) on top of it to visualize the data [21:10].
Systems: HDFS (Hadoop Distributed File System), Amazon S3 [19
__________________________________________________________
difference between Partitioning and Bucketing
Partitioning:
How it works: It divides data into multiple folders based on a specific column [03:02].
When to use: Use it when a column has low cardinality (limited distinct values), such as Country or Date [04:10].
Benefit: When you query for a specific value (e.g., Country = 'India'), the system only scans that specific folder instead of the entire dataset [03:21].

Bucketing:
How it works: It divides data into a fixed number of files using a hash function [06:21].
When to use: Use it when a column has high cardinality (many unique values), such as Customer ID or Zip Code [05:44].
Benefit: It distributes data evenly across a specified number of buckets (files) to manage large volumes of data that partitioning alone cannot handle efficiently [07:31].

2. Examples Provided
Partitioning Example [01:43]
Scenario: You have a table with billions of records including a Country column.
The Query: SELECT * FROM table WHERE Country = 'India'.
The Process: * Without partitioning, the system scans all 100 billion records.
With partitioning by Country, the system creates separate folders for India, USA, and UK.
The query goes directly to the India folder, drastically increasing speed [03:02].

Bucketing Example [06:41]
Scenario: You want to organize data by Customer ID (which has millions of unique values).
The Process:
You decide to create 4 buckets.
The system uses a hash function: Customer ID % 4.
If the result is 0, the data goes to Bucket 1; if it's 1, it goes to Bucket 2, and so on [07:07].
This results in exactly 4 files regardless of how many unique Customer IDs exist [07:17].


Partitioning:
SQL
CREATE TABLE zip_codes (...)
PARTITIONED BY (state STRING);
Bucketing:

SQL
CREATE TABLE zip_codes (...)
CLUSTERED BY (zip_code) INTO 10 BUCKETS;


_____________________________________________________________________
What are the differences between Managed Identity and Service Principal, and when should each be used?

Answer: * Managed Identity: * What it is: An identity automatically created and managed by Azure for specific resources
(like Azure Data Factory). [03:15, 01:10:35] * When to use: Use it whenever possible (recommended by Microsoft) 
for authenticating between Azure resources that support it. [13:18] * Advantages: It is highly secure and easy to manage because Azure handles the credentials,
removing the need for manual key rotation. [11:09, 11:18] * Disadvantages: It is less flexible because users have little control over its configuration, 
and it is not supported by all services (e.g., Azure DevOps). [12:31, 07:54]

Service Principal:

What it is: A security identity used by applications, services, and automation tools (created via Azure Entra ID/Active Directory) to access specific Azure resources. [03:51, 10:45]
When to use: Use it when Managed Identity is not supported, or when an external service (like Azure DevOps) needs to access Azure resources. [13:43, 07:39]
Advantages: It is highly flexible; you can create as many as needed and use them across various internal and external services. [12:49, 12:56]
Disadvantages: It is less secure and harder to manage because you must manually handle credentials (secret keys) and rotate them before they expire. [11:35, 11:55]

(
Azure services:

Managed Identity Example (ADF to Databricks):

When you create Azure Data Factory (ADF), Azure automatically creates an identity for it.

To give ADF access to Azure Databricks, you simply go to the Databricks access control and select the "Managed Identity" of that specific Data Factory.

Service Principal Example (Azure DevOps to Databricks):

If you want to use Azure DevOps to run a CI/CD pipeline in Databricks, you cannot use a Managed Identity because Azure DevOps doesn't have one in the same way.

Instead, you must create a "third-party" ID called a Service Principal in Azure Entra ID (Active Directory).

You generate a Client ID and a Secret Key for this Service Principal and then provide those credentials to Azure DevOps so it can "log in" and perform tasks in Databricks
)
________________________________________________________________
copying billion rows from a file to a database and identifies lines that are causing the problem. 
He suggests that the solution is to enable fault tolerance in the copy activity settings and check the "skip the incompatible rows" option. 
This will allow the compatible rows to be pushed into the database, while the incompatible rows can be found in the log
____________________________________________________
https://www.youtube.com/watch?v=C94-000jh6E
Edge Node (Gateway Node): 
An isolated machine outside the main cluster used for submitting Spark jobs. It serves security and organizational purposes:
Prevents direct user access to the cluster to avoid manual data corruption or security risks.
Handles authentication and authorization (e.g., via Kerberos) before jobs are submitted
Deployment Modes: The mode is determined by where the driver program runs.

Client Mode:
The driver runs on the edge node (client machine).
Logs are displayed directly on the user's screen, making it ideal for development and debugging.
Disadvantage: Higher network latency due to two-way communication between the edge node and the cluster.
Disadvantage: If the edge node is shut down or disconnected, the driver dies, and all executors are killed, stopping the process.

Cluster Mode:
The driver runs on one of the worker nodes inside the cluster.
Recommended for production workloads.
Advantage: Lower network latency since the driver and executors are within the same cluster.
Advantage: The edge node can be disconnected after submission without affecting the job, as the driver is already running in the cluster.

_______________________________________________________________________________
Question: How does Spark work? Can you quickly guide me through the Spark architecture? [00:01]

Answer: When you write a Spark application, the driver program gets executed. It follows several steps:
Schema Validation: It checks if the code is syntactically correct and matches the schema catalog. [00:15]
Unresolved Logical Plan: Once validated, it creates an unresolved logical plan. [00:26]
Logical Plan: A logical plan is created with the help of the Catalyst Optimizer, considering cost and roles. [00:39]
Physical Plan: Finally, a physical plan is created and then used for execution. [00:
______________________________________________________________
Question: What exactly is the difference between submitting a job in client mode or in cluster mode in Spark? [00:01]

Answer: - Client Mode: The driver program runs on the client machine [00:09]. This mode allows for interaction with the cluster results 
and is primarily used for development purposes, as it makes it easy to see the execution results from the cluster. [00:15]

Cluster Mode: The driver program runs on the cluster itself [00:25]. This mode is most commonly used for production environments. [00:31]
______________
