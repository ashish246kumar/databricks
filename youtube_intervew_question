
__________________________________________________________
Partitioning:
How it works: It divides data into multiple folders based on a specific column [03:02].
When to use: Use it when a column has low cardinality (limited distinct values), such as Country or Date [04:10].
Benefit: When you query for a specific value (e.g., Country = 'India'), the system only scans that specific folder instead of the entire dataset [03:21].

Bucketing:
How it works: It divides data into a fixed number of files using a hash function [06:21].
When to use: Use it when a column has high cardinality (many unique values), such as Customer ID or Zip Code [05:44].
Benefit: It distributes data evenly across a specified number of buckets (files) to manage large volumes of data that partitioning alone cannot handle efficiently [07:31].

2. Examples Provided
Partitioning Example [01:43]
Scenario: You have a table with billions of records including a Country column.
The Query: SELECT * FROM table WHERE Country = 'India'.
The Process: * Without partitioning, the system scans all 100 billion records.
With partitioning by Country, the system creates separate folders for India, USA, and UK.
The query goes directly to the India folder, drastically increasing speed [03:02].

Bucketing Example [06:41]
Scenario: You want to organize data by Customer ID (which has millions of unique values).
The Process:
You decide to create 4 buckets.
The system uses a hash function: Customer ID % 4.
If the result is 0, the data goes to Bucket 1; if it's 1, it goes to Bucket 2, and so on [07:07].
This results in exactly 4 files regardless of how many unique Customer IDs exist [07:17].


Partitioning:
SQL
CREATE TABLE zip_codes (...)
PARTITIONED BY (state STRING);
Bucketing:

SQL
CREATE TABLE zip_codes (...)
CLUSTERED BY (zip_code) INTO 10 BUCKETS;


_____________________________________________________________________
What are the differences between Managed Identity and Service Principal, and when should each be used?

Answer: * Managed Identity: * What it is: An identity automatically created and managed by Azure for specific resources
(like Azure Data Factory). [03:15, 01:10:35] * When to use: Use it whenever possible (recommended by Microsoft) 
for authenticating between Azure resources that support it. [13:18] * Advantages: It is highly secure and easy to manage because Azure handles the credentials,
removing the need for manual key rotation. [11:09, 11:18] * Disadvantages: It is less flexible because users have little control over its configuration, 
and it is not supported by all services (e.g., Azure DevOps). [12:31, 07:54]

Service Principal:

What it is: A security identity used by applications, services, and automation tools (created via Azure Entra ID/Active Directory) to access specific Azure resources. [03:51, 10:45]
When to use: Use it when Managed Identity is not supported, or when an external service (like Azure DevOps) needs to access Azure resources. [13:43, 07:39]
Advantages: It is highly flexible; you can create as many as needed and use them across various internal and external services. [12:49, 12:56]
Disadvantages: It is less secure and harder to manage because you must manually handle credentials (secret keys) and rotate them before they expire. [11:35, 11:55]

(
Azure services:

Managed Identity Example (ADF to Databricks):

When you create Azure Data Factory (ADF), Azure automatically creates an identity for it.

To give ADF access to Azure Databricks, you simply go to the Databricks access control and select the "Managed Identity" of that specific Data Factory.

Service Principal Example (Azure DevOps to Databricks):

If you want to use Azure DevOps to run a CI/CD pipeline in Databricks, you cannot use a Managed Identity because Azure DevOps doesn't have one in the same way.

Instead, you must create a "third-party" ID called a Service Principal in Azure Entra ID (Active Directory).

You generate a Client ID and a Secret Key for this Service Principal and then provide those credentials to Azure DevOps so it can "log in" and perform tasks in Databricks
)
________________________________________________________________
copying billion rows from a file to a database and identifies lines that are causing the problem. 
He suggests that the solution is to enable fault tolerance in the copy activity settings and check the "skip the incompatible rows" option. 
This will allow the compatible rows to be pushed into the database, while the incompatible rows can be found in the log
____________________________________________________
https://www.youtube.com/watch?v=C94-000jh6E
Edge Node (Gateway Node): 
An isolated machine outside the main cluster used for submitting Spark jobs. It serves security and organizational purposes:
Prevents direct user access to the cluster to avoid manual data corruption or security risks.
Handles authentication and authorization (e.g., via Kerberos) before jobs are submitted
Deployment Modes: The mode is determined by where the driver program runs.

Client Mode:
The driver runs on the edge node (client machine).
Logs are displayed directly on the user's screen, making it ideal for development and debugging.
Disadvantage: Higher network latency due to two-way communication between the edge node and the cluster.
Disadvantage: If the edge node is shut down or disconnected, the driver dies, and all executors are killed, stopping the process.

Cluster Mode:
The driver runs on one of the worker nodes inside the cluster.
Recommended for production workloads.
Advantage: Lower network latency since the driver and executors are within the same cluster.
Advantage: The edge node can be disconnected after submission without affecting the job, as the driver is already running in the cluster.

_______________________________________________________________________________
Question: How does Spark work? Can you quickly guide me through the Spark architecture? [00:01]

Answer: When you write a Spark application, the driver program gets executed. It follows several steps:
Schema Validation: It checks if the code is syntactically correct and matches the schema catalog. [00:15]
Unresolved Logical Plan: Once validated, it creates an unresolved logical plan. [00:26]
Logical Plan: A logical plan is created with the help of the Catalyst Optimizer, considering cost and roles. [00:39]
Physical Plan: Finally, a physical plan is created and then used for execution. [00:
______________________________________________________________
Question: What exactly is the difference between submitting a job in client mode or in cluster mode in Spark? [00:01]

Answer: - Client Mode: The driver program runs on the client machine [00:09]. This mode allows for interaction with the cluster results 
and is primarily used for development purposes, as it makes it easy to see the execution results from the cluster. [00:15]

Cluster Mode: The driver program runs on the cluster itself [00:25]. This mode is most commonly used for production environments. [00:31]
______________
